{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deve Sandbox Project About this site I am creating a documentation for my created environment, all based on CentOS 7 Linux Servers. The documentation on setup of secondary servers are a little bit weird, as this was planned via configuration management. the plan was the secondary servers are \"on the go servers\" meaning that they can be deployable again if destroyed \"accidentally\". This project is complete for me. I want to restart another project with another technologies. Hypervisor Details The hypervisor that i am using is running on proxmox 6.3, specifications below: i7-4790 CPU 16 GB RAM 1 TB HDD Server details Primary servers The primary servers are running on KVM. With most of the primary servers were setup before the automation server, only storage and backup server is configured to use puppet-sync and ansible-pull. Hostname IP Details dns-dhcp.sandbox.io 10.11.100.1 BIND and DHCPD server spacewalk.sandbox.io 10.11.100.101 Management server directory1.sandbox.io 10.11.100.102 LDAP/IPA server docs.sandbox.io 10.11.100.103 Main documentation server sql.sandbox.io 10.11.100.104 PostgreSQL server for spacewalk bot.sandbox.io 10.11.100.105 Puppet, Ansible server storage.sandbox.io 10.11.100.106 LUN and NFS Server for Backup backup.sandbox.io 10.11.100.107 Bacula Server Secondary servers The secondary servers are running on LXC, all configured to use the automation server for configuration management. Hostname IP Details app1.sandbox.io 10.11.100.108 1st Tomcat Server, session replication on app2 app2.sandbox.io 10.11.100.109 2nd Tomcat Server, session replication on app1 ldb.sandbox.io 10.11.100.110 Load Balancer(HAProxy) server for app1 and app2 mail.sandbox.io 10.11.100.111 Postfix Mail Server monitoring.sandbox.io 10.11.100.112 Nagios Server logs.sandbox.io 10.11.100.113 Graylog Server authority.sandbox.io 10.11.100.114 Certificate Authority Server","title":"Home"},{"location":"#deve-sandbox-project","text":"","title":"Deve Sandbox Project"},{"location":"#about-this-site","text":"I am creating a documentation for my created environment, all based on CentOS 7 Linux Servers. The documentation on setup of secondary servers are a little bit weird, as this was planned via configuration management. the plan was the secondary servers are \"on the go servers\" meaning that they can be deployable again if destroyed \"accidentally\". This project is complete for me. I want to restart another project with another technologies.","title":"About this site"},{"location":"#hypervisor-details","text":"The hypervisor that i am using is running on proxmox 6.3, specifications below: i7-4790 CPU 16 GB RAM 1 TB HDD","title":"Hypervisor Details"},{"location":"#server-details","text":"","title":"Server details"},{"location":"#primary-servers","text":"The primary servers are running on KVM. With most of the primary servers were setup before the automation server, only storage and backup server is configured to use puppet-sync and ansible-pull. Hostname IP Details dns-dhcp.sandbox.io 10.11.100.1 BIND and DHCPD server spacewalk.sandbox.io 10.11.100.101 Management server directory1.sandbox.io 10.11.100.102 LDAP/IPA server docs.sandbox.io 10.11.100.103 Main documentation server sql.sandbox.io 10.11.100.104 PostgreSQL server for spacewalk bot.sandbox.io 10.11.100.105 Puppet, Ansible server storage.sandbox.io 10.11.100.106 LUN and NFS Server for Backup backup.sandbox.io 10.11.100.107 Bacula Server","title":"Primary servers"},{"location":"#secondary-servers","text":"The secondary servers are running on LXC, all configured to use the automation server for configuration management. Hostname IP Details app1.sandbox.io 10.11.100.108 1st Tomcat Server, session replication on app2 app2.sandbox.io 10.11.100.109 2nd Tomcat Server, session replication on app1 ldb.sandbox.io 10.11.100.110 Load Balancer(HAProxy) server for app1 and app2 mail.sandbox.io 10.11.100.111 Postfix Mail Server monitoring.sandbox.io 10.11.100.112 Nagios Server logs.sandbox.io 10.11.100.113 Graylog Server authority.sandbox.io 10.11.100.114 Certificate Authority Server","title":"Secondary servers"},{"location":"app/","text":"Application Server Scope I will install 2 tomcat applications on each server running high availability setup, that is being proxied by httpd application to port 80, and will be load balance by another server installed with haproxy. app1 and app2 will synct through multicast ldb.sandbox.io --> app1.sandbox.io:80 -> app1.sandbox.io:8080 -- | |--> application(HA) -> app2.sandbox.io:80 -> app2.sandbox.io:8080 -- Simple application that will be run will be a java webpage that ensures sessions were retained after submitting request #index.jsp <%@page import=\"java.util.ArrayList\"%> <%@page import=\"java.util.Date\"%> <%@page import=\"java.util.List\"%> <%@page contentType=\"text/html\" pageEncoding=\"UTF-8\"%> <!DOCTYPE html> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"> <title>JSP Page</title> </head> <body> <FONT size = 5 COLOR=\"#0000FF\"> Instance 1 <br/><br/> </FONT> <hr/> <FONT size = 5 COLOR=\"#CC0000\"> <br/> Session Id : <%=request.getSession().getId()%> <br/> Is it New Session : <%=request.getSession().isNew()%><br/> Session Creation Date : <%=new Date(request.getSession().getCreationTime())%><br/> Session Access Date : <%=new Date(request.getSession().getLastAccessedTime())%><br/><br/> </FONT> <b>Cart List </b><br/> <hr/> <ul> <% String bookName = request.getParameter(\"bookName\"); List<String> listOfBooks = (List<String>) request.getSession().getAttribute(\"Books\"); if (listOfBooks == null) { listOfBooks = new ArrayList<String>(); request.getSession().setAttribute(\"Books\", listOfBooks); } if (bookName != null) { listOfBooks.add(bookName); request.getSession().setAttribute(\"Books\", listOfBooks); } for (String book : listOfBooks) { out.println(\"<li>\"+book + \"</li><br/>\"); } %> </ul> <hr/> <form action=\"index.jsp\" method=\"post\"> Book Name <input type=\"text\" name=\"bookName\" /> <input type=\"submit\" value=\"Add to Cart\"/> </form> <hr/> </body> </html> DHCP and DNS Entries Forward lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 app IN CNAME ldb Reverse lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 108 IN PTR app1.sandbox.io. 109 IN PTR app2.sandbox.io. 110 IN PTR ldb.sandbox.io. DHCP client entries host app1 { hardware ethernet 2E:A5:8D:FB:B9:F8; fixed-address 10.11.100.108; } -- host app2{ hardware ethernet AE:45:14:38:61:C3; fixed-address 10.11.100.109; } -- host ldb { hardware ethernet 4E:D6:AE:43:D4:84; fixed-address 10.11.100.110; } App1 and App2 Servers Installation and Setup Needed to install the following package: tomcat httpd tomcat-webapps tomcat-admin-webapps This can be done by yum or ansible yum -y install tomcat httpd tomcat-webapps tomcat-admin-webapps - name: install tomcat application yum: name: - tomcat - tomcat-webapps - tomcat-admin-webapps - httpd Instruct tomcat to only allow 256mb of memory. echo 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' > /usr/share/tomcat/conf/tomcat.conf - name: add JAVA_OPTS option to limit max permit memory size for each tomcat instances blockinfile: path: /usr/share/tomcat/conf/tomcat.conf block: 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' state: present Add manager credentials for testing tomcat - name: add admin user for manage login lineinfile: insertafter: '<tomcat-users>' path: /usr/share/tomcat/conf/tomcat-users.xml line: '<user username=\"admin\" password=\"Password123!\" roles=\"manager-gui,admin-gui\"/>' add ports for exception in firewall - name: add ports to firewalld firewalld: port: '{{item}}' state: enabled immediate: true permanent: true with_items: - 8080/tcp - 80/tcp - 4000/tcp Since this setup will be needing multicast, need to allow multicast address - name: add multicast address to firewalld firewalld: rich_rule: rule family=\"ipv4\" destination address=\"228.0.0.4\" protocol value=\"ip\" accept permanent: true immediate: true state: enabled Create directories - name: create directories for sites-enabled, sites-available and test directory for tomcat file: state: directory path: '{{item}}' with_items: - /etc/httpd/sites-enabled - /etc/httpd/sites-available - /var/lib/tomcat/webapps/test - /var/lib/tomcat/webapps/test/WEB-INF - /var/www/sandbox Insert the index.jsp file to the newly created test directory - name: create test application file copy: src: files/app/test-index.jsp dest: /var/lib/tomcat/webapps/test/index.jsp Copy each virtual host file and paste it on /etc/httpd/sites-available on each server - name: copy http tomcat configuration file for app1 copy: src: files/app/tomcat1.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app1\" - name: copy http tomcat configuration file for app2 copy: src: files/app/tomcat2.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app2\" Contents of tomcat1.conf and tomcat2.conf # tomcat1.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app1.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app1.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app1.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app1_error.log CustomLog /var/log/httpd/app1_common.log combined </VirtualHost> # tomcat2.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app2.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app2.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app2.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app2_error.log CustomLog /var/log/httpd/app2_common.log combined </VirtualHost> Create a softlink of virtualhost file to /etc/httpd/sites-enabled/ - name: create link of tomcat.conf to sites-enabled file: src: /etc/httpd/sites-available/tomcat.conf dest: /etc/httpd/sites-enabled/tomcat.conf state: link Add 'IncludeOptional sites-enabled/*.conf' in /etc/httpd/conf/httpd.conf to add all conf files in sites-enabled directory - name: add line to /etc/httpd/conf/httpd.conf lineinfile: path: /etc/httpd/conf/httpd.conf line: 'IncludeOptional sites-enabled/*.conf' - name: set boolean for httpd_can_network_connect to true Create labels for tomcat setup on both servers - name: create labels on each node in /etc/tomcat/server.xml for app1 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat1\">' when: ansible_hostname == \"app1\" - name: create labels on each node in /etc/tomcat/server.xml for app2 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat2\">' when: ansible_hostname == \"app2\" Create web.xml file /var/lib/tomcat/webapps/test/WEB-INF/ directory - name: add web.xml file to test directory copy: src: files/app/web.xml dest: /var/lib/tomcat/webapps/test/WEB-INF/web.xml Contents of web.xml <?xml version=\"1.0\" encoding=\"ISO-8859-1\"?> <web-app xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\" metadata-complete=\"true\"> <display-name>Welcome to Tomcat</display-name> <description> Welcome to Tomcat </description> <distributable/> </web-app> Need to add the cluster function on /etc/tomcat/server.xml ive done this via blockinline on ansible - name: add line block on /etc/tomcat/server.xml blockinfile: path: /etc/tomcat/server.xml marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\" insertafter: '<Engine name=\"Catalina\" defaultHost=\"localhost\"' backup: true block: | <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\" channelSendOptions=\"8\"> <Manager className=\"org.apache.catalina.ha.session.DeltaManager\" expireSessionsOnShutdown=\"false\" notifyListenersOnReplication=\"true\"/> <Channel className=\"org.apache.catalina.tribes.group.GroupChannel\"> <Membership className=\"org.apache.catalina.tribes.membership.McastService\" address=\"228.0.0.4\" port=\"45564\" frequency=\"500\" dropTime=\"3000\"/> <Sender className=\"org.apache.catalina.tribes.transport.ReplicationTransmitter\"> <Transport className=\"org.apache.catalina.tribes.transport.nio.PooledParallelSender\"/> </Sender> <Receiver className=\"org.apache.catalina.tribes.transport.nio.NioReceiver\" address=\"auto\" port=\"4000\" autoBind=\"100\" selectorTimeout=\"5000\" maxThreads=\"6\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.TcpFailureDetector\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor\"/> </Channel> <Valve className=\"org.apache.catalina.ha.tcp.ReplicationValve\" filter=\"\"/> <Valve className=\"org.apache.catalina.ha.session.JvmRouteBinderValve\"/> <ClusterListener className=\"org.apache.catalina.ha.session.JvmRouteSessionIDBinderListener\"/> <ClusterListener className=\"org.apache.catalina.ha.session.ClusterSessionListener\"/> </Cluster> Start the services - name: start httpd service: name: httpd state: restarted enabled: true - name: start tomcat service service: name: tomcat state: restarted enabled: true Issues Running on LXC Synching issues may happen if your running on linux container, this is due to tomcat app and /etc/hosts file of the lxc not correctly getting the servers own IP. Edit /etc/nsswitch if you want a simpler fix. - name: swap dns and files entry on /etc/nsswitch lineinfile: path: /etc/nsswitch.conf regexp: '^hosts' line: 'hosts: dns files myhostname' when: ansible_virtualization_type == \"lxc\" Running on KVM unlike LXC, you can run selinux on KVM and can cause issues, recommended to change booleans instead of disabling it permanently - name: set boolean for httpd_can_network_connect to true seboolean: name: '{{item}}' persistent: true state: true with_items: - httpd_can_network_connect - nis_enabled when: ansible_virtualization_type == \"kvm\" Load Balancer Setup Configuration on load balancer is simple Installation and Setup Install haproxy to the load balancer - name: install haproxy to the server yum: name: haproxy state: present replace haproxy.cfg with the configured one - name: copy haproxy.cfg file /etc/haproxy/haproxy.cfg copy: src: files/haproxy/haproxy.cfg dest: /etc/haproxy/haproxy.cfg Contents of configured haproxy.cfg #/etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 frontend tomcat_app_frontend bind *:80 stats uri /haproxy?stats default_backend tomcat_app_backend backend tomcat_app_backend balance roundrobin server app1 app1.sandbox.io:80 check server app2 app2.sandbox.io:80 checkv Open port 80/tcp on firewalld - name: open firewalld port 80 firewalld: service: http state: enabled permanent: true immediate: true Start haproxy service - name: start load balancing service: name: haproxy state: started enabled: true","title":"Application"},{"location":"app/#application-server","text":"","title":"Application Server"},{"location":"app/#scope","text":"I will install 2 tomcat applications on each server running high availability setup, that is being proxied by httpd application to port 80, and will be load balance by another server installed with haproxy. app1 and app2 will synct through multicast ldb.sandbox.io --> app1.sandbox.io:80 -> app1.sandbox.io:8080 -- | |--> application(HA) -> app2.sandbox.io:80 -> app2.sandbox.io:8080 -- Simple application that will be run will be a java webpage that ensures sessions were retained after submitting request #index.jsp <%@page import=\"java.util.ArrayList\"%> <%@page import=\"java.util.Date\"%> <%@page import=\"java.util.List\"%> <%@page contentType=\"text/html\" pageEncoding=\"UTF-8\"%> <!DOCTYPE html> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"> <title>JSP Page</title> </head> <body> <FONT size = 5 COLOR=\"#0000FF\"> Instance 1 <br/><br/> </FONT> <hr/> <FONT size = 5 COLOR=\"#CC0000\"> <br/> Session Id : <%=request.getSession().getId()%> <br/> Is it New Session : <%=request.getSession().isNew()%><br/> Session Creation Date : <%=new Date(request.getSession().getCreationTime())%><br/> Session Access Date : <%=new Date(request.getSession().getLastAccessedTime())%><br/><br/> </FONT> <b>Cart List </b><br/> <hr/> <ul> <% String bookName = request.getParameter(\"bookName\"); List<String> listOfBooks = (List<String>) request.getSession().getAttribute(\"Books\"); if (listOfBooks == null) { listOfBooks = new ArrayList<String>(); request.getSession().setAttribute(\"Books\", listOfBooks); } if (bookName != null) { listOfBooks.add(bookName); request.getSession().setAttribute(\"Books\", listOfBooks); } for (String book : listOfBooks) { out.println(\"<li>\"+book + \"</li><br/>\"); } %> </ul> <hr/> <form action=\"index.jsp\" method=\"post\"> Book Name <input type=\"text\" name=\"bookName\" /> <input type=\"submit\" value=\"Add to Cart\"/> </form> <hr/> </body> </html>","title":"Scope"},{"location":"app/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 app IN CNAME ldb Reverse lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 108 IN PTR app1.sandbox.io. 109 IN PTR app2.sandbox.io. 110 IN PTR ldb.sandbox.io. DHCP client entries host app1 { hardware ethernet 2E:A5:8D:FB:B9:F8; fixed-address 10.11.100.108; } -- host app2{ hardware ethernet AE:45:14:38:61:C3; fixed-address 10.11.100.109; } -- host ldb { hardware ethernet 4E:D6:AE:43:D4:84; fixed-address 10.11.100.110; }","title":"DHCP and DNS Entries"},{"location":"app/#app1-and-app2-servers","text":"","title":"App1 and App2 Servers"},{"location":"app/#installation-and-setup","text":"Needed to install the following package: tomcat httpd tomcat-webapps tomcat-admin-webapps This can be done by yum or ansible yum -y install tomcat httpd tomcat-webapps tomcat-admin-webapps - name: install tomcat application yum: name: - tomcat - tomcat-webapps - tomcat-admin-webapps - httpd Instruct tomcat to only allow 256mb of memory. echo 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' > /usr/share/tomcat/conf/tomcat.conf - name: add JAVA_OPTS option to limit max permit memory size for each tomcat instances blockinfile: path: /usr/share/tomcat/conf/tomcat.conf block: 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' state: present Add manager credentials for testing tomcat - name: add admin user for manage login lineinfile: insertafter: '<tomcat-users>' path: /usr/share/tomcat/conf/tomcat-users.xml line: '<user username=\"admin\" password=\"Password123!\" roles=\"manager-gui,admin-gui\"/>' add ports for exception in firewall - name: add ports to firewalld firewalld: port: '{{item}}' state: enabled immediate: true permanent: true with_items: - 8080/tcp - 80/tcp - 4000/tcp Since this setup will be needing multicast, need to allow multicast address - name: add multicast address to firewalld firewalld: rich_rule: rule family=\"ipv4\" destination address=\"228.0.0.4\" protocol value=\"ip\" accept permanent: true immediate: true state: enabled Create directories - name: create directories for sites-enabled, sites-available and test directory for tomcat file: state: directory path: '{{item}}' with_items: - /etc/httpd/sites-enabled - /etc/httpd/sites-available - /var/lib/tomcat/webapps/test - /var/lib/tomcat/webapps/test/WEB-INF - /var/www/sandbox Insert the index.jsp file to the newly created test directory - name: create test application file copy: src: files/app/test-index.jsp dest: /var/lib/tomcat/webapps/test/index.jsp Copy each virtual host file and paste it on /etc/httpd/sites-available on each server - name: copy http tomcat configuration file for app1 copy: src: files/app/tomcat1.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app1\" - name: copy http tomcat configuration file for app2 copy: src: files/app/tomcat2.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app2\" Contents of tomcat1.conf and tomcat2.conf # tomcat1.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app1.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app1.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app1.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app1_error.log CustomLog /var/log/httpd/app1_common.log combined </VirtualHost> # tomcat2.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app2.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app2.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app2.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app2_error.log CustomLog /var/log/httpd/app2_common.log combined </VirtualHost> Create a softlink of virtualhost file to /etc/httpd/sites-enabled/ - name: create link of tomcat.conf to sites-enabled file: src: /etc/httpd/sites-available/tomcat.conf dest: /etc/httpd/sites-enabled/tomcat.conf state: link Add 'IncludeOptional sites-enabled/*.conf' in /etc/httpd/conf/httpd.conf to add all conf files in sites-enabled directory - name: add line to /etc/httpd/conf/httpd.conf lineinfile: path: /etc/httpd/conf/httpd.conf line: 'IncludeOptional sites-enabled/*.conf' - name: set boolean for httpd_can_network_connect to true Create labels for tomcat setup on both servers - name: create labels on each node in /etc/tomcat/server.xml for app1 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat1\">' when: ansible_hostname == \"app1\" - name: create labels on each node in /etc/tomcat/server.xml for app2 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat2\">' when: ansible_hostname == \"app2\" Create web.xml file /var/lib/tomcat/webapps/test/WEB-INF/ directory - name: add web.xml file to test directory copy: src: files/app/web.xml dest: /var/lib/tomcat/webapps/test/WEB-INF/web.xml Contents of web.xml <?xml version=\"1.0\" encoding=\"ISO-8859-1\"?> <web-app xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\" metadata-complete=\"true\"> <display-name>Welcome to Tomcat</display-name> <description> Welcome to Tomcat </description> <distributable/> </web-app> Need to add the cluster function on /etc/tomcat/server.xml ive done this via blockinline on ansible - name: add line block on /etc/tomcat/server.xml blockinfile: path: /etc/tomcat/server.xml marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\" insertafter: '<Engine name=\"Catalina\" defaultHost=\"localhost\"' backup: true block: | <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\" channelSendOptions=\"8\"> <Manager className=\"org.apache.catalina.ha.session.DeltaManager\" expireSessionsOnShutdown=\"false\" notifyListenersOnReplication=\"true\"/> <Channel className=\"org.apache.catalina.tribes.group.GroupChannel\"> <Membership className=\"org.apache.catalina.tribes.membership.McastService\" address=\"228.0.0.4\" port=\"45564\" frequency=\"500\" dropTime=\"3000\"/> <Sender className=\"org.apache.catalina.tribes.transport.ReplicationTransmitter\"> <Transport className=\"org.apache.catalina.tribes.transport.nio.PooledParallelSender\"/> </Sender> <Receiver className=\"org.apache.catalina.tribes.transport.nio.NioReceiver\" address=\"auto\" port=\"4000\" autoBind=\"100\" selectorTimeout=\"5000\" maxThreads=\"6\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.TcpFailureDetector\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor\"/> </Channel> <Valve className=\"org.apache.catalina.ha.tcp.ReplicationValve\" filter=\"\"/> <Valve className=\"org.apache.catalina.ha.session.JvmRouteBinderValve\"/> <ClusterListener className=\"org.apache.catalina.ha.session.JvmRouteSessionIDBinderListener\"/> <ClusterListener className=\"org.apache.catalina.ha.session.ClusterSessionListener\"/> </Cluster> Start the services - name: start httpd service: name: httpd state: restarted enabled: true - name: start tomcat service service: name: tomcat state: restarted enabled: true","title":"Installation and Setup"},{"location":"app/#issues","text":"","title":"Issues"},{"location":"app/#running-on-lxc","text":"Synching issues may happen if your running on linux container, this is due to tomcat app and /etc/hosts file of the lxc not correctly getting the servers own IP. Edit /etc/nsswitch if you want a simpler fix. - name: swap dns and files entry on /etc/nsswitch lineinfile: path: /etc/nsswitch.conf regexp: '^hosts' line: 'hosts: dns files myhostname' when: ansible_virtualization_type == \"lxc\"","title":"Running on LXC"},{"location":"app/#running-on-kvm","text":"unlike LXC, you can run selinux on KVM and can cause issues, recommended to change booleans instead of disabling it permanently - name: set boolean for httpd_can_network_connect to true seboolean: name: '{{item}}' persistent: true state: true with_items: - httpd_can_network_connect - nis_enabled when: ansible_virtualization_type == \"kvm\"","title":"Running on KVM"},{"location":"app/#load-balancer-setup","text":"Configuration on load balancer is simple","title":"Load Balancer Setup"},{"location":"app/#installation-and-setup_1","text":"Install haproxy to the load balancer - name: install haproxy to the server yum: name: haproxy state: present replace haproxy.cfg with the configured one - name: copy haproxy.cfg file /etc/haproxy/haproxy.cfg copy: src: files/haproxy/haproxy.cfg dest: /etc/haproxy/haproxy.cfg Contents of configured haproxy.cfg #/etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 frontend tomcat_app_frontend bind *:80 stats uri /haproxy?stats default_backend tomcat_app_backend backend tomcat_app_backend balance roundrobin server app1 app1.sandbox.io:80 check server app2 app2.sandbox.io:80 checkv Open port 80/tcp on firewalld - name: open firewalld port 80 firewalld: service: http state: enabled permanent: true immediate: true Start haproxy service - name: start load balancing service: name: haproxy state: started enabled: true","title":"Installation and Setup"},{"location":"auth/","text":"Certificate Authority Server The CA Server will act as a root CA and intermediate CA repository and will create, sign, and validate all my PKI(Public Key Infrastructure) in the environment. I am not doing any automation in this server. Also, I am creating this server since im done looking my internal homelab having a strikethrough on https whenever I browse the sites i am maintaining. I just want the feel that i am doing this outside on my local area network :P DHCP and DNS Entries Forward lookup zone @ IN NS authority.sandbox.io. @ IN A 10.11.100.114 authority IN A 10.11.100.114 auth IN CNAME authority Reverse lookup zone @ IN NS authority.sandbox.io. authority IN A 10.11.100.114 114 IN PTR authority.sandbox.io. DHCP entry host authority { hardware ethernet 1E:13:68:37:74:28; fixed-address 10.11.100.114; } Installation and Setup Installation and directory management Install openssl and tree if is not installed, though tree is optional only if you want to get the idea on your infrastructure [root@auth ~]# yum -y install openssl tree -y Loaded plugins: fastestmirror, rhnplugin This system is receiving updates from RHN Classic or Red Hat Satellite. Loading mirror speeds from cached hostfile Package 1:openssl-1.0.2k-22.el7_9.x86_64 already installed and latest version Package tree-1.6.0-10.el7.x86_64 already installed and latest version Nothing to do We need to create root(root-ca), intermediate(sub-ca) and server directories, with certs, crl, csr, newcerts, private sub-directories. root-ca: this is our top level authority for issuing certificates, it will only issuing to intermediate(sub-ca) sub-ca: since if we only signed certificates via root-ca on real life, it is too much risk as they are much valuable. so to insulate the root CAs we created a sub-ca to signed server certificates on behalf of root-ca. server: directory for our whole environment of servers for certificate signing request certs: crt files that had been signed or self signed. private: private key files csr: certificate signing request, this is the location of csr files for sub-ca or root-ca signing. crl: certificate revocation list, location for all revoke certificates. Note: I am only emulating things and doing this in one server instead of a hierarchy of servers for root CA, intermediate CA, and so on, to build trust relationships. [root@auth ~]# mkdir -p ca/{root-ca,sub-ca,server}/{private,certs,newcerts,crl,csr} [root@auth ~]# tree ca/ ca/ \u251c\u2500\u2500 root-ca \u2502 \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 crl \u2502 \u251c\u2500\u2500 csr \u2502 \u251c\u2500\u2500 newcerts \u2502 \u2514\u2500\u2500 private \u251c\u2500\u2500 server \u2502 \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 crl \u2502 \u251c\u2500\u2500 csr \u2502 \u251c\u2500\u2500 newcerts \u2502 \u2514\u2500\u2500 private \u2514\u2500\u2500 sub-ca \u251c\u2500\u2500 certs \u251c\u2500\u2500 crl \u251c\u2500\u2500 csr \u251c\u2500\u2500 newcerts \u2514\u2500\u2500 private All private directories should be readable to user. [root@auth ca]# chmod -v 700 {root-ca,sub-ca,server}/private mode of \u2018root-ca/private\u2019 changed from 0755 (rwxr-xr-x) to 0700 (rwx------) mode of \u2018sub-ca/private\u2019 changed from 0755 (rwxr-xr-x) to 0700 (rwx------) mode of \u2018server/private\u2019 changed from 0755 (rwxr-xr-x) to 0700 (rwx------) Create an index file for root-ca and sub-ca directory touch {root-ca,sub-ca}/index the directories also needs a serial file, we can generate a random hexadecimal 16 bit output via openssl rand -hex 16 openssl rand -hex 16 > root-ca/serial openssl rand -hex 16 > sub-ca/serial Creation of private keys Now we want to generate the 3 private keys for root-ca, sub-ca, and server directory. The server private key is only openssl genrsa -out server/private/server.key 2048 as i dont want to put a passphrase on the server and manually typing the pass phrase whenever i restart nginx or httpd service. [root@auth ca]# openssl genrsa -aes256 -out root-ca/private/ca.key 4096 Generating RSA private key, 4096 bit long modulus .............................++ .....................................................................................++ e is 65537 (0x10001) Enter pass phrase for root-ca/private/ca.key: Verifying - Enter pass phrase for root-ca/private/ca.key: [root@auth ca]# openssl genrsa -aes256 -out sub-ca/private/sub-ca.key 4096 Generating RSA private key, 4096 bit long modulus ...................................................................................................................................................................++ .....................................................++ e is 65537 (0x10001) Enter pass phrase for sub-ca/private/sub-ca.key: Verifying - Enter pass phrase for sub-ca/private/sub-ca.key: [root@auth ca]# openssl genrsa -out server/private/server.key 2048 Generating RSA private key, 2048 bit long modulus ........................................+++ ...............+++ e is 65537 (0x10001) Bit strength of root-ca and sub-ca private key is implemented to use 4096 for us to have a strongest protection we can, we only use 2048 on server as this may be too much overhead on servers if it using a lot of web traffic RootCA certificate configuration Now create a configuration file for root-ca, this will be used for signing the self signed certificate of rootca #root-ca/root-ca.conf [ca] default_ca = CA_default [CA_default] dir = /root/ca/root-ca certs = $dir/certs crl_dir = $dir/crl new_certs_dir = $dir/newcerts database = $dir/index serial = $dir/serial RANDFILE = $dir/private/.rand private_key = $dir/private/ca.key certificate = $dir/certs/ca.crt crlnumber = $dir/crlnumber crl = $dir/crl/ca.crl crl_extensions = crl_ext default_crl_days = 30 default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = 365 preserve = no policy = policy_strict [ policy_strict ] countryName = supplied stateOrProvinceName = supplied organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] default_bits = 2048 distinguished_name = req_distinguished_name string_mask = utf8only default_md = sha256 x509_extensions = v3_ca [ req_distinguished_name ] countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address countryName_default = PH stateOrProvinceName_default = Cavite 0.organizationName_default = Sandbox [ v3_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \"OpenSSL Generated Server Certificate\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth Create the root certificate file by using openssl req -config root-ca.conf -key private/ca.key -new -x509 -days 3650 -sha256 -extensions v3_ca -out certs/ca.crt -config: use this config instead of openssl.cnf file located in /etc/pki/tls/ -key: location of private key -days: how long the public cert will last before expiring -x509: create the certificate in x509 format -sha256: message digest format -extensions: use the v3_ca function located on root-ca.conf -out: output location where the certificate will be created The command will output similar below: [root@auth ca]# cd root-ca/ [root@auth root-ca]# openssl req -config root-ca.conf -key private/ca.key -new -x509 -days 3650 -sha256 -extensions v3_ca -out certs/ca.crt Enter pass phrase for private/ca.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. Country Name (2 letter code) [PH]:PH State or Province Name [Cavite]:Cavite Locality Name []:Bacoor Organization Name [Sandbox]: Organizational Unit Name []:IT Common Name []:ROOTCA Email Address []: [root@auth root-ca]# You can verify your certificate file using openssl x509 -in certs/ca.crt -noout -text [root@auth root-ca]# openssl x509 -in certs/ca.crt -noout -text Certificate: Data: Version: 3 (0x2) Serial Number: f3:ac:8e:98:a4:5c:38:bc Signature Algorithm: sha256WithRSAEncryption Issuer: C=PH, ST=Cavite, L=Bacoor, O=Sandbox, OU=IT, CN=ROOTCA Validity Not Before: Dec 5 16:16:02 2021 GMT Not After : Dec 3 16:16:02 2031 GMT Subject: C=PH, ST=Cavite, L=Bacoor, O=Sandbox, OU=IT, CN=ROOTCA Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: Intermediate CA CSR and configuration We need also to create a separate ca configuration file for sub-ca directory #sub-ca.conf [ca] default_ca = CA_default [CA_default] # Change this to your sub-ca directory dir = /root/ca/sub-ca certs = $dir/certs crl_dir = $dir/crl new_certs_dir = $dir/newcerts database = $dir/index serial = $dir/serial RANDFILE = $dir/private/.rand private_key = $dir/private/sub-ca.key certificate = $dir/certs/sub-ca.crt crlnumber = $dir/crlnumber crl = $dir/crl/ca.crl crl_extensions = crl_ext default_crl_days = 30 default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = 365 preserve = no policy = policy_loose [ policy_strict ] countryName = supplied stateOrProvinceName = supplied organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] default_bits = 2048 distinguished_name = req_distinguished_name string_mask = utf8only default_md = sha256 x509_extensions = v3_ca [ req_distinguished_name ] countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address countryName_default = PH stateOrProvinceName_default = Cavite 0.organizationName_default = Sandbox [ v3_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \"OpenSSL Generated Server Certificate\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth # This is important to change for chrome browser not to tag it as insecure subjectAltName = @alt_names [ alt_names ] DNS.1 = docs.sandbox.io DNS.2 = devdocs.sandbox.io We need to create a certificate signing request on the sub-ca directory for the root ca to sign it and create a certificate file. You can use openssl req -config sub-ca.conf -new -key private/sub-ca.key -sha256 -out csr/sub-ca.csr [root@auth sub-ca]# openssl req -config sub-ca.conf -new -key private/sub-ca.key -sha256 -out csr/sub-ca.csr Enter pass phrase for private/sub-ca.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [PH]:PH State or Province Name [Cavite]: Locality Name []: Organization Name [Sandbox]: Organizational Unit Name []:IT Common Name []:authority.sandbox.io Email Address []:root@sandbox.io Move to the root-ca directory and create the sub-ca certificate file using its certificate signing request, use openssl ca -config root-ca.conf -extensions v3_intermediate_ca -days 1825 -notext -in ../sub-ca/csr/sub-ca.csr -out ../sub-ca/certs/sub-ca.crt [root@auth sub-ca]# cd ../root-ca/ [root@auth root-ca]# openssl ca -config root-ca.conf -extensions v3_intermediate_ca -days 1825 -notext -in ../sub-ca/csr/sub-ca.csr -out ../sub-ca/certs/sub-ca.crt Using configuration from root-ca.conf Enter pass phrase for /root/ca/root-ca/private/ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: d1:a1:ba:cc:a3:80:57:1b:55:8f:ff:e2:61:59:eb:15 Validity Not Before: Dec 5 16:56:56 2021 GMT Not After : Dec 4 16:56:56 2026 GMT Subject: countryName = PH stateOrProvinceName = Cavite organizationName = Sandbox organizationalUnitName = IT commonName = authority.sandbox.io emailAddress = root@sandbox.io X509v3 extensions: X509v3 Subject Key Identifier: D4:02:04:89:96:B7:51:04:99:0C:62:42:1D:DB:75:55:AB:E8:73:78 X509v3 Authority Key Identifier: keyid:E1:A0:12:4D:CF:D9:24:BD:B8:F0:D7:BC:84:FE:A4:0D:11:28:42:58 X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Certificate is to be certified until Dec 4 16:56:56 2026 GMT (1825 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated Once it is done, it will also create a pem file which is the same with the cert file we created for sub-ca, index file is also updated [root@auth ca]# cat root-ca/index V 261204165656Z D1A1BACCA380571B558FFFE26159EB15 unknown /C=PH/ST=Cavite/O=Sandbox/OU=IT/CN=authority.sandbox.io/emailAddress=root@sandbox.io [root@auth ca]# tree . \u251c\u2500\u2500 root-ca \u2502 \u251c\u2500\u2500 certs \u2502 \u2502 \u2514\u2500\u2500 ca.crt \u2502 \u251c\u2500\u2500 crl \u2502 \u251c\u2500\u2500 csr \u2502 \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 index.attr \u2502 \u251c\u2500\u2500 index.old \u2502 \u251c\u2500\u2500 newcerts \u2502 \u2502 \u2514\u2500\u2500 D1A1BACCA380571B558FFFE26159EB15.pem # this file \u2502 \u251c\u2500\u2500 private \u2502 \u2502 \u2514\u2500\u2500 ca.key \u2502 \u251c\u2500\u2500 root-ca.conf \u2502 \u251c\u2500\u2500 serial \u2502 \u2514\u2500\u2500 serial.old \u2514\u2500\u2500 sub-ca \u251c\u2500\u2500 certs \u2502 \u2514\u2500\u2500 sub-ca.crt # is the same with this file \u251c\u2500\u2500 crl \u251c\u2500\u2500 csr \u2502 \u2514\u2500\u2500 sub-ca.csr \u251c\u2500\u2500 index \u251c\u2500\u2500 newcerts \u251c\u2500\u2500 private \u2502 \u2514\u2500\u2500 sub-ca.key \u251c\u2500\u2500 serial \u2514\u2500\u2500 sub-ca.conf Server CSR signing and configuration Now create a certificate signing request for each servers on your environment that needs SSL, on my end, i will use it for my docs.sandbox.io website: openssl req -key private/server.key -new -sha256 -out csr/docs.csr [root@auth server]# openssl req -key private/server.key -new -sha256 -out csr/docs.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:PH State or Province Name (full name) []:Cavite Locality Name (eg, city) [Default City]:Dasma Organization Name (eg, company) [Default Company Ltd]:Playground Organizational Unit Name (eg, section) []:Finance Common Name (eg, your name or your servers hostname) []:docs.sandbox.io Email Address []:deve@sandbox.io Please enter the following \"extra\" attributes # Note: You can enter \"extra\" as password or you can also leave it as blank to be sent with your certificate request A challenge password []: An optional company name []: This can be done on the server itself but to make it centralize I did it on this server. Sign the certificate using the sub-ca certificate, use: openssl ca -config sub-ca.conf -extensions server_cert -days 365 -notext -in ../server/csr/docs1.csr -out ../server/certs/docs1.crt [root@auth sub-ca]# openssl ca -config sub-ca.conf -extensions server_cert -days 365 -notext -in ../server/csr/docs.csr -out . ./server/certs/docs.crt Using configuration from sub-ca.conf Enter pass phrase for /root/ca/sub-ca/private/sub-ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 8f:1f:da:80:c0:1e:c6:21:ca:32:1b:0b:11:c7:0b:e7 Validity Not Before: Dec 5 17:26:36 2021 GMT Not After : Dec 5 17:26:36 2022 GMT Subject: countryName = PH stateOrProvinceName = Cavite localityName = Dasma organizationName = Playground organizationalUnitName = Finance commonName = docs.sandbox.io emailAddress = deve@sandbox.io X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Cert Type: SSL Server Netscape Comment: OpenSSL Generated Server Certificate X509v3 Subject Key Identifier: E3:A4:49:8B:37:7D:11:BA:2C:7B:B8:3D:70:27:F5:C9:F7:2D:8D:58 X509v3 Authority Key Identifier: keyid:D4:02:04:89:96:B7:51:04:99:0C:62:42:1D:DB:75:55:AB:E8:73:78 DirName:/C=PH/ST=Cavite/L=Bacoor/O=Sandbox/OU=IT/CN=ROOTCA serial:D1:A1:BA:CC:A3:80:57:1B:55:8F:FF:E2:61:59:EB:15 X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Subject Alternative Name: DNS:docs.sandbox.io, DNS:devdocs.sandbox.io Certificate is to be certified until Dec 5 17:26:36 2022 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated You can verify the certificate file using: openssl x509 -in certs/docs.crt -noout -text [root@auth sub-ca]# cd ../server/ [root@auth server]# openssl x509 -in certs/docs.crt -noout -text Certificate: Data: Version: 3 (0x2) Serial Number: 8f:1f:da:80:c0:1e:c6:21:ca:32:1b:0b:11:c7:0b:e7 Signature Algorithm: sha256WithRSAEncryption Issuer: C=PH, ST=Cavite, O=Sandbox, OU=IT, CN=authority.sandbox.io/emailAddress=root@sandbox.io Validity Not Before: Dec 5 17:26:36 2021 GMT Not After : Dec 5 17:26:36 2022 GMT Subject: C=PH, ST=Cavite, L=Dasma, O=Playground, OU=Finance, CN=docs.sandbox.io/emailAddress=deve@sandbox.io Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Configuration to other servers Transferring of files Transfer the certificate files to docs.sandbox.io, since i am using nginx i will concatenate docs.crt and sub-ca.crt together since this is needed for the client to verify its intermediate certificate authority [root@auth certs]# cat certs/docs.crt ../../sub-ca/certs/sub-ca.crt > certs/docs-chained.crt [root@auth server]# scp certs/docs-chained.crt docs:/etc/pki/tls/certs/docs.crt root@docs password: docs.crt 100% 2143 7.7MB/s 00:00 [root@auth server]# scp private/server.key docs:/etc/pki/tls/private/docs.key root@docs password: server.key 100% 1679 5.8MB/s 00:00 Nginx configuration On docs server configure it to use HTTPS on /etc/nginx/nginx.conf server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name docs.sandbox.io; root /opt/sites/docs-site; ssl_certificate \"/etc/pki/tls/certs/docs.crt\"; ssl_certificate_key \"/etc/pki/tls/private/docs.key\"; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } Restart nginx service systemctl restart nginx Client Trust configuration On Windows(Google Chrome) Since that our root certificate file is not trusted by the clients that will access the website(unlike digicert or entrust), we need to transfer the certificate file on each clients Navigate to Privacy and Security in chrome://settings, and click security to open the tab. browse to Advance and click \"Manage Certificates\", Certificate tab will appear. Click Trusted Root Certificate Authorities. Import the certificate and click yes to the Security Warning notification You can now have a verified connection on your website, this can be done on firefox as well CentOS Clients When you access the website on Linux server this will not be trusted as well [root@client ~]# curl https://docs.sandbox.io curl: (60) Peers Certificate issuer is not recognized. More details here: http://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isnt adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If youd like to turn off curls verification of the certificate, use the -k (or --insecure) option. Transfer the root CA certificate file to client machine [root@auth ca]# scp root-ca/certs/ca.crt client:~/sandbox-root-ca.crt root@docs password: ca.crt 100% 2017 8.7MB/s 00:00 Now access the client machine and move the certificate file to /etc/pki/ca-trust/source/anchors/ then perform the update-ca-trust command [root@client ~]# ls initial-puppet-log.txt sandbox-root-ca.crt [root@client ~]# cp sandbox-root-ca.crt /etc/pki/ca-trust/source/anchors/ [root@client ~]# update-ca-trust [root@client ~]# curl https://docs.sandbox.io | tail % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 9377 100 9377 0 0 118k 0 --:--:-- --:--:-- --:--:-- 118k }; </script> </body> </html> <!-- MkDocs version : 1.2.3 Build Date UTC : 2021-12-05 18:08:53.168393+00:00 -->","title":"Authority"},{"location":"auth/#certificate-authority-server","text":"The CA Server will act as a root CA and intermediate CA repository and will create, sign, and validate all my PKI(Public Key Infrastructure) in the environment. I am not doing any automation in this server. Also, I am creating this server since im done looking my internal homelab having a strikethrough on https whenever I browse the sites i am maintaining. I just want the feel that i am doing this outside on my local area network :P","title":"Certificate Authority Server"},{"location":"auth/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS authority.sandbox.io. @ IN A 10.11.100.114 authority IN A 10.11.100.114 auth IN CNAME authority Reverse lookup zone @ IN NS authority.sandbox.io. authority IN A 10.11.100.114 114 IN PTR authority.sandbox.io. DHCP entry host authority { hardware ethernet 1E:13:68:37:74:28; fixed-address 10.11.100.114; }","title":"DHCP and DNS Entries"},{"location":"auth/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"auth/#installation-and-directory-management","text":"Install openssl and tree if is not installed, though tree is optional only if you want to get the idea on your infrastructure [root@auth ~]# yum -y install openssl tree -y Loaded plugins: fastestmirror, rhnplugin This system is receiving updates from RHN Classic or Red Hat Satellite. Loading mirror speeds from cached hostfile Package 1:openssl-1.0.2k-22.el7_9.x86_64 already installed and latest version Package tree-1.6.0-10.el7.x86_64 already installed and latest version Nothing to do We need to create root(root-ca), intermediate(sub-ca) and server directories, with certs, crl, csr, newcerts, private sub-directories. root-ca: this is our top level authority for issuing certificates, it will only issuing to intermediate(sub-ca) sub-ca: since if we only signed certificates via root-ca on real life, it is too much risk as they are much valuable. so to insulate the root CAs we created a sub-ca to signed server certificates on behalf of root-ca. server: directory for our whole environment of servers for certificate signing request certs: crt files that had been signed or self signed. private: private key files csr: certificate signing request, this is the location of csr files for sub-ca or root-ca signing. crl: certificate revocation list, location for all revoke certificates. Note: I am only emulating things and doing this in one server instead of a hierarchy of servers for root CA, intermediate CA, and so on, to build trust relationships. [root@auth ~]# mkdir -p ca/{root-ca,sub-ca,server}/{private,certs,newcerts,crl,csr} [root@auth ~]# tree ca/ ca/ \u251c\u2500\u2500 root-ca \u2502 \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 crl \u2502 \u251c\u2500\u2500 csr \u2502 \u251c\u2500\u2500 newcerts \u2502 \u2514\u2500\u2500 private \u251c\u2500\u2500 server \u2502 \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 crl \u2502 \u251c\u2500\u2500 csr \u2502 \u251c\u2500\u2500 newcerts \u2502 \u2514\u2500\u2500 private \u2514\u2500\u2500 sub-ca \u251c\u2500\u2500 certs \u251c\u2500\u2500 crl \u251c\u2500\u2500 csr \u251c\u2500\u2500 newcerts \u2514\u2500\u2500 private All private directories should be readable to user. [root@auth ca]# chmod -v 700 {root-ca,sub-ca,server}/private mode of \u2018root-ca/private\u2019 changed from 0755 (rwxr-xr-x) to 0700 (rwx------) mode of \u2018sub-ca/private\u2019 changed from 0755 (rwxr-xr-x) to 0700 (rwx------) mode of \u2018server/private\u2019 changed from 0755 (rwxr-xr-x) to 0700 (rwx------) Create an index file for root-ca and sub-ca directory touch {root-ca,sub-ca}/index the directories also needs a serial file, we can generate a random hexadecimal 16 bit output via openssl rand -hex 16 openssl rand -hex 16 > root-ca/serial openssl rand -hex 16 > sub-ca/serial","title":"Installation and directory management"},{"location":"auth/#creation-of-private-keys","text":"Now we want to generate the 3 private keys for root-ca, sub-ca, and server directory. The server private key is only openssl genrsa -out server/private/server.key 2048 as i dont want to put a passphrase on the server and manually typing the pass phrase whenever i restart nginx or httpd service. [root@auth ca]# openssl genrsa -aes256 -out root-ca/private/ca.key 4096 Generating RSA private key, 4096 bit long modulus .............................++ .....................................................................................++ e is 65537 (0x10001) Enter pass phrase for root-ca/private/ca.key: Verifying - Enter pass phrase for root-ca/private/ca.key: [root@auth ca]# openssl genrsa -aes256 -out sub-ca/private/sub-ca.key 4096 Generating RSA private key, 4096 bit long modulus ...................................................................................................................................................................++ .....................................................++ e is 65537 (0x10001) Enter pass phrase for sub-ca/private/sub-ca.key: Verifying - Enter pass phrase for sub-ca/private/sub-ca.key: [root@auth ca]# openssl genrsa -out server/private/server.key 2048 Generating RSA private key, 2048 bit long modulus ........................................+++ ...............+++ e is 65537 (0x10001) Bit strength of root-ca and sub-ca private key is implemented to use 4096 for us to have a strongest protection we can, we only use 2048 on server as this may be too much overhead on servers if it using a lot of web traffic","title":"Creation of private keys"},{"location":"auth/#rootca-certificate-configuration","text":"Now create a configuration file for root-ca, this will be used for signing the self signed certificate of rootca #root-ca/root-ca.conf [ca] default_ca = CA_default [CA_default] dir = /root/ca/root-ca certs = $dir/certs crl_dir = $dir/crl new_certs_dir = $dir/newcerts database = $dir/index serial = $dir/serial RANDFILE = $dir/private/.rand private_key = $dir/private/ca.key certificate = $dir/certs/ca.crt crlnumber = $dir/crlnumber crl = $dir/crl/ca.crl crl_extensions = crl_ext default_crl_days = 30 default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = 365 preserve = no policy = policy_strict [ policy_strict ] countryName = supplied stateOrProvinceName = supplied organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] default_bits = 2048 distinguished_name = req_distinguished_name string_mask = utf8only default_md = sha256 x509_extensions = v3_ca [ req_distinguished_name ] countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address countryName_default = PH stateOrProvinceName_default = Cavite 0.organizationName_default = Sandbox [ v3_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \"OpenSSL Generated Server Certificate\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth Create the root certificate file by using openssl req -config root-ca.conf -key private/ca.key -new -x509 -days 3650 -sha256 -extensions v3_ca -out certs/ca.crt -config: use this config instead of openssl.cnf file located in /etc/pki/tls/ -key: location of private key -days: how long the public cert will last before expiring -x509: create the certificate in x509 format -sha256: message digest format -extensions: use the v3_ca function located on root-ca.conf -out: output location where the certificate will be created The command will output similar below: [root@auth ca]# cd root-ca/ [root@auth root-ca]# openssl req -config root-ca.conf -key private/ca.key -new -x509 -days 3650 -sha256 -extensions v3_ca -out certs/ca.crt Enter pass phrase for private/ca.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. Country Name (2 letter code) [PH]:PH State or Province Name [Cavite]:Cavite Locality Name []:Bacoor Organization Name [Sandbox]: Organizational Unit Name []:IT Common Name []:ROOTCA Email Address []: [root@auth root-ca]# You can verify your certificate file using openssl x509 -in certs/ca.crt -noout -text [root@auth root-ca]# openssl x509 -in certs/ca.crt -noout -text Certificate: Data: Version: 3 (0x2) Serial Number: f3:ac:8e:98:a4:5c:38:bc Signature Algorithm: sha256WithRSAEncryption Issuer: C=PH, ST=Cavite, L=Bacoor, O=Sandbox, OU=IT, CN=ROOTCA Validity Not Before: Dec 5 16:16:02 2021 GMT Not After : Dec 3 16:16:02 2031 GMT Subject: C=PH, ST=Cavite, L=Bacoor, O=Sandbox, OU=IT, CN=ROOTCA Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus:","title":"RootCA certificate configuration"},{"location":"auth/#intermediate-ca-csr-and-configuration","text":"We need also to create a separate ca configuration file for sub-ca directory #sub-ca.conf [ca] default_ca = CA_default [CA_default] # Change this to your sub-ca directory dir = /root/ca/sub-ca certs = $dir/certs crl_dir = $dir/crl new_certs_dir = $dir/newcerts database = $dir/index serial = $dir/serial RANDFILE = $dir/private/.rand private_key = $dir/private/sub-ca.key certificate = $dir/certs/sub-ca.crt crlnumber = $dir/crlnumber crl = $dir/crl/ca.crl crl_extensions = crl_ext default_crl_days = 30 default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = 365 preserve = no policy = policy_loose [ policy_strict ] countryName = supplied stateOrProvinceName = supplied organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] default_bits = 2048 distinguished_name = req_distinguished_name string_mask = utf8only default_md = sha256 x509_extensions = v3_ca [ req_distinguished_name ] countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address countryName_default = PH stateOrProvinceName_default = Cavite 0.organizationName_default = Sandbox [ v3_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \"OpenSSL Generated Server Certificate\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth # This is important to change for chrome browser not to tag it as insecure subjectAltName = @alt_names [ alt_names ] DNS.1 = docs.sandbox.io DNS.2 = devdocs.sandbox.io We need to create a certificate signing request on the sub-ca directory for the root ca to sign it and create a certificate file. You can use openssl req -config sub-ca.conf -new -key private/sub-ca.key -sha256 -out csr/sub-ca.csr [root@auth sub-ca]# openssl req -config sub-ca.conf -new -key private/sub-ca.key -sha256 -out csr/sub-ca.csr Enter pass phrase for private/sub-ca.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [PH]:PH State or Province Name [Cavite]: Locality Name []: Organization Name [Sandbox]: Organizational Unit Name []:IT Common Name []:authority.sandbox.io Email Address []:root@sandbox.io Move to the root-ca directory and create the sub-ca certificate file using its certificate signing request, use openssl ca -config root-ca.conf -extensions v3_intermediate_ca -days 1825 -notext -in ../sub-ca/csr/sub-ca.csr -out ../sub-ca/certs/sub-ca.crt [root@auth sub-ca]# cd ../root-ca/ [root@auth root-ca]# openssl ca -config root-ca.conf -extensions v3_intermediate_ca -days 1825 -notext -in ../sub-ca/csr/sub-ca.csr -out ../sub-ca/certs/sub-ca.crt Using configuration from root-ca.conf Enter pass phrase for /root/ca/root-ca/private/ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: d1:a1:ba:cc:a3:80:57:1b:55:8f:ff:e2:61:59:eb:15 Validity Not Before: Dec 5 16:56:56 2021 GMT Not After : Dec 4 16:56:56 2026 GMT Subject: countryName = PH stateOrProvinceName = Cavite organizationName = Sandbox organizationalUnitName = IT commonName = authority.sandbox.io emailAddress = root@sandbox.io X509v3 extensions: X509v3 Subject Key Identifier: D4:02:04:89:96:B7:51:04:99:0C:62:42:1D:DB:75:55:AB:E8:73:78 X509v3 Authority Key Identifier: keyid:E1:A0:12:4D:CF:D9:24:BD:B8:F0:D7:BC:84:FE:A4:0D:11:28:42:58 X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Certificate is to be certified until Dec 4 16:56:56 2026 GMT (1825 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated Once it is done, it will also create a pem file which is the same with the cert file we created for sub-ca, index file is also updated [root@auth ca]# cat root-ca/index V 261204165656Z D1A1BACCA380571B558FFFE26159EB15 unknown /C=PH/ST=Cavite/O=Sandbox/OU=IT/CN=authority.sandbox.io/emailAddress=root@sandbox.io [root@auth ca]# tree . \u251c\u2500\u2500 root-ca \u2502 \u251c\u2500\u2500 certs \u2502 \u2502 \u2514\u2500\u2500 ca.crt \u2502 \u251c\u2500\u2500 crl \u2502 \u251c\u2500\u2500 csr \u2502 \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 index.attr \u2502 \u251c\u2500\u2500 index.old \u2502 \u251c\u2500\u2500 newcerts \u2502 \u2502 \u2514\u2500\u2500 D1A1BACCA380571B558FFFE26159EB15.pem # this file \u2502 \u251c\u2500\u2500 private \u2502 \u2502 \u2514\u2500\u2500 ca.key \u2502 \u251c\u2500\u2500 root-ca.conf \u2502 \u251c\u2500\u2500 serial \u2502 \u2514\u2500\u2500 serial.old \u2514\u2500\u2500 sub-ca \u251c\u2500\u2500 certs \u2502 \u2514\u2500\u2500 sub-ca.crt # is the same with this file \u251c\u2500\u2500 crl \u251c\u2500\u2500 csr \u2502 \u2514\u2500\u2500 sub-ca.csr \u251c\u2500\u2500 index \u251c\u2500\u2500 newcerts \u251c\u2500\u2500 private \u2502 \u2514\u2500\u2500 sub-ca.key \u251c\u2500\u2500 serial \u2514\u2500\u2500 sub-ca.conf","title":"Intermediate CA CSR and configuration"},{"location":"auth/#server-csr-signing-and-configuration","text":"Now create a certificate signing request for each servers on your environment that needs SSL, on my end, i will use it for my docs.sandbox.io website: openssl req -key private/server.key -new -sha256 -out csr/docs.csr [root@auth server]# openssl req -key private/server.key -new -sha256 -out csr/docs.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:PH State or Province Name (full name) []:Cavite Locality Name (eg, city) [Default City]:Dasma Organization Name (eg, company) [Default Company Ltd]:Playground Organizational Unit Name (eg, section) []:Finance Common Name (eg, your name or your servers hostname) []:docs.sandbox.io Email Address []:deve@sandbox.io Please enter the following \"extra\" attributes # Note: You can enter \"extra\" as password or you can also leave it as blank to be sent with your certificate request A challenge password []: An optional company name []: This can be done on the server itself but to make it centralize I did it on this server. Sign the certificate using the sub-ca certificate, use: openssl ca -config sub-ca.conf -extensions server_cert -days 365 -notext -in ../server/csr/docs1.csr -out ../server/certs/docs1.crt [root@auth sub-ca]# openssl ca -config sub-ca.conf -extensions server_cert -days 365 -notext -in ../server/csr/docs.csr -out . ./server/certs/docs.crt Using configuration from sub-ca.conf Enter pass phrase for /root/ca/sub-ca/private/sub-ca.key: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 8f:1f:da:80:c0:1e:c6:21:ca:32:1b:0b:11:c7:0b:e7 Validity Not Before: Dec 5 17:26:36 2021 GMT Not After : Dec 5 17:26:36 2022 GMT Subject: countryName = PH stateOrProvinceName = Cavite localityName = Dasma organizationName = Playground organizationalUnitName = Finance commonName = docs.sandbox.io emailAddress = deve@sandbox.io X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Cert Type: SSL Server Netscape Comment: OpenSSL Generated Server Certificate X509v3 Subject Key Identifier: E3:A4:49:8B:37:7D:11:BA:2C:7B:B8:3D:70:27:F5:C9:F7:2D:8D:58 X509v3 Authority Key Identifier: keyid:D4:02:04:89:96:B7:51:04:99:0C:62:42:1D:DB:75:55:AB:E8:73:78 DirName:/C=PH/ST=Cavite/L=Bacoor/O=Sandbox/OU=IT/CN=ROOTCA serial:D1:A1:BA:CC:A3:80:57:1B:55:8F:FF:E2:61:59:EB:15 X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Subject Alternative Name: DNS:docs.sandbox.io, DNS:devdocs.sandbox.io Certificate is to be certified until Dec 5 17:26:36 2022 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated You can verify the certificate file using: openssl x509 -in certs/docs.crt -noout -text [root@auth sub-ca]# cd ../server/ [root@auth server]# openssl x509 -in certs/docs.crt -noout -text Certificate: Data: Version: 3 (0x2) Serial Number: 8f:1f:da:80:c0:1e:c6:21:ca:32:1b:0b:11:c7:0b:e7 Signature Algorithm: sha256WithRSAEncryption Issuer: C=PH, ST=Cavite, O=Sandbox, OU=IT, CN=authority.sandbox.io/emailAddress=root@sandbox.io Validity Not Before: Dec 5 17:26:36 2021 GMT Not After : Dec 5 17:26:36 2022 GMT Subject: C=PH, ST=Cavite, L=Dasma, O=Playground, OU=Finance, CN=docs.sandbox.io/emailAddress=deve@sandbox.io Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit)","title":"Server CSR signing and configuration"},{"location":"auth/#configuration-to-other-servers","text":"","title":"Configuration to other servers"},{"location":"auth/#transferring-of-files","text":"Transfer the certificate files to docs.sandbox.io, since i am using nginx i will concatenate docs.crt and sub-ca.crt together since this is needed for the client to verify its intermediate certificate authority [root@auth certs]# cat certs/docs.crt ../../sub-ca/certs/sub-ca.crt > certs/docs-chained.crt [root@auth server]# scp certs/docs-chained.crt docs:/etc/pki/tls/certs/docs.crt root@docs password: docs.crt 100% 2143 7.7MB/s 00:00 [root@auth server]# scp private/server.key docs:/etc/pki/tls/private/docs.key root@docs password: server.key 100% 1679 5.8MB/s 00:00","title":"Transferring of files"},{"location":"auth/#nginx-configuration","text":"On docs server configure it to use HTTPS on /etc/nginx/nginx.conf server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name docs.sandbox.io; root /opt/sites/docs-site; ssl_certificate \"/etc/pki/tls/certs/docs.crt\"; ssl_certificate_key \"/etc/pki/tls/private/docs.key\"; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } Restart nginx service systemctl restart nginx","title":"Nginx configuration"},{"location":"auth/#client-trust-configuration","text":"","title":"Client Trust configuration"},{"location":"auth/#on-windowsgoogle-chrome","text":"Since that our root certificate file is not trusted by the clients that will access the website(unlike digicert or entrust), we need to transfer the certificate file on each clients Navigate to Privacy and Security in chrome://settings, and click security to open the tab. browse to Advance and click \"Manage Certificates\", Certificate tab will appear. Click Trusted Root Certificate Authorities. Import the certificate and click yes to the Security Warning notification You can now have a verified connection on your website, this can be done on firefox as well","title":"On Windows(Google Chrome)"},{"location":"auth/#centos-clients","text":"When you access the website on Linux server this will not be trusted as well [root@client ~]# curl https://docs.sandbox.io curl: (60) Peers Certificate issuer is not recognized. More details here: http://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isnt adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If youd like to turn off curls verification of the certificate, use the -k (or --insecure) option. Transfer the root CA certificate file to client machine [root@auth ca]# scp root-ca/certs/ca.crt client:~/sandbox-root-ca.crt root@docs password: ca.crt 100% 2017 8.7MB/s 00:00 Now access the client machine and move the certificate file to /etc/pki/ca-trust/source/anchors/ then perform the update-ca-trust command [root@client ~]# ls initial-puppet-log.txt sandbox-root-ca.crt [root@client ~]# cp sandbox-root-ca.crt /etc/pki/ca-trust/source/anchors/ [root@client ~]# update-ca-trust [root@client ~]# curl https://docs.sandbox.io | tail % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 9377 100 9377 0 0 118k 0 --:--:-- --:--:-- --:--:-- 118k }; </script> </body> </html> <!-- MkDocs version : 1.2.3 Build Date UTC : 2021-12-05 18:08:53.168393+00:00 -->","title":"CentOS Clients"},{"location":"backup/","text":"Backup The Backup Server is a bacula server connected to the storage server DHCP and DNS Entries Forward lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 Reverse lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 107 IN PTR backup.sandbox.io. DHCP entry host backup { hardware ethernet 7A:B2:FA:F6:44:AB; fixed-address 10.11.100.107; } Process bconsole -> bacula-dir(9101) -> bacula-fd(9102) -> bacula-sd(9103) -> physical media bacula-dir -> SQL DBMS(Catalogs) bacula-fd is the important to open the port in client if you will install all functions in one server Installation and Setup Initial Setup Install necessary applications - name: install targetcli and iscsi-initiator-utils yum: name: - iscsi-initiator-utils - bacula-director - bacula-storage - bacula-console - bacula-client - postgresql - postgresql-server state: present Open ports for bacula-dir(9101) and bacula-sd(9103) - name: open ports on firewalld firewalld: port: '{{item}}' immediate: true permanent: true state: enabled with_items: - '9101/tcp' - '9103/tcp' Setup iscsi initiator to connect to LUN storage - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started enabled: true - name: discover the LUN share open_iscsi: show_nodes: true portal: 'storage.sandbox.io' discover: true - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage' Mounting the shares Add directory where the NFS and LUN storage will mount to and add context - name: create directory for bacula configuration file: path: '{{item}}' state: directory owner: bacula group: bacula with_items: - '/bacula' - '/bacula/LUN' - '/bacula/NFS' - '/etc/bacula/conf.d' - name: add fcontext to bacula directory for backup to run sefcontext: target: '/bacula(/.*)?' setype: bacula_store_t state: present - name: set use_nfs_home_dirs to true seboolean: name: use_nfs_home_dirs state: true persistent: true - name: restorecon the bacula directory shell: cmd: restorecon -Rv /bacula Create systemd files for mounting LUN and NFS - name: moved storage systemd files to /etc/systemd/system directory copy: src: 'files/backup/systemd/{{item}}' dest: /etc/systemd/system with_items: - 'bacula-LUN.mount' - 'bacula-LUN.automount' - 'bacula-NFS.automount' - 'bacula-NFS.mount' register: copy_files - name: systemd reload systemd: daemon_reload: true when: copy_files.changed == true Naming conventions of the systemd mount files should correlate where you want to mount the shared storage. For example we want to mount the NFS folder to /bacula/NFS, we need to create a systemd file that named bacula-NFS.mount Contents of bacula-LUN.mount [Unit] Description=Storage LUN mount for itself After=network.service NetworkManager.service [Mount] What=/dev/sdb1 Where=/bacula/LUN Type=xfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-LUN.automount [Unit] Description=Storage LUN automount for itself After=network.service NetworkManager.service [Automount] Where=/bacula/LUN [Install] WantedBy=multi-user.target Contents of bacula-NFS.mount [Unit] Description=Storage NFS mount for itself After=network.service NetworkManager.service [Mount] What=storage.sandbox.io:/var/nfs Where=/bacula/NFS Type=nfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-NFS.automount [Unit] Description=Storage NFS automount for itself After= network.service NetworkManager.service [Automount] Where=/bacula/NFS [Install] WantedBy=multi-user.target Bacula Setup Setup bacula database. - name: stat if bacula_sql_setup exists stat: path: /home/users/bot-acc/.system-checks/bacula_sql_setup register: bacula_sql_setup - name: execute commands for bacula database shell: cmd: '{{item}}' with_items: - sudo -Hiu postgres /usr/libexec/bacula/create_postgresql_database - sudo -Hiu postgres /usr/libexec/bacula/make_postgresql_tables - sudo -Hiu postgres /usr/libexec/bacula/grant_postgresql_privileges - sudo -Hiu postgres psql -U postgres -c \"ALTER DATABASE bacula OWNER TO bacula\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER bacula WITH PASSWORD 'Password123'\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER postgres WITH PASSWORD 'Password321'\" - touch /home/users/bot-acc/.system-checks/bacula_sql_setup when: not bacula_sql_setup.stat.exists Copy the files - name: replace files on /etc/bacula copy: src: 'files/backup/bacula-conf/{{item}}' dest: '/etc/bacula' with_items: - bacula-dir.conf - bacula-sd.conf - bacula-fd.conf - bconsole.conf - name: synchronized files in conf.d of /etc/bacula synchronize: src: 'files/backup/bacula-conf/nodes/' dest: '/etc/bacula/conf.d' Bacula Director Configuration Define Director function Director { Name = bacula-dir DIRport = 9101 QueryFile = \"/etc/bacula/query.sql\" WorkingDirectory = \"/var/spool/bacula\" PidDirectory = \"/var/run\" Maximum Concurrent Jobs = 1 Password = \"Password123!\" # Password for bconsole connection Messages = Daemon DirAddress = 127.0.0.1 # Director is installed in localhost } JobDefs is defined definition for jobs created later JobDefs { Name = \"DefaultJob\" Type = Backup Level = Incremental Client = Backup-Server #Move to Jobs FileSet = \"Full Set\" #Schedule = \"WeeklyCycle\" Storage = File-LUN Messages = Standard Pool = File Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } JobDefs { Name = \"RemoteDefault\" Type = Backup Level = Incremental FileSet = \"FileSet-Sandbox\" #Schedule = \"WeeklyCycle\" Storage = File-NFS Messages = Standard Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } Define Jobs to be run for backup Job { Name = \"Backup-Main\" JobDefs = \"DefaultJob\" } Job { Name = \"BackupCatalog\" JobDefs = \"DefaultJob\" Level = Full FileSet=\"Catalog\" #Schedule = \"WeeklyCycleAfterBackup\" RunBeforeJob = \"/usr/libexec/bacula/make_catalog_backup.pl MyCatalog\" RunAfterJob = \"/usr/libexec/bacula/delete_catalog_backup\" Write Bootstrap = \"/var/spool/bacula/%n.bsr\" Priority = 11 # run after main backup } Define Jobs to be run for restore Job { Name = \"Restore-Backup\" Type = Restore Client= Backup-Server FileSet=\"Full Set\" Storage = File-LUN Pool = Default Messages = Standard Where = /tmp/bacula-restore } Job { Name = \"Restore-Remote\" Type = Restore Client= DEV2 FileSet = \"FileSet-Sandbox\" Storage = File-NFS Pool = Default Messages = Standard Where = /tmp/bacula-restore } FileSet define what to include and exclude FileSet { Name = \"Full Set\" Include { Options { signature = MD5 compression = GZIP } File = / } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } FileSet { Name = \"Catalog\" Include { Options { signature = MD5 } File = \"/var/spool/bacula/bacula.sql\" } } FileSet { Name = \"FileSet-Sandbox\" Include { Options { signature = MD5 compression = GZIP } File = /home File = /etc File = /opt File = /var } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } Schedule define the time to run the backup, will not use it but good to be included Schedule { Name = \"WeeklyCycle\" Run = Full 1st sun at 23:05 Run = Differential 2nd-5th sun at 23:05 Run = Incremental mon-sat at 23:05 } Schedule { Name = \"WeeklyCycleAfterBackup\" Run = Full sun-sat at 23:10 } Storage define where the backup/restore location is, this is the connector to bacula-sd Storage { Name = File-LUN Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = LUNStorage Media Type = File } Storage { Name = File-NFS Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = NFSStorage Media Type = File } Define the Pool function Pool { Name = Default Pool Type = Backup Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days } Pool { Name = File Pool Type = Backup Label Format = BACKUP-Bacula- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G reasonable Maximum Volumes = 100 } Pool { Name = Scratch Pool Type = Backup } Pool { Name = Pool-DEV2 Pool Type = Backup Label Format = BACKUP-DEV2- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G g reasonable Maximum Volumes = 100 } Catalog function defines the database authentication Catalog { Name = MyCatalog dbname = \"bacula\"; dbuser = \"bacula\"; dbpassword = \"Password123\" } Not entirely sure on what console does as of the moment but will check on how Console { Name = bacula-mon Password = \"@@MON_DIR_PASSWORD@@\" CommandACL = status, .status } Client defines all the nodes that will need to be backup Client { Name = Backup-Server Address = backup.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Client { Name = @@CHANGE_THIS@@ Address = fqdn.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" n File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Add the following line in the config file for your config file to search created configurations in conf.d directory @|\"find /etc/bacula/conf.d -name '*.conf' -type f -exec echo @{} \\;\" Check your work bacula-dir -tc /etc/bacula/bacula-dir.conf Bacula Storage Daemon Configuration bacula-sd.conf will define all configuration for bacula-sd service Define Storage function Storage { Name = bacula-sd SDPort = 9103 WorkingDirectory = \"/var/spool/bacula\" Pid Directory = \"/var/run\" Maximum Concurrent Jobs = 20 SDAddress = backup.sandbox.io } Define Director authentication Director { Name = bacula-dir Password = \"Password123!\" } Device directive will choose where you want to put your files Device { Name = LUNStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled med ia Archive Device = /bacula/LUN Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Device { Name = NFSStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled me$ ia Archive Device = /bacula/NFS Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Check your configuration bacula-sd -tc /etc/bacula/bacula-sd.conf Bacula File Daemon(Client) Configuration Define director function for authentication on file daemon Director { Name = bacula-dir Password = \"Password123!\" } This is the only directives that is needed to change, else is optional","title":"Backup"},{"location":"backup/#backup","text":"The Backup Server is a bacula server connected to the storage server","title":"Backup"},{"location":"backup/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 Reverse lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 107 IN PTR backup.sandbox.io. DHCP entry host backup { hardware ethernet 7A:B2:FA:F6:44:AB; fixed-address 10.11.100.107; } Process bconsole -> bacula-dir(9101) -> bacula-fd(9102) -> bacula-sd(9103) -> physical media bacula-dir -> SQL DBMS(Catalogs) bacula-fd is the important to open the port in client if you will install all functions in one server","title":"DHCP and DNS Entries"},{"location":"backup/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"backup/#initial-setup","text":"Install necessary applications - name: install targetcli and iscsi-initiator-utils yum: name: - iscsi-initiator-utils - bacula-director - bacula-storage - bacula-console - bacula-client - postgresql - postgresql-server state: present Open ports for bacula-dir(9101) and bacula-sd(9103) - name: open ports on firewalld firewalld: port: '{{item}}' immediate: true permanent: true state: enabled with_items: - '9101/tcp' - '9103/tcp' Setup iscsi initiator to connect to LUN storage - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started enabled: true - name: discover the LUN share open_iscsi: show_nodes: true portal: 'storage.sandbox.io' discover: true - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage'","title":"Initial Setup"},{"location":"backup/#mounting-the-shares","text":"Add directory where the NFS and LUN storage will mount to and add context - name: create directory for bacula configuration file: path: '{{item}}' state: directory owner: bacula group: bacula with_items: - '/bacula' - '/bacula/LUN' - '/bacula/NFS' - '/etc/bacula/conf.d' - name: add fcontext to bacula directory for backup to run sefcontext: target: '/bacula(/.*)?' setype: bacula_store_t state: present - name: set use_nfs_home_dirs to true seboolean: name: use_nfs_home_dirs state: true persistent: true - name: restorecon the bacula directory shell: cmd: restorecon -Rv /bacula Create systemd files for mounting LUN and NFS - name: moved storage systemd files to /etc/systemd/system directory copy: src: 'files/backup/systemd/{{item}}' dest: /etc/systemd/system with_items: - 'bacula-LUN.mount' - 'bacula-LUN.automount' - 'bacula-NFS.automount' - 'bacula-NFS.mount' register: copy_files - name: systemd reload systemd: daemon_reload: true when: copy_files.changed == true Naming conventions of the systemd mount files should correlate where you want to mount the shared storage. For example we want to mount the NFS folder to /bacula/NFS, we need to create a systemd file that named bacula-NFS.mount Contents of bacula-LUN.mount [Unit] Description=Storage LUN mount for itself After=network.service NetworkManager.service [Mount] What=/dev/sdb1 Where=/bacula/LUN Type=xfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-LUN.automount [Unit] Description=Storage LUN automount for itself After=network.service NetworkManager.service [Automount] Where=/bacula/LUN [Install] WantedBy=multi-user.target Contents of bacula-NFS.mount [Unit] Description=Storage NFS mount for itself After=network.service NetworkManager.service [Mount] What=storage.sandbox.io:/var/nfs Where=/bacula/NFS Type=nfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-NFS.automount [Unit] Description=Storage NFS automount for itself After= network.service NetworkManager.service [Automount] Where=/bacula/NFS [Install] WantedBy=multi-user.target","title":"Mounting the shares"},{"location":"backup/#bacula-setup","text":"Setup bacula database. - name: stat if bacula_sql_setup exists stat: path: /home/users/bot-acc/.system-checks/bacula_sql_setup register: bacula_sql_setup - name: execute commands for bacula database shell: cmd: '{{item}}' with_items: - sudo -Hiu postgres /usr/libexec/bacula/create_postgresql_database - sudo -Hiu postgres /usr/libexec/bacula/make_postgresql_tables - sudo -Hiu postgres /usr/libexec/bacula/grant_postgresql_privileges - sudo -Hiu postgres psql -U postgres -c \"ALTER DATABASE bacula OWNER TO bacula\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER bacula WITH PASSWORD 'Password123'\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER postgres WITH PASSWORD 'Password321'\" - touch /home/users/bot-acc/.system-checks/bacula_sql_setup when: not bacula_sql_setup.stat.exists Copy the files - name: replace files on /etc/bacula copy: src: 'files/backup/bacula-conf/{{item}}' dest: '/etc/bacula' with_items: - bacula-dir.conf - bacula-sd.conf - bacula-fd.conf - bconsole.conf - name: synchronized files in conf.d of /etc/bacula synchronize: src: 'files/backup/bacula-conf/nodes/' dest: '/etc/bacula/conf.d'","title":"Bacula Setup"},{"location":"backup/#bacula-director-configuration","text":"Define Director function Director { Name = bacula-dir DIRport = 9101 QueryFile = \"/etc/bacula/query.sql\" WorkingDirectory = \"/var/spool/bacula\" PidDirectory = \"/var/run\" Maximum Concurrent Jobs = 1 Password = \"Password123!\" # Password for bconsole connection Messages = Daemon DirAddress = 127.0.0.1 # Director is installed in localhost } JobDefs is defined definition for jobs created later JobDefs { Name = \"DefaultJob\" Type = Backup Level = Incremental Client = Backup-Server #Move to Jobs FileSet = \"Full Set\" #Schedule = \"WeeklyCycle\" Storage = File-LUN Messages = Standard Pool = File Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } JobDefs { Name = \"RemoteDefault\" Type = Backup Level = Incremental FileSet = \"FileSet-Sandbox\" #Schedule = \"WeeklyCycle\" Storage = File-NFS Messages = Standard Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } Define Jobs to be run for backup Job { Name = \"Backup-Main\" JobDefs = \"DefaultJob\" } Job { Name = \"BackupCatalog\" JobDefs = \"DefaultJob\" Level = Full FileSet=\"Catalog\" #Schedule = \"WeeklyCycleAfterBackup\" RunBeforeJob = \"/usr/libexec/bacula/make_catalog_backup.pl MyCatalog\" RunAfterJob = \"/usr/libexec/bacula/delete_catalog_backup\" Write Bootstrap = \"/var/spool/bacula/%n.bsr\" Priority = 11 # run after main backup } Define Jobs to be run for restore Job { Name = \"Restore-Backup\" Type = Restore Client= Backup-Server FileSet=\"Full Set\" Storage = File-LUN Pool = Default Messages = Standard Where = /tmp/bacula-restore } Job { Name = \"Restore-Remote\" Type = Restore Client= DEV2 FileSet = \"FileSet-Sandbox\" Storage = File-NFS Pool = Default Messages = Standard Where = /tmp/bacula-restore } FileSet define what to include and exclude FileSet { Name = \"Full Set\" Include { Options { signature = MD5 compression = GZIP } File = / } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } FileSet { Name = \"Catalog\" Include { Options { signature = MD5 } File = \"/var/spool/bacula/bacula.sql\" } } FileSet { Name = \"FileSet-Sandbox\" Include { Options { signature = MD5 compression = GZIP } File = /home File = /etc File = /opt File = /var } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } Schedule define the time to run the backup, will not use it but good to be included Schedule { Name = \"WeeklyCycle\" Run = Full 1st sun at 23:05 Run = Differential 2nd-5th sun at 23:05 Run = Incremental mon-sat at 23:05 } Schedule { Name = \"WeeklyCycleAfterBackup\" Run = Full sun-sat at 23:10 } Storage define where the backup/restore location is, this is the connector to bacula-sd Storage { Name = File-LUN Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = LUNStorage Media Type = File } Storage { Name = File-NFS Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = NFSStorage Media Type = File } Define the Pool function Pool { Name = Default Pool Type = Backup Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days } Pool { Name = File Pool Type = Backup Label Format = BACKUP-Bacula- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G reasonable Maximum Volumes = 100 } Pool { Name = Scratch Pool Type = Backup } Pool { Name = Pool-DEV2 Pool Type = Backup Label Format = BACKUP-DEV2- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G g reasonable Maximum Volumes = 100 } Catalog function defines the database authentication Catalog { Name = MyCatalog dbname = \"bacula\"; dbuser = \"bacula\"; dbpassword = \"Password123\" } Not entirely sure on what console does as of the moment but will check on how Console { Name = bacula-mon Password = \"@@MON_DIR_PASSWORD@@\" CommandACL = status, .status } Client defines all the nodes that will need to be backup Client { Name = Backup-Server Address = backup.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Client { Name = @@CHANGE_THIS@@ Address = fqdn.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" n File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Add the following line in the config file for your config file to search created configurations in conf.d directory @|\"find /etc/bacula/conf.d -name '*.conf' -type f -exec echo @{} \\;\" Check your work bacula-dir -tc /etc/bacula/bacula-dir.conf","title":"Bacula Director Configuration"},{"location":"backup/#bacula-storage-daemon-configuration","text":"bacula-sd.conf will define all configuration for bacula-sd service Define Storage function Storage { Name = bacula-sd SDPort = 9103 WorkingDirectory = \"/var/spool/bacula\" Pid Directory = \"/var/run\" Maximum Concurrent Jobs = 20 SDAddress = backup.sandbox.io } Define Director authentication Director { Name = bacula-dir Password = \"Password123!\" } Device directive will choose where you want to put your files Device { Name = LUNStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled med ia Archive Device = /bacula/LUN Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Device { Name = NFSStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled me$ ia Archive Device = /bacula/NFS Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Check your configuration bacula-sd -tc /etc/bacula/bacula-sd.conf","title":"Bacula Storage Daemon Configuration"},{"location":"backup/#bacula-file-daemonclient-configuration","text":"Define director function for authentication on file daemon Director { Name = bacula-dir Password = \"Password123!\" } This is the only directives that is needed to change, else is optional","title":"Bacula File Daemon(Client) Configuration"},{"location":"bot/","text":"Automation Server The Servers FQDN will be bot.sandbox.io, for this one i dont want to lenghten all of my servers FQDN. DHCP and DNS Entries Forward lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 Reverse lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 105 IN PTR bot.sandbox.io. DHCP entry host bot { hardware ethernet 9A:5D:A5:C2:A3:25; fixed-address 10.11.100.105; } Initial Phase vimrc file setup .vimrc on the root account and copy the file to /etc/skel so all accounts created moving forward has the same configuration, this would be helpful as we would use ansible later set tabstop=4 set shiftwidth=4 set expandtab set number set smarttab set autoindent Firewall Ensure firewall port(8140) for puppet server is open. firewall-cmd --add-port=8140/tcp && firewall-cmd --runtime-to-permanent Puppet Server Installation and setup The Puppet server version would be puppet 7, this is why i downloaded the repository on the main website of puppet server and installed it to spacewalk. Note that the package i want to download is puppetserver instead of puppet-server, there is a puppet-server package lurking in EPEL repository and yum install -y puppetserver Once installed, edit /etc/sysconfig/puppetserver JAVA_ARGS option and change it according to your reference, i setup my puppet server to use only 512mb of my memory resources [root@bot ~]# cat /etc/sysconfig/puppetserver | grep ^JAVA_ARGS= JAVA_ARGS=\"-Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger\" [root@bot ~]# ps -o uname,rss,pmem,comm -u puppet USER RSS %MEM COMMAND puppet 610592 60.1 java Puppet autosign feature will make easier configuration when spawning a new VM, it will automatically attach to the puppet server and will not make authentication procedure, and thus making it unsecure. Since this is a development project and in a controlled network, i have enabled it. echo '*.sandbox.io' > /etc/puppetlabs/puppet/autosign.conf The working directory for you to setup configuration files is /etc/puppetlabs/code/environments/production . Create and edit site.pp file in manifests directory. # manifests/site.pp node default { include base } # This configuration will apply to all host connecting bot.sandbox.io Create directory in modules directory named base with a subdirectory called manifests. create init.pp file and that would be the main configuration file for every hosts that will connect to the puppet server # modules/base/manifests/init.pp class base { file { '/etc/yum.repos.d' : ensure => directory, recurse => true, purge => true } package { [git, ansible, policycoreutils-python, setroubleshoot-server, ipa-client, firewalld]: require => File['/etc/yum.repos.d'], ensure => present } } # Will remove repo files, and install necessary packages If the server has puppet-agent installed, you can type the following: puppet config set server bot.sandbox.io --section main puppet agent --test The output should look like this Info: Creating a new RSA SSL key for dev1.sandbox.io Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml Info: Creating a new SSL certificate request for dev1.sandbox.io Info: Certificate Request fingerprint (SHA256): 1D:D9:11:63:35:D8:00:26:EE:C0:11:E5:6D:B7:9B:CD:97:7E:44:B4:DA:59:FC:46:41:BC:54:AB:91:3A:5E:F3 Info: Downloaded certificate for dev1.sandbox.io from https://bot.sandbox.io:8140/puppet-ca/v1 Info: Using configured environment 'production' Info: Retrieving pluginfacts Info: Retrieving plugin Info: Caching catalog for dev1.sandbox.io Info: Applying configuration version '1629663815' Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Base.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-CR.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Debuginfo.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Media.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Sources.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Vault.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-fasttrack.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-x86_64-kernel.repo]/ensure: removed Notice: /Stage[main]/Base/Package[git]/ensure: created Notice: /Stage[main]/Base/Package[ansible]/ensure: created Notice: /Stage[main]/Base/Package[policycoreutils-python]/ensure: created Notice: /Stage[main]/Base/Package[setroubleshoot-server]/ensure: created Notice: /Stage[main]/Base/Package[ipa-client]/ensure: created Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml Notice: Applied catalog in 312.49 seconds We can move forward on adding additional configurations on the base/init.pp file Other Configurations ive added config.pp file in modules/base/manifests directory to put all my variables and not crowding the init.pp file, need it to be declared in init.pp file include base::config include base::secret Run and enable FirewallD service service { 'firewalld': ensure => running, enable => true, require => Package['firewalld'] } Add public key for easy access on root account file { '/root/.ssh': ensure => directory, owner => 0, group => 0, mode => '700' } file { '/root/.ssh/authorized_keys': ensure => file, owner => 0, group => 0, mode => '600', content => $::base::config::root_pub_key, require => File['/root/.ssh'] } Pre configured vimrc setup will be installed in root and skel directory file { ['/etc/skel/.vimrc', '/root/.vimrc']: ensure => file, content => $::base::config::vim_rc } Remove 'rhgb quiet' options in /etc/default/grub file, this is to view errors arising upon bootup. exec { 'remove rhgb and quiet': command => \"sed -i 's/ rhgb quiet//g' /etc/default/grub && grub2-mkconfig -o /boot/grub2/grub.cfg\", path => '/usr/bin:/usr/sbin', onlyif => \"grep ' rhgb quiet' /etc/default/grub\" } Disable root password authentication and use public key exec { 'disable root password auth': command => 'sed -i \"s/#PermitRootLogin yes/PermitRootLogin without-password/\" /etc/ssh/sshd_config', path => '/usr/bin:/usr/sbin', onlyif => 'grep \"#PermitRootLogin\" /etc/ssh/sshd_config', notify => Service['sshd'] } # SSH daemon restart placeholder for ssh to restart once /etc/ssh/sshd_config has been change. service { 'sshd': ensure=> running } Connect to FreeIPA Server exec { 'execute ipa-client-install': command => \"ipa-client-install -U -p '${::base::secret::ipa_user}' -w '${::base::secret::ipa_pass}' --domain=sandbox.io --server=directory1.sandbox.io --mkhomedir\", path => '/usr/bin:/usr/sbin', unless => \"grep 'URI ldaps://directory1.sandbox.io' /etc/openldap/ldap.conf 2>/dev/null\", require => Package['ipa-client'] } Create home directory of bot-acc for ansible file { '/home/users': ensure => directory, mode => '755' } file { 'bot-acc setup1': path => '/home/users/bot-acc', ensure => directory, source => '/etc/skel', recurse => true, mode => '600', owner => '779800006', group => '779800006', require => [ File['/home/users'] , Exec['execute ipa-client-install'] ] } Create local account so server is still accessible if cannot be access root via ssh user { 'user-account': ensure => present, name => $::base::secret::sys_user, password => $::base::secret::sys_passwd, managehome => true } Create ssh dir for bot-acc account, copy the files under modules/base/files/ansible_ssh/ directory file { 'bot-acc sshdir': ensure => directory, path => '/home/users/bot-acc/.ssh', mode => '600', owner => '779800006', group => '779800006', recurse => true, source => 'puppet:///modules/base/ansible_ssh', require => File['bot-acc setup1'] } Create .system-checks directory in /home/users/bot-acc, this will act as directory of files for the system to check prior to their execution file { 'bot-acc system_checks directory': ensure => directory, path => '/home/users/bot-acc/.system-checks', mode => '600', owner => '779800006', group => '779800006', require => Exec['copy /etc/skel to bot-acc'] } ansible_ssh files [root@bot production]# ll modules/base/files/ansible_ssh/ total 12 -rw-r--r--. 1 root root 381 Sep 4 14:29 authorized_keys -rw-r--r--. 1 root root 106 Sep 4 21:00 config -rw-r--r--. 1 root root 1675 Sep 4 23:42 id_rsa_ansible Set ansible password .ansible_pass file file { 'ansible_pass': ensure => file, path => '/home/users/bot-acc/.ansible_pass', mode => '600', owner => '779800006', group => '779800006', content => $::base::secret::ansible_pass } Setup sudoers file for freeipa groups file { 'sudoers for ipa': ensure => file, path => '/etc/sudoers.d/ipa', mode => '600', source => 'puppet:///modules/base/ipa_sudoers' } Current modules/base/files/ipa_sudoers contents %bot_admin ALL=(ALL) NOPASSWD: ALL Setup service file for ansible pull file { 'ansible_service file': ensure => file, path => '/etc/systemd/system/ansible-config.service', mode => '644', source => 'puppet:///modules/base/ansible_service/ansible-config.service' } ansible-config.service contents [Unit] Description=Run ansible-pull at first boot to apply environment configuration After=sssd.service [Service] ExecStart=/root/ansible-config.sh Type=oneshot [Install] WantedBy=multi-user.target Setup shell command ansible-command.sh file { 'ansible_pull command': ensure => file, path => '/root/ansible-config.sh', mode => '755', source => 'puppet:///modules/base/ansible_service/ansible-command.sh' } ansible-command.sh contents #!/bin/bash echo `date` > /home/users/bot-acc/bot-output.txt chmod 666 /home/users/bot-acc/bot-output.txt /opt/puppetlabs/bin/puppet agent --test | tee -a /home/users/bot-acc/bot-output.txt /usr/sbin/runuser -l bot-acc -c 'ansible-pull -U ansible:[redacted] -i inventory --accept-host-key --vault-password-file=~/.ansible_pass' | tee /root/ansible-output.txt In order to run puppet upon bootup, i will add it to kickstart file of spacewalk /opt/puppetlabs/bin/puppet config set server bot.sandbox.io --section main /opt/puppetlabs/bin/puppet agent --test | tee /root/puppet-log.txt Ansible Ansible package is already installed via PuppetServer Ansible Pull After creating the service for ansible to work, we are now on the setting up ansible for ansible pull [root@bot ansible_pull]# tree -L 2 . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 postconfig.yml \u2502 \u2514\u2500\u2500 preconfig.yml \u251c\u2500\u2500 files \u2502 \u251c\u2500\u2500 gpg_keys \u2502 \u2514\u2500\u2500 dev \u251c\u2500\u2500 inventory \u251c\u2500\u2500 local.yml \u2514\u2500\u2500 nodes \u2514\u2500\u2500 dev-setup.yml local.yml file will serve as our main file that will route configurations based on the hostname of the servers # Example local.yml file --- - hosts: localhost tasks: - name: run pre install setup include: configs/preconfig.yml - name: run setup for storage server include: nodes/dev-setup.yml when: ansible_hostname == \"dev\" - name: run post install setup include: configs/postconfig.yml ... inventory file will need all the hostnames of the servers that will sync on local.yml docs.sandbox.io dev.sandbox.io ansible.cfg will be our configuration setup for ansible-pull, host_key_checking = False will ignore StrictHostKeyChecking of the server. display_skipped_hosts = False will disable displaying of skipped tasks. other configurations are for privesc. [defaults] host_key_checking = False display_skipped_hosts = False [privilege_escalation] become = True become_method = sudo become_user = root become_ask_pass = False preconfig.yml and postconfig.yml in config directory will act as a base configuration for ansible. installing necessary things first before node configurations, and cleanup after. # Example preconfig.yml file - name: make sure base GPG keys are installed rpm_key: key: '{{item}}' state: present with_items: - 'files/gpg_keys/centos7/centos-base-gpg' - 'files/gpg_keys/centos7/centos-epel-gpg' - 'files/gpg_keys/centos7/puppet-release-1-gpg' - 'files/gpg_keys/centos7/puppet-release-2-gpg' - 'files/gpg_keys/centos7/spacewalk-base-gpg' - 'files/gpg_keys/centos7/spacewalk-java-gpg' In order for ansible pull to work, we need to configure the directory to use git. git init git add -A git commit -m \"initial commit\" ansible-command.sh already configured the ansible-pull command.","title":"Automation"},{"location":"bot/#automation-server","text":"The Servers FQDN will be bot.sandbox.io, for this one i dont want to lenghten all of my servers FQDN.","title":"Automation Server"},{"location":"bot/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 Reverse lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 105 IN PTR bot.sandbox.io. DHCP entry host bot { hardware ethernet 9A:5D:A5:C2:A3:25; fixed-address 10.11.100.105; }","title":"DHCP and DNS Entries"},{"location":"bot/#initial-phase","text":"","title":"Initial Phase"},{"location":"bot/#vimrc-file","text":"setup .vimrc on the root account and copy the file to /etc/skel so all accounts created moving forward has the same configuration, this would be helpful as we would use ansible later set tabstop=4 set shiftwidth=4 set expandtab set number set smarttab set autoindent","title":"vimrc file"},{"location":"bot/#firewall","text":"Ensure firewall port(8140) for puppet server is open. firewall-cmd --add-port=8140/tcp && firewall-cmd --runtime-to-permanent","title":"Firewall"},{"location":"bot/#puppet-server","text":"","title":"Puppet Server"},{"location":"bot/#installation-and-setup","text":"The Puppet server version would be puppet 7, this is why i downloaded the repository on the main website of puppet server and installed it to spacewalk. Note that the package i want to download is puppetserver instead of puppet-server, there is a puppet-server package lurking in EPEL repository and yum install -y puppetserver Once installed, edit /etc/sysconfig/puppetserver JAVA_ARGS option and change it according to your reference, i setup my puppet server to use only 512mb of my memory resources [root@bot ~]# cat /etc/sysconfig/puppetserver | grep ^JAVA_ARGS= JAVA_ARGS=\"-Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger\" [root@bot ~]# ps -o uname,rss,pmem,comm -u puppet USER RSS %MEM COMMAND puppet 610592 60.1 java Puppet autosign feature will make easier configuration when spawning a new VM, it will automatically attach to the puppet server and will not make authentication procedure, and thus making it unsecure. Since this is a development project and in a controlled network, i have enabled it. echo '*.sandbox.io' > /etc/puppetlabs/puppet/autosign.conf The working directory for you to setup configuration files is /etc/puppetlabs/code/environments/production . Create and edit site.pp file in manifests directory. # manifests/site.pp node default { include base } # This configuration will apply to all host connecting bot.sandbox.io Create directory in modules directory named base with a subdirectory called manifests. create init.pp file and that would be the main configuration file for every hosts that will connect to the puppet server # modules/base/manifests/init.pp class base { file { '/etc/yum.repos.d' : ensure => directory, recurse => true, purge => true } package { [git, ansible, policycoreutils-python, setroubleshoot-server, ipa-client, firewalld]: require => File['/etc/yum.repos.d'], ensure => present } } # Will remove repo files, and install necessary packages If the server has puppet-agent installed, you can type the following: puppet config set server bot.sandbox.io --section main puppet agent --test The output should look like this Info: Creating a new RSA SSL key for dev1.sandbox.io Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml Info: Creating a new SSL certificate request for dev1.sandbox.io Info: Certificate Request fingerprint (SHA256): 1D:D9:11:63:35:D8:00:26:EE:C0:11:E5:6D:B7:9B:CD:97:7E:44:B4:DA:59:FC:46:41:BC:54:AB:91:3A:5E:F3 Info: Downloaded certificate for dev1.sandbox.io from https://bot.sandbox.io:8140/puppet-ca/v1 Info: Using configured environment 'production' Info: Retrieving pluginfacts Info: Retrieving plugin Info: Caching catalog for dev1.sandbox.io Info: Applying configuration version '1629663815' Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Base.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-CR.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Debuginfo.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Media.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Sources.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Vault.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-fasttrack.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-x86_64-kernel.repo]/ensure: removed Notice: /Stage[main]/Base/Package[git]/ensure: created Notice: /Stage[main]/Base/Package[ansible]/ensure: created Notice: /Stage[main]/Base/Package[policycoreutils-python]/ensure: created Notice: /Stage[main]/Base/Package[setroubleshoot-server]/ensure: created Notice: /Stage[main]/Base/Package[ipa-client]/ensure: created Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml Notice: Applied catalog in 312.49 seconds We can move forward on adding additional configurations on the base/init.pp file","title":"Installation and setup"},{"location":"bot/#other-configurations","text":"ive added config.pp file in modules/base/manifests directory to put all my variables and not crowding the init.pp file, need it to be declared in init.pp file include base::config include base::secret Run and enable FirewallD service service { 'firewalld': ensure => running, enable => true, require => Package['firewalld'] } Add public key for easy access on root account file { '/root/.ssh': ensure => directory, owner => 0, group => 0, mode => '700' } file { '/root/.ssh/authorized_keys': ensure => file, owner => 0, group => 0, mode => '600', content => $::base::config::root_pub_key, require => File['/root/.ssh'] } Pre configured vimrc setup will be installed in root and skel directory file { ['/etc/skel/.vimrc', '/root/.vimrc']: ensure => file, content => $::base::config::vim_rc } Remove 'rhgb quiet' options in /etc/default/grub file, this is to view errors arising upon bootup. exec { 'remove rhgb and quiet': command => \"sed -i 's/ rhgb quiet//g' /etc/default/grub && grub2-mkconfig -o /boot/grub2/grub.cfg\", path => '/usr/bin:/usr/sbin', onlyif => \"grep ' rhgb quiet' /etc/default/grub\" } Disable root password authentication and use public key exec { 'disable root password auth': command => 'sed -i \"s/#PermitRootLogin yes/PermitRootLogin without-password/\" /etc/ssh/sshd_config', path => '/usr/bin:/usr/sbin', onlyif => 'grep \"#PermitRootLogin\" /etc/ssh/sshd_config', notify => Service['sshd'] } # SSH daemon restart placeholder for ssh to restart once /etc/ssh/sshd_config has been change. service { 'sshd': ensure=> running } Connect to FreeIPA Server exec { 'execute ipa-client-install': command => \"ipa-client-install -U -p '${::base::secret::ipa_user}' -w '${::base::secret::ipa_pass}' --domain=sandbox.io --server=directory1.sandbox.io --mkhomedir\", path => '/usr/bin:/usr/sbin', unless => \"grep 'URI ldaps://directory1.sandbox.io' /etc/openldap/ldap.conf 2>/dev/null\", require => Package['ipa-client'] } Create home directory of bot-acc for ansible file { '/home/users': ensure => directory, mode => '755' } file { 'bot-acc setup1': path => '/home/users/bot-acc', ensure => directory, source => '/etc/skel', recurse => true, mode => '600', owner => '779800006', group => '779800006', require => [ File['/home/users'] , Exec['execute ipa-client-install'] ] } Create local account so server is still accessible if cannot be access root via ssh user { 'user-account': ensure => present, name => $::base::secret::sys_user, password => $::base::secret::sys_passwd, managehome => true } Create ssh dir for bot-acc account, copy the files under modules/base/files/ansible_ssh/ directory file { 'bot-acc sshdir': ensure => directory, path => '/home/users/bot-acc/.ssh', mode => '600', owner => '779800006', group => '779800006', recurse => true, source => 'puppet:///modules/base/ansible_ssh', require => File['bot-acc setup1'] } Create .system-checks directory in /home/users/bot-acc, this will act as directory of files for the system to check prior to their execution file { 'bot-acc system_checks directory': ensure => directory, path => '/home/users/bot-acc/.system-checks', mode => '600', owner => '779800006', group => '779800006', require => Exec['copy /etc/skel to bot-acc'] } ansible_ssh files [root@bot production]# ll modules/base/files/ansible_ssh/ total 12 -rw-r--r--. 1 root root 381 Sep 4 14:29 authorized_keys -rw-r--r--. 1 root root 106 Sep 4 21:00 config -rw-r--r--. 1 root root 1675 Sep 4 23:42 id_rsa_ansible Set ansible password .ansible_pass file file { 'ansible_pass': ensure => file, path => '/home/users/bot-acc/.ansible_pass', mode => '600', owner => '779800006', group => '779800006', content => $::base::secret::ansible_pass } Setup sudoers file for freeipa groups file { 'sudoers for ipa': ensure => file, path => '/etc/sudoers.d/ipa', mode => '600', source => 'puppet:///modules/base/ipa_sudoers' } Current modules/base/files/ipa_sudoers contents %bot_admin ALL=(ALL) NOPASSWD: ALL Setup service file for ansible pull file { 'ansible_service file': ensure => file, path => '/etc/systemd/system/ansible-config.service', mode => '644', source => 'puppet:///modules/base/ansible_service/ansible-config.service' } ansible-config.service contents [Unit] Description=Run ansible-pull at first boot to apply environment configuration After=sssd.service [Service] ExecStart=/root/ansible-config.sh Type=oneshot [Install] WantedBy=multi-user.target Setup shell command ansible-command.sh file { 'ansible_pull command': ensure => file, path => '/root/ansible-config.sh', mode => '755', source => 'puppet:///modules/base/ansible_service/ansible-command.sh' } ansible-command.sh contents #!/bin/bash echo `date` > /home/users/bot-acc/bot-output.txt chmod 666 /home/users/bot-acc/bot-output.txt /opt/puppetlabs/bin/puppet agent --test | tee -a /home/users/bot-acc/bot-output.txt /usr/sbin/runuser -l bot-acc -c 'ansible-pull -U ansible:[redacted] -i inventory --accept-host-key --vault-password-file=~/.ansible_pass' | tee /root/ansible-output.txt In order to run puppet upon bootup, i will add it to kickstart file of spacewalk /opt/puppetlabs/bin/puppet config set server bot.sandbox.io --section main /opt/puppetlabs/bin/puppet agent --test | tee /root/puppet-log.txt","title":"Other Configurations"},{"location":"bot/#ansible","text":"Ansible package is already installed via PuppetServer","title":"Ansible"},{"location":"bot/#ansible-pull","text":"After creating the service for ansible to work, we are now on the setting up ansible for ansible pull [root@bot ansible_pull]# tree -L 2 . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 postconfig.yml \u2502 \u2514\u2500\u2500 preconfig.yml \u251c\u2500\u2500 files \u2502 \u251c\u2500\u2500 gpg_keys \u2502 \u2514\u2500\u2500 dev \u251c\u2500\u2500 inventory \u251c\u2500\u2500 local.yml \u2514\u2500\u2500 nodes \u2514\u2500\u2500 dev-setup.yml local.yml file will serve as our main file that will route configurations based on the hostname of the servers # Example local.yml file --- - hosts: localhost tasks: - name: run pre install setup include: configs/preconfig.yml - name: run setup for storage server include: nodes/dev-setup.yml when: ansible_hostname == \"dev\" - name: run post install setup include: configs/postconfig.yml ... inventory file will need all the hostnames of the servers that will sync on local.yml docs.sandbox.io dev.sandbox.io ansible.cfg will be our configuration setup for ansible-pull, host_key_checking = False will ignore StrictHostKeyChecking of the server. display_skipped_hosts = False will disable displaying of skipped tasks. other configurations are for privesc. [defaults] host_key_checking = False display_skipped_hosts = False [privilege_escalation] become = True become_method = sudo become_user = root become_ask_pass = False preconfig.yml and postconfig.yml in config directory will act as a base configuration for ansible. installing necessary things first before node configurations, and cleanup after. # Example preconfig.yml file - name: make sure base GPG keys are installed rpm_key: key: '{{item}}' state: present with_items: - 'files/gpg_keys/centos7/centos-base-gpg' - 'files/gpg_keys/centos7/centos-epel-gpg' - 'files/gpg_keys/centos7/puppet-release-1-gpg' - 'files/gpg_keys/centos7/puppet-release-2-gpg' - 'files/gpg_keys/centos7/spacewalk-base-gpg' - 'files/gpg_keys/centos7/spacewalk-java-gpg' In order for ansible pull to work, we need to configure the directory to use git. git init git add -A git commit -m \"initial commit\" ansible-command.sh already configured the ansible-pull command.","title":"Ansible Pull"},{"location":"docs/","text":"Documentation The server would be a nginx server that is hosting 3 websites on 3 different domain names. devdocs.sandbox.io | running mkdocs serve on port 8008, proxied to port 80 portfolio.sandbox.io | local hosting of my portfolio running jekyll in port 8009, proxied to port 80 docs.sandbox.io | build mkdocs static files DHCP and DNS Entries Forward lookup zone @ IN NS docs.sandbox.io. @ IN A 10.11.100.103 docs IN A 10.11.100.103 devdocs IN CNAME docs portfolio IN CNAME docs Reverse lookup zone @ IN NS docs.sandbox.io. docs IN A 10.11.100.103 103 IN PTR docs.sandbox.io. DHCP entry host docs { hardware ethernet 0E:32:D1:C2:8A:73; fixed-address 10.11.100.103; } Installation and Setup Install the following packages if its not yet installed on your server - name: install necesarry files yum: name: - \"ruby\" - \"ruby-devel\" - \"openssl-devel\" - \"redhat-rpm-config\" - \"gcc-c++\" - \"patch\" - \"readline\" - \"readline-devel\" - \"zlib\" - \"zlib-devel\" - \"openssl-devel\" - \"make\" - \"bzip2\" - \"autoconf\" - \"automake\" - \"libtool\" - \"bison\" - \"sqlite-devel\" - \"libyaml-devel\" - \"libffi-devel\" - \"@Development Tools\" - \"nginx\" - \"python3\" state: present Copy the files to /tmp - name: create tmp directory file: path: /tmp/ansible state: directory - name: copy selinux module copy: src: '{{item}}' dest: /tmp/ansible with_items: - 'files/docs/selinux/my-nginx.pp' - 'files/docs/selinux/my-nginx.te' - 'files/docs/rvm/mpapis.asc' - 'files/docs/rvm/pkuczynski.asc' - 'files/docs/rvm/rvm-installer' - 'files/docs/install_ruby.sh' If the server you are using is using selinux, run semodule to clear warnings of selinux - name: perform nginx selinux configuration shell: cmd: semodule -i /tmp/ansible/my-nginx.pp creates: /home/users/bot-acc/.system-checks/nginx-selinux-done Contents of my-nginx.te which is the make file to my-nginx.pp module my-nginx 1.0; require { type httpd_t; type usr_t; class file append; } #============= httpd_t ============== #!!!! WARNING: 'usr_t' is a base type. allow httpd_t usr_t:file append; Run install_ruby.sh - name: execute shell file shell: cmd: /tmp/install_ruby.sh creates: /home/users/bot-acc/.system-checks/rvm-install-done Contents of install_ruby.sh, i will install ruby via RVM #!/bin/bash gpg --import /tmp/ansible/mpapis.asc gpg2 --import /tmp/ansible/pkuczynski.asc bash /tmp/ansible/rvm-installer stable # contents: get.rvm.io # curl -L get.rvm.io | bash -s stable # can be run instead of \"bash /tmp/ansible/rvm-installer stable\" source /etc/profile.d/rvm.sh rvm install 2.4.0 gem install jekyll bundler Create directories of the sites that will be served by this server - name: create sites directory in /opt file: path: '{{item}}' owner: nginx group: bot-acc state: directory mode: \"0775\" with_items: - /opt/sites - /opt/sites/portfolio - /opt/sites/sandbox Clone the repos - name: git clone portfolio git: repo: '[portfolio_repo]' dest: '/opt/sites/portfolio' update: false become: no - name: git clone sandbox git: repo: '[sandbox_repo]' dest: '/opt/sites/sandbox' update: false become: no install mkdocs via pip3, mkdocs in pip3 is the most updated one - name: install mkdocs using pip pip: name: mkdocs executable: pip3 in order for jekyll to serve the porfolio site, we need to add other settings to _config.yml - name: add configuration to serve portfolio blockinfile: path: /opt/sites/portfolio/_config.yml block: | host: 0.0.0.0 port: 8009 We need to serve mkdocs and jekyll on startup, copy the files, and do a daemon-reload. - name: copy service files to /etc/systemd/system/ copy: src: '{{item}}' dest: /etc/systemd/system with_items: - files/docs/systemd/portfolio-dev.service - files/docs/systemd/sandbox-dev.service - name: daemon reload systemd: daemon_reload: true Contents of portfolio-dev.service [Unit] Description=portfolio dev mode serving in port 8009 [Service] WorkingDirectory=/opt/sites/portfolio ExecStart=/usr/local/rvm/gems/ruby-2.4.0/wrappers/jekyll serve --livereload --trace --baseurl \"\" [Install] WantedBy=multi-user.target Contents of sandbox-dev.service [Unit] Description=mkdocs dev mode serving in port 8008 [Service] Environment=\"LC_ALL=en_US.utf-8\" WorkingDirectory=/opt/sites/sandbox ExecStart=/usr/local/bin/mkdocs serve -a 0.0.0.0:8008 Restart=on-failure RestartSec=5s [Install] WantedBy=multi-user.target Copy contents of nginx-proxy.conf and paste it to /etc/nginx/conf.d - name: copy nginx proxy configuration file to /etc/nginx/conf.d copy: src: files/docs/nginx-proxy.conf dest: /etc/nginx/conf.d/proxy.conf Contents of /etc/nginx/conf.d/proxy.conf server { listen 80; server_name portfolio.sandbox.io; # access_log off; # error_log off; location / { proxy_pass http://localhost:8009; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_redirect off; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; client_max_body_size 10m; client_body_buffer_size 128k; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; } } server { listen 80; server_name devdocs.sandbox.io; # access_log off; # error_log off; location / { proxy_pass http://localhost:8008; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_redirect off; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; client_max_body_size 10m; client_body_buffer_size 128k; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; } } server { listen 80; listen [::]:80; server_name docs.sandbox.io; root /opt/sites/docs-site; include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } start/restart the services - name: start services service: name: '{{item}}' state: restarted enabled: true with_items: - nginx - portfolio-dev - sandbox-dev Sample images","title":"Documentation"},{"location":"docs/#documentation","text":"The server would be a nginx server that is hosting 3 websites on 3 different domain names. devdocs.sandbox.io | running mkdocs serve on port 8008, proxied to port 80 portfolio.sandbox.io | local hosting of my portfolio running jekyll in port 8009, proxied to port 80 docs.sandbox.io | build mkdocs static files","title":"Documentation"},{"location":"docs/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS docs.sandbox.io. @ IN A 10.11.100.103 docs IN A 10.11.100.103 devdocs IN CNAME docs portfolio IN CNAME docs Reverse lookup zone @ IN NS docs.sandbox.io. docs IN A 10.11.100.103 103 IN PTR docs.sandbox.io. DHCP entry host docs { hardware ethernet 0E:32:D1:C2:8A:73; fixed-address 10.11.100.103; }","title":"DHCP and DNS Entries"},{"location":"docs/#installation-and-setup","text":"Install the following packages if its not yet installed on your server - name: install necesarry files yum: name: - \"ruby\" - \"ruby-devel\" - \"openssl-devel\" - \"redhat-rpm-config\" - \"gcc-c++\" - \"patch\" - \"readline\" - \"readline-devel\" - \"zlib\" - \"zlib-devel\" - \"openssl-devel\" - \"make\" - \"bzip2\" - \"autoconf\" - \"automake\" - \"libtool\" - \"bison\" - \"sqlite-devel\" - \"libyaml-devel\" - \"libffi-devel\" - \"@Development Tools\" - \"nginx\" - \"python3\" state: present Copy the files to /tmp - name: create tmp directory file: path: /tmp/ansible state: directory - name: copy selinux module copy: src: '{{item}}' dest: /tmp/ansible with_items: - 'files/docs/selinux/my-nginx.pp' - 'files/docs/selinux/my-nginx.te' - 'files/docs/rvm/mpapis.asc' - 'files/docs/rvm/pkuczynski.asc' - 'files/docs/rvm/rvm-installer' - 'files/docs/install_ruby.sh' If the server you are using is using selinux, run semodule to clear warnings of selinux - name: perform nginx selinux configuration shell: cmd: semodule -i /tmp/ansible/my-nginx.pp creates: /home/users/bot-acc/.system-checks/nginx-selinux-done Contents of my-nginx.te which is the make file to my-nginx.pp module my-nginx 1.0; require { type httpd_t; type usr_t; class file append; } #============= httpd_t ============== #!!!! WARNING: 'usr_t' is a base type. allow httpd_t usr_t:file append; Run install_ruby.sh - name: execute shell file shell: cmd: /tmp/install_ruby.sh creates: /home/users/bot-acc/.system-checks/rvm-install-done Contents of install_ruby.sh, i will install ruby via RVM #!/bin/bash gpg --import /tmp/ansible/mpapis.asc gpg2 --import /tmp/ansible/pkuczynski.asc bash /tmp/ansible/rvm-installer stable # contents: get.rvm.io # curl -L get.rvm.io | bash -s stable # can be run instead of \"bash /tmp/ansible/rvm-installer stable\" source /etc/profile.d/rvm.sh rvm install 2.4.0 gem install jekyll bundler Create directories of the sites that will be served by this server - name: create sites directory in /opt file: path: '{{item}}' owner: nginx group: bot-acc state: directory mode: \"0775\" with_items: - /opt/sites - /opt/sites/portfolio - /opt/sites/sandbox Clone the repos - name: git clone portfolio git: repo: '[portfolio_repo]' dest: '/opt/sites/portfolio' update: false become: no - name: git clone sandbox git: repo: '[sandbox_repo]' dest: '/opt/sites/sandbox' update: false become: no install mkdocs via pip3, mkdocs in pip3 is the most updated one - name: install mkdocs using pip pip: name: mkdocs executable: pip3 in order for jekyll to serve the porfolio site, we need to add other settings to _config.yml - name: add configuration to serve portfolio blockinfile: path: /opt/sites/portfolio/_config.yml block: | host: 0.0.0.0 port: 8009 We need to serve mkdocs and jekyll on startup, copy the files, and do a daemon-reload. - name: copy service files to /etc/systemd/system/ copy: src: '{{item}}' dest: /etc/systemd/system with_items: - files/docs/systemd/portfolio-dev.service - files/docs/systemd/sandbox-dev.service - name: daemon reload systemd: daemon_reload: true Contents of portfolio-dev.service [Unit] Description=portfolio dev mode serving in port 8009 [Service] WorkingDirectory=/opt/sites/portfolio ExecStart=/usr/local/rvm/gems/ruby-2.4.0/wrappers/jekyll serve --livereload --trace --baseurl \"\" [Install] WantedBy=multi-user.target Contents of sandbox-dev.service [Unit] Description=mkdocs dev mode serving in port 8008 [Service] Environment=\"LC_ALL=en_US.utf-8\" WorkingDirectory=/opt/sites/sandbox ExecStart=/usr/local/bin/mkdocs serve -a 0.0.0.0:8008 Restart=on-failure RestartSec=5s [Install] WantedBy=multi-user.target Copy contents of nginx-proxy.conf and paste it to /etc/nginx/conf.d - name: copy nginx proxy configuration file to /etc/nginx/conf.d copy: src: files/docs/nginx-proxy.conf dest: /etc/nginx/conf.d/proxy.conf Contents of /etc/nginx/conf.d/proxy.conf server { listen 80; server_name portfolio.sandbox.io; # access_log off; # error_log off; location / { proxy_pass http://localhost:8009; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_redirect off; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; client_max_body_size 10m; client_body_buffer_size 128k; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; } } server { listen 80; server_name devdocs.sandbox.io; # access_log off; # error_log off; location / { proxy_pass http://localhost:8008; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_redirect off; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; client_max_body_size 10m; client_body_buffer_size 128k; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; } } server { listen 80; listen [::]:80; server_name docs.sandbox.io; root /opt/sites/docs-site; include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } start/restart the services - name: start services service: name: '{{item}}' state: restarted enabled: true with_items: - nginx - portfolio-dev - sandbox-dev Sample images","title":"Installation and Setup"},{"location":"freeipa/","text":"FreeIPA Documentation DHCP and DNS Entries Forward lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 Reverse lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 102 IN PTR directory.sandbox.io. DHCP client entry host directory { hardware ethernet 32:80:18:70:D6:CA; fixed-address 10.11.100.102; } Installation Prerequisites TCP Open Ports: - 80, 443: HTTP/HTTPS - 389, 636: LDAP/LDAPS - 88, 464: Kerberos UDP Open Ports: - 88, 464: Kerberos - 123: NTP 2Gb Recommended RAM, though the RAM usage will average to 1.3Gb it will be faster to boot on 2Gb of memory. Installing FreeIPA Server Install FreeIPA server using yum -y install ipa-server Once the packages are install you can continue installation by using ipa-server-install [root@directory ~]# ipa-server-install The log file for this installation can be found in /var/log/ipaserver-install.log ============================================================================== This program will set up the IPA Server. This includes: * Configure a stand-alone CA (dogtag) for certificate management * Configure the Network Time Daemon (ntpd) * Create and configure an instance of Directory Server * Create and configure a Kerberos Key Distribution Center (KDC) * Configure Apache (httpd) * Configure the KDC to enable PKINIT The command will ask for things you need to fill up, you should leave all default if you are using the freeipa server as kerberos authentication too. All default answers will be encapsulated with [ ] To accept the default shown in brackets, press the Enter key. WARNING: conflicting time&date synchronization service 'chronyd' will be disabled in favor of ntpd Do you want to configure integrated DNS (BIND)? [no]: Enter the fully qualified domain name of the computer on which you are setting up server software. Using the form <hostname>.<domainname> Example: master.example.com. Server host name [directory.sandbox.io]: The domain name has been determined based on the host name. Please confirm the domain name [sandbox.io]: The kerberos protocol requires a Realm name to be defined. This is typically the domain name converted to uppercase. Please provide a realm name [SANDBOX.IO]: This will also ask for manager and admin password, set it up Certain directory server operations require an administrative user. This user is referred to as the Directory Manager and has full access to the Directory for system management tasks and will be added to the instance of directory server created for IPA. The password must be at least 8 characters long. Directory Manager password: Password (confirm): The IPA server requires an administrative user, named 'admin'. This user is a regular system account used for IPA server administration. IPA admin password: Password (confirm): Once done, it will prompt you to configure the freeipa server with the set values, type yes. The IPA Master Server will be configured with: Hostname: directory.sandbox.io IP address(es): 10.11.100.200 Domain name: sandbox.io Realm name: SANDBOX.IO Continue to configure the system with these values? [no]: yes It will start configuring itself, after setting up it will also note you to open necessary ports for the server to be able to communicate outside ... SSSD enabled Configured /etc/openldap/ldap.conf Configured /etc/ssh/ssh_config Configured /etc/ssh/sshd_config Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful Please add records in this file to your DNS system: /tmp/ipa.system.records.zcm0BH.db ============================================================================== Setup complete Next steps: 1. You must make sure these network ports are open: TCP Ports: * 80, 443: HTTP/HTTPS * 389, 636: LDAP/LDAPS * 88, 464: kerberos UDP Ports: * 88, 464: kerberos * 123: ntp 2. You can now obtain a kerberos ticket using the command: 'kinit admin' This ticket will allow you to use the IPA tools (e.g., ipa user-add) and the web user interface. Be sure to back up the CA certificates stored in /root/cacert.p12 These files are required to create replicas. The password for these files is the Directory Manager password The installer will also provide DNS entries for you to put in your DNS server _kerberos-master._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos-master._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos.sandbox.io. 86400 IN TXT \"SANDBOX.IO\" _kpasswd._tcp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _kpasswd._udp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _ldap._tcp.sandbox.io. 86400 IN SRV 0 100 389 directory.sandbox.io. _ntp._udp.sandbox.io. 86400 IN SRV 0 100 123 directory.sandbox.io. Open the necessary ports to firewall [root@directory ~]# firewall-cmd --add-port={80,443,389,636,88,464}/tcp success [root@directory ~]# firewall-cmd --add-port={88,464,123}/udp success [root@directory ~]# firewall-cmd --list-all public (active) target: default icmp-block-inversion: no interfaces: eth0 sources: services: dhcpv6-client ssh ports: 80/tcp 443/tcp 389/tcp 636/tcp 88/tcp 464/tcp 88/udp 464/udp 123/udp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: The installation is now complete, you may want to add users/group via command line or the browser. Installing FreeIPA Client Install FreeIPA client yum install -y ipa-client Setup FreeIPA Client using ipa-client-install --mkhomedir , the server will automatically search the directory server. [root@docs ~]# ipa-client-install --mkhomedir WARNING: ntpd time&date synchronization service will not be configured as conflicting service (chronyd) is enabled Use --force-ntpd option to disable it and force configuration of ntpd Discovery was successful! Client hostname: docs.sandbox.io Realm: SANDBOX.IO DNS Domain: sandbox.io IPA Server: directory.sandbox.io BaseDN: dc=sandbox,dc=io Continue to configure the system with these values? [no]: yes Skipping synchronizing time with NTP server. User authorized to enroll computers: dbalgos Password for dbalgos@SANDBOX.IO: Successfully retrieved CA cert Subject: CN=Certificate Authority,O=SANDBOX.IO Issuer: CN=Certificate Authority,O=SANDBOX.IO Valid From: 2021-08-16 05:49:51 Valid Until: 2041-08-16 05:49:51 Enrolled in IPA realm SANDBOX.IO Created /etc/ipa/default.conf New SSSD config will be created Configured sudoers in /etc/nsswitch.conf ... Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful You can now login any account registered in the FreeIPA server [root@docs ~]# id dbalgos uid=779800005(dbalgos) gid=779800005(dbalgos) groups=779800005(dbalgos),779800000(admins) In case we want to uninstall, we can use ipa-client-install --uninstall command [root@directory ~]# ipa-client-install --uninstall Unenrolling client from IPA server Unenrolling host failed: Error obtaining initial credentials: Cannot contact any KDC for requested realm. Removing Kerberos service principals from /etc/krb5.keytab Disabling client Kerberos and LDAP configurations Redundant SSSD configuration file /etc/sssd/sssd.conf was moved to /etc/sssd/sssd.conf.deleted Restoring client configuration files Unconfiguring the NIS domain. nscd daemon is not installed, skip configuration nslcd daemon is not installed, skip configuration Systemwide CA database updated. Client uninstall complete. The original nsswitch.conf configuration has been restored. You may need to restart services or reboot the machine. Do you want to reboot the machine? [no]: y","title":"FreeIPA"},{"location":"freeipa/#freeipa-documentation","text":"","title":"FreeIPA Documentation"},{"location":"freeipa/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 Reverse lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 102 IN PTR directory.sandbox.io. DHCP client entry host directory { hardware ethernet 32:80:18:70:D6:CA; fixed-address 10.11.100.102; }","title":"DHCP and DNS Entries"},{"location":"freeipa/#installation","text":"","title":"Installation"},{"location":"freeipa/#prerequisites","text":"TCP Open Ports: - 80, 443: HTTP/HTTPS - 389, 636: LDAP/LDAPS - 88, 464: Kerberos UDP Open Ports: - 88, 464: Kerberos - 123: NTP 2Gb Recommended RAM, though the RAM usage will average to 1.3Gb it will be faster to boot on 2Gb of memory.","title":"Prerequisites"},{"location":"freeipa/#installing-freeipa-server","text":"Install FreeIPA server using yum -y install ipa-server Once the packages are install you can continue installation by using ipa-server-install [root@directory ~]# ipa-server-install The log file for this installation can be found in /var/log/ipaserver-install.log ============================================================================== This program will set up the IPA Server. This includes: * Configure a stand-alone CA (dogtag) for certificate management * Configure the Network Time Daemon (ntpd) * Create and configure an instance of Directory Server * Create and configure a Kerberos Key Distribution Center (KDC) * Configure Apache (httpd) * Configure the KDC to enable PKINIT The command will ask for things you need to fill up, you should leave all default if you are using the freeipa server as kerberos authentication too. All default answers will be encapsulated with [ ] To accept the default shown in brackets, press the Enter key. WARNING: conflicting time&date synchronization service 'chronyd' will be disabled in favor of ntpd Do you want to configure integrated DNS (BIND)? [no]: Enter the fully qualified domain name of the computer on which you are setting up server software. Using the form <hostname>.<domainname> Example: master.example.com. Server host name [directory.sandbox.io]: The domain name has been determined based on the host name. Please confirm the domain name [sandbox.io]: The kerberos protocol requires a Realm name to be defined. This is typically the domain name converted to uppercase. Please provide a realm name [SANDBOX.IO]: This will also ask for manager and admin password, set it up Certain directory server operations require an administrative user. This user is referred to as the Directory Manager and has full access to the Directory for system management tasks and will be added to the instance of directory server created for IPA. The password must be at least 8 characters long. Directory Manager password: Password (confirm): The IPA server requires an administrative user, named 'admin'. This user is a regular system account used for IPA server administration. IPA admin password: Password (confirm): Once done, it will prompt you to configure the freeipa server with the set values, type yes. The IPA Master Server will be configured with: Hostname: directory.sandbox.io IP address(es): 10.11.100.200 Domain name: sandbox.io Realm name: SANDBOX.IO Continue to configure the system with these values? [no]: yes It will start configuring itself, after setting up it will also note you to open necessary ports for the server to be able to communicate outside ... SSSD enabled Configured /etc/openldap/ldap.conf Configured /etc/ssh/ssh_config Configured /etc/ssh/sshd_config Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful Please add records in this file to your DNS system: /tmp/ipa.system.records.zcm0BH.db ============================================================================== Setup complete Next steps: 1. You must make sure these network ports are open: TCP Ports: * 80, 443: HTTP/HTTPS * 389, 636: LDAP/LDAPS * 88, 464: kerberos UDP Ports: * 88, 464: kerberos * 123: ntp 2. You can now obtain a kerberos ticket using the command: 'kinit admin' This ticket will allow you to use the IPA tools (e.g., ipa user-add) and the web user interface. Be sure to back up the CA certificates stored in /root/cacert.p12 These files are required to create replicas. The password for these files is the Directory Manager password The installer will also provide DNS entries for you to put in your DNS server _kerberos-master._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos-master._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos.sandbox.io. 86400 IN TXT \"SANDBOX.IO\" _kpasswd._tcp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _kpasswd._udp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _ldap._tcp.sandbox.io. 86400 IN SRV 0 100 389 directory.sandbox.io. _ntp._udp.sandbox.io. 86400 IN SRV 0 100 123 directory.sandbox.io. Open the necessary ports to firewall [root@directory ~]# firewall-cmd --add-port={80,443,389,636,88,464}/tcp success [root@directory ~]# firewall-cmd --add-port={88,464,123}/udp success [root@directory ~]# firewall-cmd --list-all public (active) target: default icmp-block-inversion: no interfaces: eth0 sources: services: dhcpv6-client ssh ports: 80/tcp 443/tcp 389/tcp 636/tcp 88/tcp 464/tcp 88/udp 464/udp 123/udp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: The installation is now complete, you may want to add users/group via command line or the browser.","title":"Installing FreeIPA Server"},{"location":"freeipa/#installing-freeipa-client","text":"Install FreeIPA client yum install -y ipa-client Setup FreeIPA Client using ipa-client-install --mkhomedir , the server will automatically search the directory server. [root@docs ~]# ipa-client-install --mkhomedir WARNING: ntpd time&date synchronization service will not be configured as conflicting service (chronyd) is enabled Use --force-ntpd option to disable it and force configuration of ntpd Discovery was successful! Client hostname: docs.sandbox.io Realm: SANDBOX.IO DNS Domain: sandbox.io IPA Server: directory.sandbox.io BaseDN: dc=sandbox,dc=io Continue to configure the system with these values? [no]: yes Skipping synchronizing time with NTP server. User authorized to enroll computers: dbalgos Password for dbalgos@SANDBOX.IO: Successfully retrieved CA cert Subject: CN=Certificate Authority,O=SANDBOX.IO Issuer: CN=Certificate Authority,O=SANDBOX.IO Valid From: 2021-08-16 05:49:51 Valid Until: 2041-08-16 05:49:51 Enrolled in IPA realm SANDBOX.IO Created /etc/ipa/default.conf New SSSD config will be created Configured sudoers in /etc/nsswitch.conf ... Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful You can now login any account registered in the FreeIPA server [root@docs ~]# id dbalgos uid=779800005(dbalgos) gid=779800005(dbalgos) groups=779800005(dbalgos),779800000(admins) In case we want to uninstall, we can use ipa-client-install --uninstall command [root@directory ~]# ipa-client-install --uninstall Unenrolling client from IPA server Unenrolling host failed: Error obtaining initial credentials: Cannot contact any KDC for requested realm. Removing Kerberos service principals from /etc/krb5.keytab Disabling client Kerberos and LDAP configurations Redundant SSSD configuration file /etc/sssd/sssd.conf was moved to /etc/sssd/sssd.conf.deleted Restoring client configuration files Unconfiguring the NIS domain. nscd daemon is not installed, skip configuration nslcd daemon is not installed, skip configuration Systemwide CA database updated. Client uninstall complete. The original nsswitch.conf configuration has been restored. You may need to restart services or reboot the machine. Do you want to reboot the machine? [no]: y","title":"Installing FreeIPA Client"},{"location":"gateway/","text":"DNS and DHCP Server Installation and Setup Proxmox Setup Create a linux bridge on the proxmox server to isolate the network to other side Perform a reboot on the server, once it is done we can create a KVM or LXC. then after creating the virtual machine we need to other NIC on it and link it to the created linux bridge. Start the VM, ensure the KVM/LXC have a second NIC, on my end it is eth1 [root@dns-dhcp ~]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether e2:67:0e:2f:3a:7d brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.254.107/24 brd 192.168.254.255 scope global dynamic eth0 valid_lft 250156sec preferred_lft 250156sec inet6 fe80::e067:eff:fe2f:3a7d/64 scope link valid_lft forever preferred_lft forever 3: eth1@if15: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 0a:bf:e2:77:5c:5f brd ff:ff:ff:ff:ff:ff link-netnsid 0 Install necessary packages: yum install -y iptables iptables-services bind bind-utils bind-libs dhcp [root@dns-dhcp ~]# yum install -y iptables iptables-services bind bind-utils bind-libs dhcp Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: mirror.hostlink.com.hk * extras: mirror.hostlink.com.hk * updates: mirror.hostlink.com.hk Package iptables-services-1.4.21-35.el7.x86_64 already installed and latest version Package iptables-1.4.21-35.el7.x86_64 already installed and latest version Package 32:bind-9.11.4-26.P2.el7_9.8.x86_64 already installed and latest version Package 32:bind-utils-9.11.4-26.P2.el7_9.8.x86_64 already installed and latest version Package 32:bind-libs-9.11.4-26.P2.el7_9.8.x86_64 already installed and latest version Package 12:dhcp-4.2.5-83.el7.centos.1.x86_64 already installed and latest version Nothing to do Disable and stop firewalld as we will use iptables as the firewall systemctl stop firewalld && systemctl disable firewalld eth1 configuration must be set below: DEVICE=eth1 BOOTPROTO=none IPADDR=10.11.100.1 NETMASK=255.255.255.0 ONBOOT=yes NM_CONTROLLED=no TYPE=Ethernet DNS1=\"127.0.0.1\" After that you need to restart network service or use ifdown/up command [root@dns-dhcp ~]# systemctl restart network [root@dns-dhcp ~]# ip a 3: eth1@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 0a:bf:e2:77:5c:5f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.11.101.1/24 brd 10.11.101.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::8bf:e2ff:fe77:5c5f/64 scope link valid_lft forever preferred_lft forever DHCP server setup Edit dhcpd.conf to use spacewalk ip address as next-server for pxe default-lease-time 600; max-lease-time 7200; log-facility local7; authoritative; ignore client-updates; subnet 10.11.100.0 netmask 255.255.255.0{ range 10.11.100.10 10.11.100.50; option routers 10.11.100.1; option subnet-mask 255.255.255.0; option domain-name-servers 10.11.100.1; option domain-name \"sandbox.io\"; next-server 10.11.100.101; filename \"pxelinux.0\"; } For static IP assigning, see below as an example: host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; } Enable and start dhcpd server systemctl enable --now dhcpd You should be able to verify that dhcpd is working via journalctl Dec 06 11:52:47 dns-dhcp dhcpd[1681]: DHCPDISCOVER from de:a9:15:7b:de:e1 via eth1 Dec 06 11:52:48 dns-dhcp dhcpd[1681]: DHCPOFFER on 10.11.100.10 to de:a9:15:7b:de:e1 (client-test) via eth1 Dec 06 11:52:48 dns-dhcp dhcpd[1681]: DHCPREQUEST for 10.11.100.10 (10.11.100.1) from de:a9:15:7b:de:e1 (client-test) via eth1 Dec 06 11:52:48 dns-dhcp dhcpd[1681]: DHCPACK on 10.11.100.10 to de:a9:15:7b:de:e1 (client-test) via eth1 DNS Server setup named.conf configuration Edit /etc/named.conf file, this file is the main configuration file of the bind package manager options { listen-on port 53 { 127.0.0.1; 10.11.100.1; }; listen-on-v6 port 53 { ::1; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; recursing-file \"/var/named/data/named.recursing\"; secroots-file \"/var/named/data/named.secroots\"; allow-query { localhost; 10.11.100.0/24; }; allow-transfer { localhost; 192.168.254.254; }; recursion yes; dnssec-enable yes; dnssec-validation yes; bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; pid-file \"/run/named/named.pid\"; session-keyfile \"/run/named/session.key\"; }; On the lower part of /etc/named.conf file, add the filenames on where you will put DNS records. zone \".\" IN { type hint; file \"named.ca\"; }; zone \"sandbox.io\" IN { type master; file \"sandbox.forward\"; allow-update { none; }; }; zone \"100.11.10.in-addr.arpa\" IN { type master; file \"sandbox.reverse\"; allow-update { none; }; }; Forward and reverse lookup zone The file sandbox.forward and sandbox.reverse that was stated in named.conf file needs to be created in /var/named touch /var/named/sandbox.{forward,reverse} [root@dhcp-dns-copy ~]# ls -ld /var/named/sandbox.{forward,reverse} -rw-r--r-- 1 root root 0 Dec 6 12:38 /var/named/sandbox.forward -rw-r--r-- 1 root root 0 Dec 6 12:38 /var/named/sandbox.reverse Both of those file will have the same heading template like below in order to work $TTL 86400 @ IN SOA dnsdhcp.sandbox.io. root.sandbox.io ( 2011071001 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL ) For every hostname, domain name should be stated with its ip address /etc/named/sandbox.forward @ IN NS dnsdhcp.sandbox.io. @ IN A 10.11.100.1 dnsdhcp IN A 10.11.100.1 dns-dhcp IN CNAME dnsdhcp #if CNAME is needed /etc/named/sandbox.reverse @ IN NS dnsdhcp.sandbox.io. dnsdhcp IN A 10.11.100.1 1 IN PTR dnsdhcp.sandbox.io. Start and enable bind service systemctl enable --now named You should have response on the DNS server when doing nslookup [root@dns-dhcp ~]# nslookup dnsdhcp Server: 127.0.0.1 Address: 127.0.0.1#53 Name: dnsdhcp.sandbox.io Address: 10.11.100.1 [root@dns-dhcp ~]# nslookup 10.11.100.1 1.100.11.10.in-addr.arpa name = dnsdhcp.sandbox.io. Firewall plus other settings Iptables firewall configuration must be configured same as below, alternatively you can add this to /etc/sysconfig/iptables then restart iptables: systemctl restart iptables *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [33:3512] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p tcp --dport 53 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p udp --dport 53 -j ACCEPT -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -i eth0 -o eth1 -m state --state RELATED,ESTABLISHED -j ACCEPT -A FORWARD -i eth1 -o eth0 -j ACCEPT -A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT *nat :PREROUTING ACCEPT [76:8121] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [6:1968] :POSTROUTING ACCEPT [6:1968] -A POSTROUTING -o eth0 -j MASQUERADE COMMIT If you want to do it on command line, just insert iptables command on every rule. Note that when dealing the Postrouting rule, we need to insert -t nat too iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE You can check the rules via iptables -L , and iptables -t nat -nvL for the postrouting rule. save the rule when done [root@dns-dhcp ~]# iptables -L Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT icmp -- anywhere anywhere ACCEPT all -- anywhere anywhere ACCEPT tcp -- anywhere anywhere state NEW tcp dpt:ssh ACCEPT tcp -- 10.11.100.0/24 anywhere tcp dpt:domain ACCEPT udp -- 10.11.100.0/24 anywhere udp dpt:domain REJECT all -- anywhere anywhere reject-with icmp-host-prohibited Chain FORWARD (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT all -- anywhere anywhere REJECT all -- anywhere anywhere reject-with icmp-host-prohibited Chain OUTPUT (policy ACCEPT) target prot opt source destination [root@dns-dhcp ~]# iptables -t nat -nvL Chain PREROUTING (policy ACCEPT 87 packets, 9331 bytes) pkts bytes target prot opt in out source destination Chain INPUT (policy ACCEPT 4 packets, 260 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 12 packets, 3420 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 12 packets, 3420 bytes) pkts bytes target prot opt in out source destination 17 1428 MASQUERADE all -- * eth0 0.0.0.0/0 0.0.0.0/0 [root@dns-dhcp ~]# iptables-save > /etc/sysconfig/iptables # or service iptables save [root@dns-dhcp ~]# systemctl restart iptables Execute this on the command line to add ip forwarding in sysctl.conf echo net.ipv4.ip_forward = 1 >> /etc/sysctl.conf Reboot the server","title":"DNS and DHCP"},{"location":"gateway/#dns-and-dhcp-server","text":"","title":"DNS and DHCP Server"},{"location":"gateway/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"gateway/#proxmox-setup","text":"Create a linux bridge on the proxmox server to isolate the network to other side Perform a reboot on the server, once it is done we can create a KVM or LXC. then after creating the virtual machine we need to other NIC on it and link it to the created linux bridge. Start the VM, ensure the KVM/LXC have a second NIC, on my end it is eth1 [root@dns-dhcp ~]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether e2:67:0e:2f:3a:7d brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.254.107/24 brd 192.168.254.255 scope global dynamic eth0 valid_lft 250156sec preferred_lft 250156sec inet6 fe80::e067:eff:fe2f:3a7d/64 scope link valid_lft forever preferred_lft forever 3: eth1@if15: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 0a:bf:e2:77:5c:5f brd ff:ff:ff:ff:ff:ff link-netnsid 0 Install necessary packages: yum install -y iptables iptables-services bind bind-utils bind-libs dhcp [root@dns-dhcp ~]# yum install -y iptables iptables-services bind bind-utils bind-libs dhcp Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: mirror.hostlink.com.hk * extras: mirror.hostlink.com.hk * updates: mirror.hostlink.com.hk Package iptables-services-1.4.21-35.el7.x86_64 already installed and latest version Package iptables-1.4.21-35.el7.x86_64 already installed and latest version Package 32:bind-9.11.4-26.P2.el7_9.8.x86_64 already installed and latest version Package 32:bind-utils-9.11.4-26.P2.el7_9.8.x86_64 already installed and latest version Package 32:bind-libs-9.11.4-26.P2.el7_9.8.x86_64 already installed and latest version Package 12:dhcp-4.2.5-83.el7.centos.1.x86_64 already installed and latest version Nothing to do Disable and stop firewalld as we will use iptables as the firewall systemctl stop firewalld && systemctl disable firewalld eth1 configuration must be set below: DEVICE=eth1 BOOTPROTO=none IPADDR=10.11.100.1 NETMASK=255.255.255.0 ONBOOT=yes NM_CONTROLLED=no TYPE=Ethernet DNS1=\"127.0.0.1\" After that you need to restart network service or use ifdown/up command [root@dns-dhcp ~]# systemctl restart network [root@dns-dhcp ~]# ip a 3: eth1@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 0a:bf:e2:77:5c:5f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.11.101.1/24 brd 10.11.101.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::8bf:e2ff:fe77:5c5f/64 scope link valid_lft forever preferred_lft forever","title":"Proxmox Setup"},{"location":"gateway/#dhcp-server-setup","text":"Edit dhcpd.conf to use spacewalk ip address as next-server for pxe default-lease-time 600; max-lease-time 7200; log-facility local7; authoritative; ignore client-updates; subnet 10.11.100.0 netmask 255.255.255.0{ range 10.11.100.10 10.11.100.50; option routers 10.11.100.1; option subnet-mask 255.255.255.0; option domain-name-servers 10.11.100.1; option domain-name \"sandbox.io\"; next-server 10.11.100.101; filename \"pxelinux.0\"; } For static IP assigning, see below as an example: host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; } Enable and start dhcpd server systemctl enable --now dhcpd You should be able to verify that dhcpd is working via journalctl Dec 06 11:52:47 dns-dhcp dhcpd[1681]: DHCPDISCOVER from de:a9:15:7b:de:e1 via eth1 Dec 06 11:52:48 dns-dhcp dhcpd[1681]: DHCPOFFER on 10.11.100.10 to de:a9:15:7b:de:e1 (client-test) via eth1 Dec 06 11:52:48 dns-dhcp dhcpd[1681]: DHCPREQUEST for 10.11.100.10 (10.11.100.1) from de:a9:15:7b:de:e1 (client-test) via eth1 Dec 06 11:52:48 dns-dhcp dhcpd[1681]: DHCPACK on 10.11.100.10 to de:a9:15:7b:de:e1 (client-test) via eth1","title":"DHCP server setup"},{"location":"gateway/#dns-server-setup","text":"","title":"DNS Server setup"},{"location":"gateway/#namedconf-configuration","text":"Edit /etc/named.conf file, this file is the main configuration file of the bind package manager options { listen-on port 53 { 127.0.0.1; 10.11.100.1; }; listen-on-v6 port 53 { ::1; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; recursing-file \"/var/named/data/named.recursing\"; secroots-file \"/var/named/data/named.secroots\"; allow-query { localhost; 10.11.100.0/24; }; allow-transfer { localhost; 192.168.254.254; }; recursion yes; dnssec-enable yes; dnssec-validation yes; bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; pid-file \"/run/named/named.pid\"; session-keyfile \"/run/named/session.key\"; }; On the lower part of /etc/named.conf file, add the filenames on where you will put DNS records. zone \".\" IN { type hint; file \"named.ca\"; }; zone \"sandbox.io\" IN { type master; file \"sandbox.forward\"; allow-update { none; }; }; zone \"100.11.10.in-addr.arpa\" IN { type master; file \"sandbox.reverse\"; allow-update { none; }; };","title":"named.conf configuration"},{"location":"gateway/#forward-and-reverse-lookup-zone","text":"The file sandbox.forward and sandbox.reverse that was stated in named.conf file needs to be created in /var/named touch /var/named/sandbox.{forward,reverse} [root@dhcp-dns-copy ~]# ls -ld /var/named/sandbox.{forward,reverse} -rw-r--r-- 1 root root 0 Dec 6 12:38 /var/named/sandbox.forward -rw-r--r-- 1 root root 0 Dec 6 12:38 /var/named/sandbox.reverse Both of those file will have the same heading template like below in order to work $TTL 86400 @ IN SOA dnsdhcp.sandbox.io. root.sandbox.io ( 2011071001 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL ) For every hostname, domain name should be stated with its ip address /etc/named/sandbox.forward @ IN NS dnsdhcp.sandbox.io. @ IN A 10.11.100.1 dnsdhcp IN A 10.11.100.1 dns-dhcp IN CNAME dnsdhcp #if CNAME is needed /etc/named/sandbox.reverse @ IN NS dnsdhcp.sandbox.io. dnsdhcp IN A 10.11.100.1 1 IN PTR dnsdhcp.sandbox.io. Start and enable bind service systemctl enable --now named You should have response on the DNS server when doing nslookup [root@dns-dhcp ~]# nslookup dnsdhcp Server: 127.0.0.1 Address: 127.0.0.1#53 Name: dnsdhcp.sandbox.io Address: 10.11.100.1 [root@dns-dhcp ~]# nslookup 10.11.100.1 1.100.11.10.in-addr.arpa name = dnsdhcp.sandbox.io.","title":"Forward and reverse lookup zone"},{"location":"gateway/#firewall-plus-other-settings","text":"Iptables firewall configuration must be configured same as below, alternatively you can add this to /etc/sysconfig/iptables then restart iptables: systemctl restart iptables *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [33:3512] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p tcp --dport 53 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p udp --dport 53 -j ACCEPT -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -i eth0 -o eth1 -m state --state RELATED,ESTABLISHED -j ACCEPT -A FORWARD -i eth1 -o eth0 -j ACCEPT -A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT *nat :PREROUTING ACCEPT [76:8121] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [6:1968] :POSTROUTING ACCEPT [6:1968] -A POSTROUTING -o eth0 -j MASQUERADE COMMIT If you want to do it on command line, just insert iptables command on every rule. Note that when dealing the Postrouting rule, we need to insert -t nat too iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE You can check the rules via iptables -L , and iptables -t nat -nvL for the postrouting rule. save the rule when done [root@dns-dhcp ~]# iptables -L Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT icmp -- anywhere anywhere ACCEPT all -- anywhere anywhere ACCEPT tcp -- anywhere anywhere state NEW tcp dpt:ssh ACCEPT tcp -- 10.11.100.0/24 anywhere tcp dpt:domain ACCEPT udp -- 10.11.100.0/24 anywhere udp dpt:domain REJECT all -- anywhere anywhere reject-with icmp-host-prohibited Chain FORWARD (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT all -- anywhere anywhere REJECT all -- anywhere anywhere reject-with icmp-host-prohibited Chain OUTPUT (policy ACCEPT) target prot opt source destination [root@dns-dhcp ~]# iptables -t nat -nvL Chain PREROUTING (policy ACCEPT 87 packets, 9331 bytes) pkts bytes target prot opt in out source destination Chain INPUT (policy ACCEPT 4 packets, 260 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 12 packets, 3420 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 12 packets, 3420 bytes) pkts bytes target prot opt in out source destination 17 1428 MASQUERADE all -- * eth0 0.0.0.0/0 0.0.0.0/0 [root@dns-dhcp ~]# iptables-save > /etc/sysconfig/iptables # or service iptables save [root@dns-dhcp ~]# systemctl restart iptables Execute this on the command line to add ip forwarding in sysctl.conf echo net.ipv4.ip_forward = 1 >> /etc/sysctl.conf Reboot the server","title":"Firewall plus other settings"},{"location":"logs/","text":"Logs Server Logs Server is designed to centralized all the logs of the environment. This can be done only by a simple syslog server, but since we may want to visualized the logs i am installing graylog and elasticsearch. mongodb is also included in the installation. DHCP and DNS Entries Forward lookup zone @ IN NS logs.sandbox.io. @ IN A 10.11.100.113 logs IN A 10.11.100.113 Reverse lookup zone @ IN NS logs.sandbox.io. logs IN A 10.11.100.113 113 IN PTR logs.sandbox.io. DHCP entry host logs { hardware ethernet 46:4E:3B:74:F6:A4; fixed-address 10.11.100.113; } Installation and Setup Pushing packages to spacewalk There are packages that is not available on base or epel repo of centos/spacewalk, in order to have the required packages i downloaded the packages outside first then pushing the packages on spacewalk server yum install --downloadonly --downloaddir=[download_location] [package] scp -r [download_location] spacewalk:[directory] On the spacewalk server, this is for the mongod package, you can add elasticsearch and graylog accordingly [root@spacewalk mongodb]# ls mongodb-org-4.0.27-1.el7.x86_64.rpm mongodb-org-shell-4.0.27-1.el7.x86_64.rpm mongodb-org-mongos-4.0.27-1.el7.x86_64.rpm mongodb-org-tools-4.0.27-1.el7.x86_64.rpm mongodb-org-server-4.0.27-1.el7.x86_64.rpm [root@spacewalk mongodb]# rhnpush --server=http://localhost --channel=mongodb-4-el7-x86_64 --nosig --dir=/root/files/rpm/mongodb Username: [redacted] Password: [redacted] Graylog Installation Install all package needed. - name: install packages for graylog yum: name: - wget - pwgen - java-1.8.0-openjdk-headless - elasticsearch - mongodb-org - graylog-server Change elasticsearch.yml cluster.name - name: replace cluster.name lineinfile: path: /etc/elasticsearch/elasticsearch.yml regexp: '^#cluster.name:' line: 'cluster.name: graylog' Change server.conf in /etc/graylog with the following - name: root_password_sha2 lineinfile: path: /etc/graylog/server/server.conf regexp: '^root_password_sha2' line: 'root_password_sha2 = a109e36947ad56de1dca1cc49f0ef8ac9ad9a7b1aa0df41fb3c4cb73c1ff01ea' - name: root_email lineinfile: path: /etc/graylog/server/server.conf regexp: '^#root_email' line: 'root_email = \"root@sandbox.io\"' - name: elasticsearch_shards lineinfile: path: /etc/graylog/server/server.conf regexp: '^elasticsearch_shards' line: 'elasticsearch_shards = 1' - name: http_bind_address lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_bind_address' line: 'http_bind_address = logs.sandbox.io:9000' For the password secret, you can generate a secret using pwgen -N 1 -s 96 - name: password_secret lineinfile: path: /etc/graylog/server/server.conf regexp: '^password_secret' line: 'password_secret = Mxz2LOVjGFs5417wEEAKaheZDjYDE0GgcfB58ISc6I4C3A97wuee2n22ajXugw9igqngw7jb5Yq6kqOhUcWIlvZDR85eIO00' When using Self Signed Certificate to the graylog server, you can refer here - name: copy cert.pem to /etc/pki/tls/certs/ copy: src: files/logs/ssl/cert.pem dest: /etc/pki/tls/certs/cert.pem - name: copy pkcs8-encrypted.pem to /etc/pki/tls/private/ copy: src: files/logs/ssl/pkcs8-encrypted.pem dest: /etc/pki/tls/private/pkcs8-encrypted.pem # on server.conf - name: http_enable_tls lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_enable_tls' line: 'http_enable_tls = true' - name: http_tls_cert_file lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_tls_cert_file' line: 'http_tls_cert_file = /etc/pki/tls/certs/cert.pem' - name: http_tls_key_file lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_tls_key_file' line: 'http_tls_key_file = /etc/pki/tls/private/pkcs8-encrypted.pem' - name: http_tls_key_password lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_tls_key_password' line: 'http_tls_key_password = secret' You need to add the self signed certificate to JVM trust store - name: create symlink for cacerts.jks file: src: /etc/pki/ca-trust/extracted/java/cacerts dest: /etc/pki/tls/certs/cacerts.jks state: link - name: import java cert java_cert: cert_path: /etc/pki/tls/certs/cert.pem keystore_path: /etc/pki/tls/certs/cacerts.jks keystore_pass: changeit keystore_create: true cert_alias: graylog-self-signed state: present Since both graylog and elasticsearch consumes a lot of memory, you may want to lessen the memory its using. this task is optional - name: GRAYLOG_SERVER_JAVA_OPTS lineinfile: path: /etc/sysconfig/graylog-server regexp: '^GRAYLOG_SERVER_JAVA_OPTS' line: 'GRAYLOG_SERVER_JAVA_OPTS=\"-Xms512m -Xmx512m -XX:NewRatio=1 -server -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow\"' - name: 512m java options for elastic search lineinfile: path: /etc/elasticsearch/jvm.options regexp: '{{item.0}}' line: '{{item.1}}' with_nested: - ['^-Xms', '^-Xmx'] - ['-Xms512m', '-Xmx512m'] Mongod has some issues regarding SELINUX, you may want to disable it or create a selinux modules in order it to work - name: copy files copy: src: '{{item}}' dest: /tmp with_items: - 'files/logs/modules/my-ftdc-01.pp' - name: execute semodules shell: cmd: 'semodule -i /tmp/my-ftdc-01.pp' creates: /home/users/bot-acc/.system-checks/semodule_run contents of my-ftdc-01 is in binary format and can be only seen in its .te file #my-ftdc-01.te module my-ftdc 1.0; require { type proc_net_t; type mongod_t; class file { open read }; } #============= mongod_t ============== allow mongod_t proc_net_t:file open; #!!!! This avc is allowed in the current policy allow mongod_t proc_net_t:file read; Open ports for graylog to work with - name: enable firewalld for rsyslog firewalld: port: '{{item}}' immediate: true permanent: true state: enabled with_items: - 1514/udp - 9000/tcp If graylog is newly installed, you may want to perform a daemon reload. Enable and start the services right after - name: daemon reload systemd: daemon_reload: true - name: enable and start services service: name: '{{item}}' state: started enabled: true with_items: - elasticsearch - mongod - graylog-server Browse to https://logs.sandbox.io:9000 to access graylog UI Create a new input by navigating to system -> inputs Then create new syslog UDP input Wait until the newly created input to run, after that you should start collecting logs data from servers Syslog configuration per node In order for servers to send logs on graylog, we need to set it up on /etc/rsyslog.conf on each server, then restart the service. - name: add rsyslog server details lineinfile: path: /etc/rsyslog.conf line: \"*.info;mail.none;authpriv.none;cron.none @logs.sandbox.io:1514\" - name: start and enable snmpd service: name: '{{item}}' state: restarted enabled: true with_items: - 'rsyslog'","title":"Logs"},{"location":"logs/#logs-server","text":"Logs Server is designed to centralized all the logs of the environment. This can be done only by a simple syslog server, but since we may want to visualized the logs i am installing graylog and elasticsearch. mongodb is also included in the installation.","title":"Logs Server"},{"location":"logs/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS logs.sandbox.io. @ IN A 10.11.100.113 logs IN A 10.11.100.113 Reverse lookup zone @ IN NS logs.sandbox.io. logs IN A 10.11.100.113 113 IN PTR logs.sandbox.io. DHCP entry host logs { hardware ethernet 46:4E:3B:74:F6:A4; fixed-address 10.11.100.113; }","title":"DHCP and DNS Entries"},{"location":"logs/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"logs/#pushing-packages-to-spacewalk","text":"There are packages that is not available on base or epel repo of centos/spacewalk, in order to have the required packages i downloaded the packages outside first then pushing the packages on spacewalk server yum install --downloadonly --downloaddir=[download_location] [package] scp -r [download_location] spacewalk:[directory] On the spacewalk server, this is for the mongod package, you can add elasticsearch and graylog accordingly [root@spacewalk mongodb]# ls mongodb-org-4.0.27-1.el7.x86_64.rpm mongodb-org-shell-4.0.27-1.el7.x86_64.rpm mongodb-org-mongos-4.0.27-1.el7.x86_64.rpm mongodb-org-tools-4.0.27-1.el7.x86_64.rpm mongodb-org-server-4.0.27-1.el7.x86_64.rpm [root@spacewalk mongodb]# rhnpush --server=http://localhost --channel=mongodb-4-el7-x86_64 --nosig --dir=/root/files/rpm/mongodb Username: [redacted] Password: [redacted]","title":"Pushing packages to spacewalk"},{"location":"logs/#graylog-installation","text":"Install all package needed. - name: install packages for graylog yum: name: - wget - pwgen - java-1.8.0-openjdk-headless - elasticsearch - mongodb-org - graylog-server Change elasticsearch.yml cluster.name - name: replace cluster.name lineinfile: path: /etc/elasticsearch/elasticsearch.yml regexp: '^#cluster.name:' line: 'cluster.name: graylog' Change server.conf in /etc/graylog with the following - name: root_password_sha2 lineinfile: path: /etc/graylog/server/server.conf regexp: '^root_password_sha2' line: 'root_password_sha2 = a109e36947ad56de1dca1cc49f0ef8ac9ad9a7b1aa0df41fb3c4cb73c1ff01ea' - name: root_email lineinfile: path: /etc/graylog/server/server.conf regexp: '^#root_email' line: 'root_email = \"root@sandbox.io\"' - name: elasticsearch_shards lineinfile: path: /etc/graylog/server/server.conf regexp: '^elasticsearch_shards' line: 'elasticsearch_shards = 1' - name: http_bind_address lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_bind_address' line: 'http_bind_address = logs.sandbox.io:9000' For the password secret, you can generate a secret using pwgen -N 1 -s 96 - name: password_secret lineinfile: path: /etc/graylog/server/server.conf regexp: '^password_secret' line: 'password_secret = Mxz2LOVjGFs5417wEEAKaheZDjYDE0GgcfB58ISc6I4C3A97wuee2n22ajXugw9igqngw7jb5Yq6kqOhUcWIlvZDR85eIO00' When using Self Signed Certificate to the graylog server, you can refer here - name: copy cert.pem to /etc/pki/tls/certs/ copy: src: files/logs/ssl/cert.pem dest: /etc/pki/tls/certs/cert.pem - name: copy pkcs8-encrypted.pem to /etc/pki/tls/private/ copy: src: files/logs/ssl/pkcs8-encrypted.pem dest: /etc/pki/tls/private/pkcs8-encrypted.pem # on server.conf - name: http_enable_tls lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_enable_tls' line: 'http_enable_tls = true' - name: http_tls_cert_file lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_tls_cert_file' line: 'http_tls_cert_file = /etc/pki/tls/certs/cert.pem' - name: http_tls_key_file lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_tls_key_file' line: 'http_tls_key_file = /etc/pki/tls/private/pkcs8-encrypted.pem' - name: http_tls_key_password lineinfile: path: /etc/graylog/server/server.conf regexp: '^#http_tls_key_password' line: 'http_tls_key_password = secret' You need to add the self signed certificate to JVM trust store - name: create symlink for cacerts.jks file: src: /etc/pki/ca-trust/extracted/java/cacerts dest: /etc/pki/tls/certs/cacerts.jks state: link - name: import java cert java_cert: cert_path: /etc/pki/tls/certs/cert.pem keystore_path: /etc/pki/tls/certs/cacerts.jks keystore_pass: changeit keystore_create: true cert_alias: graylog-self-signed state: present Since both graylog and elasticsearch consumes a lot of memory, you may want to lessen the memory its using. this task is optional - name: GRAYLOG_SERVER_JAVA_OPTS lineinfile: path: /etc/sysconfig/graylog-server regexp: '^GRAYLOG_SERVER_JAVA_OPTS' line: 'GRAYLOG_SERVER_JAVA_OPTS=\"-Xms512m -Xmx512m -XX:NewRatio=1 -server -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow\"' - name: 512m java options for elastic search lineinfile: path: /etc/elasticsearch/jvm.options regexp: '{{item.0}}' line: '{{item.1}}' with_nested: - ['^-Xms', '^-Xmx'] - ['-Xms512m', '-Xmx512m'] Mongod has some issues regarding SELINUX, you may want to disable it or create a selinux modules in order it to work - name: copy files copy: src: '{{item}}' dest: /tmp with_items: - 'files/logs/modules/my-ftdc-01.pp' - name: execute semodules shell: cmd: 'semodule -i /tmp/my-ftdc-01.pp' creates: /home/users/bot-acc/.system-checks/semodule_run contents of my-ftdc-01 is in binary format and can be only seen in its .te file #my-ftdc-01.te module my-ftdc 1.0; require { type proc_net_t; type mongod_t; class file { open read }; } #============= mongod_t ============== allow mongod_t proc_net_t:file open; #!!!! This avc is allowed in the current policy allow mongod_t proc_net_t:file read; Open ports for graylog to work with - name: enable firewalld for rsyslog firewalld: port: '{{item}}' immediate: true permanent: true state: enabled with_items: - 1514/udp - 9000/tcp If graylog is newly installed, you may want to perform a daemon reload. Enable and start the services right after - name: daemon reload systemd: daemon_reload: true - name: enable and start services service: name: '{{item}}' state: started enabled: true with_items: - elasticsearch - mongod - graylog-server Browse to https://logs.sandbox.io:9000 to access graylog UI Create a new input by navigating to system -> inputs Then create new syslog UDP input Wait until the newly created input to run, after that you should start collecting logs data from servers","title":"Graylog Installation"},{"location":"logs/#syslog-configuration-per-node","text":"In order for servers to send logs on graylog, we need to set it up on /etc/rsyslog.conf on each server, then restart the service. - name: add rsyslog server details lineinfile: path: /etc/rsyslog.conf line: \"*.info;mail.none;authpriv.none;cron.none @logs.sandbox.io:1514\" - name: start and enable snmpd service: name: '{{item}}' state: restarted enabled: true with_items: - 'rsyslog'","title":"Syslog configuration per node"},{"location":"mail/","text":"Mail Server Design of the mail server is to gather all emails on the sandbox infastructure, once enough emails are gathered the emails will will be forwarded to my email DHCP and DNS Entries Forward lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 sandbox.io. IN MX 111 mail The MX record will forward all emails that is pointed to @sandbox.io to mail.sandbox.io Reverse lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 111 IN PTR mail.sandbox.io. DHCP entry host mail { hardware ethernet 02:AE:24:24:B7:7A; fixed-address 10.11.100.111; } Installation and Setup Install postfix, mailx, cyrus-sasl-plain packages - name: install postfix and other packages yum: name: - postfix - mailx - cyrus-sasl-plain state: present Open smtp port on the firewall - name: open smtp port 25 to firewalld firewalld: service: smtp immediate: true permanent: true state: enabled We need to change interface on postfix configuration file as we need the server to listen on its IP address to gather emails name: change inet_interface configuration on /etc/postfix/main.cf lineinfile: path: /etc/postfix/main.cf regexp: \"^inet_interfaces\" line: \"inet_interfaces = {{ ansible_fqdn }}\" register: interfaces_reg Append configuration settings on postfix main.cf file - name: append block of file on end of main.cf blockinfile: path: /etc/postfix/main.cf block: | myhostname = sandbox.io relayhost = [smtp.gmail.com]:587 smtp_use_tls = yes smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt smtp_sasl_security_options = noanonymous smtp_sasl_tls_security_options = noanonymous Create sasl_passwd to /etc/postfix directory, this will contain your gmail username and password so apply security if possible - name: copy sasl_passwd to /etc/postfix copy: src: files/mail/sasl_passwd dest: /etc/postfix/sasl_passwd Contents: [root@mail ~]# cat /etc/postfix/sasl_passwd [smtp.gmail.com]:587 [username]:[password] With sasl_passwd created, we can execute postmap binary to produce sasl db - name: execute shell command if sasl_db is does not exist shell: cmd: postmap /etc/postfix/sasl_passwd when: not sasl_db.stat.exists Restart the postfix server and enable it - name: start postfix service: name: postfix state: restarted enabled: true copy the file send_mail.sh to /root, it will be the shell to run every 10 minutes to check if there is enough emails to forward to my gmail account - name: copy files/mail/send_mail.sh to /root copy: src: files/mail/send_mail.sh dest: /root/send_mail.sh mode: '0755' Contents of send_mail.sh #!/bin/bash if ! [ -e /var/spool/mail/root ] ; then logger 'root mail does not exist' exit fi if [ -v $1 ] ; then echo './send_mail.sh $1' exit fi MAIL_COUNT=`grep \"^Subject:\" /var/spool/mail/root | wc -l` DATE_ATTACH=`date +%s` EMAIL=$1 if [ $MAIL_COUNT -ge 5 ]; then cat /var/spool/mail/root | sed -e '/^From /,+8d' | sed -e '/^User-Agent/,+4d' > /tmp/mail_body printf '\\n\\n-------------------------\\nSee attachment for full details'>> /tmp/mail_body cp /var/spool/mail/root /tmp/mail_attachment_$DATE_ATTACH mail -a /tmp/mail_attachment_$DATE_ATTACH -s \"All mails on `hostname -f`: `date \"+%m-%d-%Y %I:%M %p\"`\" $EMAIL < /tmp/mail_body if [ $? -eq 0 ]; then logger 'MAILER: Email sent' logger 'MAILER: Deleting mail logs' echo -n '' > /var/spool/mail/root logger 'MAILER: Deleting temporary files' rm -rf /tmp/mail_attachment_$DATE_ATTACH rm -rf /tmp/mail_body else logger 'MAILER: Email not sent' fi else logger 'MAILER: Not enough mails, skipping...' fi Create cron job for send_mail.sh - name: create cron job for send_mail.sh copy: src: files/mail/cron_mailer dest: /etc/cron.d/mailer Contents of mailer cronjob */10 * * * * root /root/send_mail.sh [email_address] Email from the mail server should look like below","title":"Mail"},{"location":"mail/#mail-server","text":"Design of the mail server is to gather all emails on the sandbox infastructure, once enough emails are gathered the emails will will be forwarded to my email","title":"Mail Server"},{"location":"mail/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 sandbox.io. IN MX 111 mail The MX record will forward all emails that is pointed to @sandbox.io to mail.sandbox.io Reverse lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 111 IN PTR mail.sandbox.io. DHCP entry host mail { hardware ethernet 02:AE:24:24:B7:7A; fixed-address 10.11.100.111; }","title":"DHCP and DNS Entries"},{"location":"mail/#installation-and-setup","text":"Install postfix, mailx, cyrus-sasl-plain packages - name: install postfix and other packages yum: name: - postfix - mailx - cyrus-sasl-plain state: present Open smtp port on the firewall - name: open smtp port 25 to firewalld firewalld: service: smtp immediate: true permanent: true state: enabled We need to change interface on postfix configuration file as we need the server to listen on its IP address to gather emails name: change inet_interface configuration on /etc/postfix/main.cf lineinfile: path: /etc/postfix/main.cf regexp: \"^inet_interfaces\" line: \"inet_interfaces = {{ ansible_fqdn }}\" register: interfaces_reg Append configuration settings on postfix main.cf file - name: append block of file on end of main.cf blockinfile: path: /etc/postfix/main.cf block: | myhostname = sandbox.io relayhost = [smtp.gmail.com]:587 smtp_use_tls = yes smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt smtp_sasl_security_options = noanonymous smtp_sasl_tls_security_options = noanonymous Create sasl_passwd to /etc/postfix directory, this will contain your gmail username and password so apply security if possible - name: copy sasl_passwd to /etc/postfix copy: src: files/mail/sasl_passwd dest: /etc/postfix/sasl_passwd Contents: [root@mail ~]# cat /etc/postfix/sasl_passwd [smtp.gmail.com]:587 [username]:[password] With sasl_passwd created, we can execute postmap binary to produce sasl db - name: execute shell command if sasl_db is does not exist shell: cmd: postmap /etc/postfix/sasl_passwd when: not sasl_db.stat.exists Restart the postfix server and enable it - name: start postfix service: name: postfix state: restarted enabled: true copy the file send_mail.sh to /root, it will be the shell to run every 10 minutes to check if there is enough emails to forward to my gmail account - name: copy files/mail/send_mail.sh to /root copy: src: files/mail/send_mail.sh dest: /root/send_mail.sh mode: '0755' Contents of send_mail.sh #!/bin/bash if ! [ -e /var/spool/mail/root ] ; then logger 'root mail does not exist' exit fi if [ -v $1 ] ; then echo './send_mail.sh $1' exit fi MAIL_COUNT=`grep \"^Subject:\" /var/spool/mail/root | wc -l` DATE_ATTACH=`date +%s` EMAIL=$1 if [ $MAIL_COUNT -ge 5 ]; then cat /var/spool/mail/root | sed -e '/^From /,+8d' | sed -e '/^User-Agent/,+4d' > /tmp/mail_body printf '\\n\\n-------------------------\\nSee attachment for full details'>> /tmp/mail_body cp /var/spool/mail/root /tmp/mail_attachment_$DATE_ATTACH mail -a /tmp/mail_attachment_$DATE_ATTACH -s \"All mails on `hostname -f`: `date \"+%m-%d-%Y %I:%M %p\"`\" $EMAIL < /tmp/mail_body if [ $? -eq 0 ]; then logger 'MAILER: Email sent' logger 'MAILER: Deleting mail logs' echo -n '' > /var/spool/mail/root logger 'MAILER: Deleting temporary files' rm -rf /tmp/mail_attachment_$DATE_ATTACH rm -rf /tmp/mail_body else logger 'MAILER: Email not sent' fi else logger 'MAILER: Not enough mails, skipping...' fi Create cron job for send_mail.sh - name: create cron job for send_mail.sh copy: src: files/mail/cron_mailer dest: /etc/cron.d/mailer Contents of mailer cronjob */10 * * * * root /root/send_mail.sh [email_address] Email from the mail server should look like below","title":"Installation and Setup"},{"location":"monitoring/","text":"Monitoring Server Monitoring server is design to monitor the systems in the environment via SNMP. The application that will run on this server is Nagios I want to try to use net-snmp as much as possible and dont want to install NRPE on each device, kinda making it agentless monitoring system. DHCP and DNS Entries Forward lookup zone @ IN NS monitoring.sandbox.io. @ IN A 10.11.100.112 monitoring IN A 10.11.100.112 Reverse lookup zone @ IN NS monitoring.sandbox.io. monitoring IN A 10.11.100.112 112 IN PTR monitoring.sandbox.io. DHCP entry host monitoring { hardware ethernet 22:B2:C3:62:05:96; fixed-address 10.11.100.112; } Installation and Setup Install Nagios and necessary files - name: install necessary files yum: name: - nagios - nagios-selinux - nagios-plugins - nagios-plugins-ping - nagios-plugins-http - nagios-plugins-load - nagios-plugins-ssh - nagios-plugins-snmp - nagios-plugins-tcp - nagios-common - python2-passlib state: present Open HTTP port on firewall - name: open firewall ports firewalld: service: http state: enabled permanent: true immediate: true Configure nagios.cfg file to use /etc/nagios/servers as location of the servers and what to monitor to them - name: add configuration folder configuration in nagios.cfg lineinfile: regexp: '^#cfg_dir=/etc/nagios/servers' line: 'cfg_dir=/etc/nagios/servers' path: /etc/nagios/nagios.cfg /etc/nagios/servers must be cleaned first before adding files, this is due to i want to edit some configuration files on this, some remnants of the this i edit may persist especially if i delete servers configurations on the main repo. This would be followed by creating the directory again - name: clean server directory first before adding files file: path: /etc/nagios/servers state: absent - name: create what have you deleted xd file: state: directory mode: '0755' path: /etc/nagios/servers Copy the contents of /usr/share/nagios/html to /var/ww/html - name: copy web contents of nagios to html root directory copy: src: /usr/share/nagios/html/ dest: /var/www/html directory_mode: true remote_src: yes owner: apache group: apache Replace email on contacts.cfg - name: replace mail email replace: regexp: 'nagios@localhost' replace: 'root@sandbox.io' path: /etc/nagios/objects/contacts.cfg Copy created SNMP nagios plugins to /usr/lib64/nagios/plugins - name: copy created snmp nagios plugins to /usr/lib64/nagios/plugins copy: src: '{{item}}' dest: /usr/lib64/nagios/plugins mode: \"755\" with_items: - 'files/nagios/tools/check_snmp_memory' - 'files/nagios/tools/check_snmp_disk' - 'files/nagios/tools/check_snmp_app' - 'files/nagios/tools/check_snmp_load' Add additional commands for nagios to follow when monitoring the servers - name: additional functions to commands.cfg blockinfile: path: /etc/nagios/objects/commands.cfg block: | define command { command_name check_snmp_memory command_line $USER1$/check_snmp_memory $HOSTADDRESS$ $ARG1$ $ARG2$ } define command { command_name check_snmp_disk command_line $USER1$/check_snmp_disk $HOSTADDRESS$ $ARG1$ $ARG2$ } define command { command_name check_snmp_app command_line $USER1$/check_snmp_app $HOSTADDRESS$ $ARG1$ } define command { command_name check_snmp_load command_line $USER1$/check_snmp_load $HOSTADDRESS$ $ARG1$ $ARG2$ } check_snmp_memory is using a calculation of (total_memory - ( free_memory + cache_memory)) / total memory to calculate the memory space available. I have included cache_memory since that cache_memory is an unused memory the kernel is borrowing for a while and will be available once processes needs it. #!/bin/bash Help() { # Display Help echo \"Syntax: ./check_snmp_memory [host] [warning_value_int] [critical_value_int]\" echo \"Warning value should not be greater than Critical value\" } if [ $2 -gt $3 ]; then Help exit 1 fi if [ \"$#\" -ne 3 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"CRITICAL: No response to $1\" exit 2 fi FREE_MEM=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.4.6 | cut -d \" \" -f 1` CACHE_MEM=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.4.15 | cut -d \" \" -f 1` TOTAL_MEM=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.4.5 | cut -d \" \" -f 1` MEM_PERCENT=`echo \"$(( (($TOTAL_MEM - ($FREE_MEM + $CACHE_MEM)) * 100) / $TOTAL_MEM ))\"` MEM_PERCENT_STRING=`echo 'scale=2;' \"$(( ($TOTAL_MEM - ($FREE_MEM + $CACHE_MEM)) * 100 ))\" / $TOTAL_MEM | bc -l` USED_MEM_MB=`echo \"$(( ($TOTAL_MEM - ($FREE_MEM + $CACHE_MEM)) / 1024 ))\" MB` TOTAL_MEM_MB=`echo \"$(( $TOTAL_MEM/1024 ))\" MB` if [ $MEM_PERCENT -lt $2 ]; then echo \"OK: $MEM_PERCENT_STRING% - $USED_MEM_MB / $TOTAL_MEM_MB \" exit fi if [ $MEM_PERCENT -ge $2 ] && [ $MEM_PERCENT -lt $3 ]; then echo \"WARNING: $MEM_PERCENT_STRING% - $USED_MEM_MB / $TOTAL_MEM_MB\" exit 1 fi if [ $MEM_PERCENT -ge $3 ]; then echo \"CRITICAL: $MEM_PERCENT_STRING% - $USED_MEM_MB / $TOTAL_MEM_MB\" exit 2 fi check_snmp_disk checks and print the current,available and total disk space of / #!/bin/bash Help() { echo \"Syntax: ./check_snmp_storage [host] [warning_value_int] [critical_value_int]\" echo \"Warning value should not be greater than Critical value\" } if [ $2 -gt $3 ]; then Help exit 1 fi if [ \"$#\" -ne 3 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"HOST CRITICAL: No response to $1\" exit 2 fi PERCENT_SPACE=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.9.1.8.1` USED_SPACE=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.9.1.7.1` AVAIL_SPACE=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.9.1.6.1` TOTAL_SPACE=`echo $USED_SPACE + $AVAIL_SPACE | bc` USED_SPACE_GB=`echo 'scale=2;' \"$USED_SPACE / 1048576\" | bc -l` AVAIL_SPACE_GB=`echo 'scale=2;' \"$AVAIL_SPACE / 1048576\" | bc -l` TOTAL_SPACE_GB=`echo 'scale=2;' \"$TOTAL_SPACE / 1048576\" | bc -l` if [ $PERCENT_SPACE -lt $2 ]; then echo \"STORAGE OK: $PERCENT_SPACE% - $USED_SPACE_GB GB / $TOTAL_SPACE_GB GB - $AVAIL_SPACE_GB GB available\" exit fi if [ $PERCENT_SPACE -ge $2 ] && [ $PERCENT_SPACE -lt $3 ]; then echo \"STORAGE WARNING: $PERCENT_SPACE% - $USED_SPACE_GB GB / $TOTAL_SPACE_GB GB - $AVAIL_SPACE_GB GB available\" exit 1 fi if [ $PERCENT_SPACE -ge $3 ]; then echo \"STORAGE CRITICAL: $PERCENT_SPACE% - $USED_SPACE_GB GB / $TOTAL_SPACE_GB GB - $AVAIL_SPACE_GB GB available\" exit 2 fi check_snmp_app checks if process you want to monitor is running on the server #!/bin/bash Help() { echo \"Syntax: ./check_snmp_app [host] [app]\" echo \"Warning value should not be greater than Critical value\" } if [ \"$#\" -ne 2 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"HOST CRITICAL: No response to $1\" exit 2 fi PROC_COUNT=`snmpwalk -v 2c -c sandbox $1 .1.3.6.1.2.1.25.4.2.1.2 | grep -i $2 | wc -l` if [ $PROC_COUNT -eq 0 ]; then echo \"APP CRITICAL: $2 process not running\" exit 2 else echo \"APP OK: $2 process running, running instances - $PROC_COUNT\" exit fi check_snmp_load checks the load average of the server, i have change the configuration a bit and made load average into percentage(per cpu), it works for me but this is not the real CPU utilization of the server. #!/bin/bash Help() { echo \"Syntax: ./check_snmp_load [host] [warning_value_percentage] [critical_value_percentage]\" echo \"Example ./check_snmp_load test.sandbox.io 70 90\" echo \"Warning value should not be greater than Critical value\" } if [ $2 -gt $3 ]; then Help exit 1 fi if [ \"$#\" -ne 3 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"HOST CRITICAL: No response to $1\" exit 2 fi # Will count all CPU of the host, multiplying it to 100 for calculation CPU_COUNT=`snmpwalk -v 2c -c sandbox $1 1.3.6.1.2.1.25.3.3.1.2 | wc -l` CPU_CAP=`echo $CPU_COUNT '*'100 | bc` # Get average load values on 1st,5th and 15th minute. Multiplying it to 100 for calculation purposes, since it is using a value of \"0.0X\" CPU_1ST=`echo \\`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.10.1.3.0\\` '*' 100 / 1 | bc` CPU_5TH=`echo \\`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.10.1.3.1\\` '*' 100 / 1 | bc` CPU_15TH=`echo \\`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.10.1.3.2\\` '*' 100 / 1 | bc` # Checking which load average is the highest if [ $CPU_1ST -ge $CPU_5TH ] && [ $CPU_1ST -ge $CPU_15TH ]; then HI_CPU=$CPU_1ST HI_TIME=\"1 minute mark\" elif [ $CPU_5TH -gt $CPU_1ST ] && [ $CPU_5TH -ge $CPU_15TH ]; then HI_CPU=$CPU_5TH HI_TIME=\"5 minute mark\" else HI_CPU=$CPU_15TH HI_TIME=\"15 minute mark\" fi CALC_CPU=`echo 'scale=2;' $HI_CPU / $CPU_CAP '*' 100 | bc` CALC_CPU_INT=`echo $CALC_CPU / 1 | bc` if [ $CALC_CPU_INT -lt $2 ]; then echo \"LOAD AVG OK: $CALC_CPU_INT% at $HI_TIME\" exit fi if [ $CALC_CPU_INT -ge $2 ] && [ $CALC_CPU_INT -lt $3 ]; then echo \"LOAD AVG WARNING: $CALC_CPU_INT% at $HI_TIME\" exit 1 fi if [ $CALC_CPU_INT -ge $3 ]; then echo \"LOAD AVG CRITICAL: $CALC_CPU_INT% at $HI_TIME\" exit 2 fi Set the username and password when logging in to monitoring server - name: set htpasswd to nagiosadmin htpasswd: path: /etc/nagios/htpasswd.users name: [nagios_username] password: [nagios_password] Since /etc/nagios/servers is already cleaned out, we can now paste each servers configuration - name: add servers copy: src: ../files/nagios/servers/ dest: /etc/nagios/servers mode: '0644' group: nagios Servers configuration file should look like these, but can be change based on what you would look like on a configuration file define host { use linux-server host_name dev2 alias DEV server address dev2.sandbox.io max_check_attempts 5 check_interval 2 check_period 24x7 notification_interval 30 notification_period 24x7 } define service { use generic-service host_name dev2 service_description PING check_command check_ping!100.0,20%!500.0,60% } define service { use generic-service host_name dev2 service_description Memory Usage check_command check_snmp_memory!85!90 # notifications_enabled 1 } define service { use generic-service host_name dev2 service_description HTTP Port Check check_command check_tcp!80 } define service { use generic-service host_name dev2 service_description Storage Usage check_command check_snmp_disk!85!90 } define service { use generic-service host_name dev2 service_description Tomcat Service Check check_command check_snmp_app!java # Tomcat service cant be found on the process, only java which is kinda sadge } define service { use generic-service host_name dev2 service_description Load Average check_command check_snmp_load!80!90 } Start or restart nagios and httpd service - name: start the services service: name: '{{item}}' enabled: true state: restarted with_items: - nagios - httpd Example monitoring per nodes are below SNMP install for clients We need to open port 161/udp in firewalld firewall-cmd --add-service=snmp --permanent firewall-cmd --reload Install net-snmp and utils - name: install snmp tools for nagios monitoring yum: name: - \"net-snmp\" - \"net-snmp-utils\" state: present Copy snmp.conf file in the repo to /etc/snmp/snmpd.conf - name: copy snmp.conf file copy: src: files/nagios/snmp/snmpd.conf dest: /etc/snmp/snmpd.conf Start snmp service - name: start and enable snmpd service: name: snmpd state: started enabled: true","title":"Monitoring"},{"location":"monitoring/#monitoring-server","text":"Monitoring server is design to monitor the systems in the environment via SNMP. The application that will run on this server is Nagios I want to try to use net-snmp as much as possible and dont want to install NRPE on each device, kinda making it agentless monitoring system.","title":"Monitoring Server"},{"location":"monitoring/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS monitoring.sandbox.io. @ IN A 10.11.100.112 monitoring IN A 10.11.100.112 Reverse lookup zone @ IN NS monitoring.sandbox.io. monitoring IN A 10.11.100.112 112 IN PTR monitoring.sandbox.io. DHCP entry host monitoring { hardware ethernet 22:B2:C3:62:05:96; fixed-address 10.11.100.112; }","title":"DHCP and DNS Entries"},{"location":"monitoring/#installation-and-setup","text":"Install Nagios and necessary files - name: install necessary files yum: name: - nagios - nagios-selinux - nagios-plugins - nagios-plugins-ping - nagios-plugins-http - nagios-plugins-load - nagios-plugins-ssh - nagios-plugins-snmp - nagios-plugins-tcp - nagios-common - python2-passlib state: present Open HTTP port on firewall - name: open firewall ports firewalld: service: http state: enabled permanent: true immediate: true Configure nagios.cfg file to use /etc/nagios/servers as location of the servers and what to monitor to them - name: add configuration folder configuration in nagios.cfg lineinfile: regexp: '^#cfg_dir=/etc/nagios/servers' line: 'cfg_dir=/etc/nagios/servers' path: /etc/nagios/nagios.cfg /etc/nagios/servers must be cleaned first before adding files, this is due to i want to edit some configuration files on this, some remnants of the this i edit may persist especially if i delete servers configurations on the main repo. This would be followed by creating the directory again - name: clean server directory first before adding files file: path: /etc/nagios/servers state: absent - name: create what have you deleted xd file: state: directory mode: '0755' path: /etc/nagios/servers Copy the contents of /usr/share/nagios/html to /var/ww/html - name: copy web contents of nagios to html root directory copy: src: /usr/share/nagios/html/ dest: /var/www/html directory_mode: true remote_src: yes owner: apache group: apache Replace email on contacts.cfg - name: replace mail email replace: regexp: 'nagios@localhost' replace: 'root@sandbox.io' path: /etc/nagios/objects/contacts.cfg Copy created SNMP nagios plugins to /usr/lib64/nagios/plugins - name: copy created snmp nagios plugins to /usr/lib64/nagios/plugins copy: src: '{{item}}' dest: /usr/lib64/nagios/plugins mode: \"755\" with_items: - 'files/nagios/tools/check_snmp_memory' - 'files/nagios/tools/check_snmp_disk' - 'files/nagios/tools/check_snmp_app' - 'files/nagios/tools/check_snmp_load' Add additional commands for nagios to follow when monitoring the servers - name: additional functions to commands.cfg blockinfile: path: /etc/nagios/objects/commands.cfg block: | define command { command_name check_snmp_memory command_line $USER1$/check_snmp_memory $HOSTADDRESS$ $ARG1$ $ARG2$ } define command { command_name check_snmp_disk command_line $USER1$/check_snmp_disk $HOSTADDRESS$ $ARG1$ $ARG2$ } define command { command_name check_snmp_app command_line $USER1$/check_snmp_app $HOSTADDRESS$ $ARG1$ } define command { command_name check_snmp_load command_line $USER1$/check_snmp_load $HOSTADDRESS$ $ARG1$ $ARG2$ } check_snmp_memory is using a calculation of (total_memory - ( free_memory + cache_memory)) / total memory to calculate the memory space available. I have included cache_memory since that cache_memory is an unused memory the kernel is borrowing for a while and will be available once processes needs it. #!/bin/bash Help() { # Display Help echo \"Syntax: ./check_snmp_memory [host] [warning_value_int] [critical_value_int]\" echo \"Warning value should not be greater than Critical value\" } if [ $2 -gt $3 ]; then Help exit 1 fi if [ \"$#\" -ne 3 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"CRITICAL: No response to $1\" exit 2 fi FREE_MEM=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.4.6 | cut -d \" \" -f 1` CACHE_MEM=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.4.15 | cut -d \" \" -f 1` TOTAL_MEM=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.4.5 | cut -d \" \" -f 1` MEM_PERCENT=`echo \"$(( (($TOTAL_MEM - ($FREE_MEM + $CACHE_MEM)) * 100) / $TOTAL_MEM ))\"` MEM_PERCENT_STRING=`echo 'scale=2;' \"$(( ($TOTAL_MEM - ($FREE_MEM + $CACHE_MEM)) * 100 ))\" / $TOTAL_MEM | bc -l` USED_MEM_MB=`echo \"$(( ($TOTAL_MEM - ($FREE_MEM + $CACHE_MEM)) / 1024 ))\" MB` TOTAL_MEM_MB=`echo \"$(( $TOTAL_MEM/1024 ))\" MB` if [ $MEM_PERCENT -lt $2 ]; then echo \"OK: $MEM_PERCENT_STRING% - $USED_MEM_MB / $TOTAL_MEM_MB \" exit fi if [ $MEM_PERCENT -ge $2 ] && [ $MEM_PERCENT -lt $3 ]; then echo \"WARNING: $MEM_PERCENT_STRING% - $USED_MEM_MB / $TOTAL_MEM_MB\" exit 1 fi if [ $MEM_PERCENT -ge $3 ]; then echo \"CRITICAL: $MEM_PERCENT_STRING% - $USED_MEM_MB / $TOTAL_MEM_MB\" exit 2 fi check_snmp_disk checks and print the current,available and total disk space of / #!/bin/bash Help() { echo \"Syntax: ./check_snmp_storage [host] [warning_value_int] [critical_value_int]\" echo \"Warning value should not be greater than Critical value\" } if [ $2 -gt $3 ]; then Help exit 1 fi if [ \"$#\" -ne 3 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"HOST CRITICAL: No response to $1\" exit 2 fi PERCENT_SPACE=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.9.1.8.1` USED_SPACE=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.9.1.7.1` AVAIL_SPACE=`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.9.1.6.1` TOTAL_SPACE=`echo $USED_SPACE + $AVAIL_SPACE | bc` USED_SPACE_GB=`echo 'scale=2;' \"$USED_SPACE / 1048576\" | bc -l` AVAIL_SPACE_GB=`echo 'scale=2;' \"$AVAIL_SPACE / 1048576\" | bc -l` TOTAL_SPACE_GB=`echo 'scale=2;' \"$TOTAL_SPACE / 1048576\" | bc -l` if [ $PERCENT_SPACE -lt $2 ]; then echo \"STORAGE OK: $PERCENT_SPACE% - $USED_SPACE_GB GB / $TOTAL_SPACE_GB GB - $AVAIL_SPACE_GB GB available\" exit fi if [ $PERCENT_SPACE -ge $2 ] && [ $PERCENT_SPACE -lt $3 ]; then echo \"STORAGE WARNING: $PERCENT_SPACE% - $USED_SPACE_GB GB / $TOTAL_SPACE_GB GB - $AVAIL_SPACE_GB GB available\" exit 1 fi if [ $PERCENT_SPACE -ge $3 ]; then echo \"STORAGE CRITICAL: $PERCENT_SPACE% - $USED_SPACE_GB GB / $TOTAL_SPACE_GB GB - $AVAIL_SPACE_GB GB available\" exit 2 fi check_snmp_app checks if process you want to monitor is running on the server #!/bin/bash Help() { echo \"Syntax: ./check_snmp_app [host] [app]\" echo \"Warning value should not be greater than Critical value\" } if [ \"$#\" -ne 2 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"HOST CRITICAL: No response to $1\" exit 2 fi PROC_COUNT=`snmpwalk -v 2c -c sandbox $1 .1.3.6.1.2.1.25.4.2.1.2 | grep -i $2 | wc -l` if [ $PROC_COUNT -eq 0 ]; then echo \"APP CRITICAL: $2 process not running\" exit 2 else echo \"APP OK: $2 process running, running instances - $PROC_COUNT\" exit fi check_snmp_load checks the load average of the server, i have change the configuration a bit and made load average into percentage(per cpu), it works for me but this is not the real CPU utilization of the server. #!/bin/bash Help() { echo \"Syntax: ./check_snmp_load [host] [warning_value_percentage] [critical_value_percentage]\" echo \"Example ./check_snmp_load test.sandbox.io 70 90\" echo \"Warning value should not be greater than Critical value\" } if [ $2 -gt $3 ]; then Help exit 1 fi if [ \"$#\" -ne 3 ]; then Help exit 1 fi snmpwalk -v2c -c sandbox $1 > /dev/null 2>&1 if [ $? -gt 0 ]; then echo \"HOST CRITICAL: No response to $1\" exit 2 fi # Will count all CPU of the host, multiplying it to 100 for calculation CPU_COUNT=`snmpwalk -v 2c -c sandbox $1 1.3.6.1.2.1.25.3.3.1.2 | wc -l` CPU_CAP=`echo $CPU_COUNT '*'100 | bc` # Get average load values on 1st,5th and 15th minute. Multiplying it to 100 for calculation purposes, since it is using a value of \"0.0X\" CPU_1ST=`echo \\`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.10.1.3.0\\` '*' 100 / 1 | bc` CPU_5TH=`echo \\`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.10.1.3.1\\` '*' 100 / 1 | bc` CPU_15TH=`echo \\`snmpgetnext -Oqv -v 2c -c sandbox $1 .1.3.6.1.4.1.2021.10.1.3.2\\` '*' 100 / 1 | bc` # Checking which load average is the highest if [ $CPU_1ST -ge $CPU_5TH ] && [ $CPU_1ST -ge $CPU_15TH ]; then HI_CPU=$CPU_1ST HI_TIME=\"1 minute mark\" elif [ $CPU_5TH -gt $CPU_1ST ] && [ $CPU_5TH -ge $CPU_15TH ]; then HI_CPU=$CPU_5TH HI_TIME=\"5 minute mark\" else HI_CPU=$CPU_15TH HI_TIME=\"15 minute mark\" fi CALC_CPU=`echo 'scale=2;' $HI_CPU / $CPU_CAP '*' 100 | bc` CALC_CPU_INT=`echo $CALC_CPU / 1 | bc` if [ $CALC_CPU_INT -lt $2 ]; then echo \"LOAD AVG OK: $CALC_CPU_INT% at $HI_TIME\" exit fi if [ $CALC_CPU_INT -ge $2 ] && [ $CALC_CPU_INT -lt $3 ]; then echo \"LOAD AVG WARNING: $CALC_CPU_INT% at $HI_TIME\" exit 1 fi if [ $CALC_CPU_INT -ge $3 ]; then echo \"LOAD AVG CRITICAL: $CALC_CPU_INT% at $HI_TIME\" exit 2 fi Set the username and password when logging in to monitoring server - name: set htpasswd to nagiosadmin htpasswd: path: /etc/nagios/htpasswd.users name: [nagios_username] password: [nagios_password] Since /etc/nagios/servers is already cleaned out, we can now paste each servers configuration - name: add servers copy: src: ../files/nagios/servers/ dest: /etc/nagios/servers mode: '0644' group: nagios Servers configuration file should look like these, but can be change based on what you would look like on a configuration file define host { use linux-server host_name dev2 alias DEV server address dev2.sandbox.io max_check_attempts 5 check_interval 2 check_period 24x7 notification_interval 30 notification_period 24x7 } define service { use generic-service host_name dev2 service_description PING check_command check_ping!100.0,20%!500.0,60% } define service { use generic-service host_name dev2 service_description Memory Usage check_command check_snmp_memory!85!90 # notifications_enabled 1 } define service { use generic-service host_name dev2 service_description HTTP Port Check check_command check_tcp!80 } define service { use generic-service host_name dev2 service_description Storage Usage check_command check_snmp_disk!85!90 } define service { use generic-service host_name dev2 service_description Tomcat Service Check check_command check_snmp_app!java # Tomcat service cant be found on the process, only java which is kinda sadge } define service { use generic-service host_name dev2 service_description Load Average check_command check_snmp_load!80!90 } Start or restart nagios and httpd service - name: start the services service: name: '{{item}}' enabled: true state: restarted with_items: - nagios - httpd Example monitoring per nodes are below","title":"Installation and Setup"},{"location":"monitoring/#snmp-install-for-clients","text":"We need to open port 161/udp in firewalld firewall-cmd --add-service=snmp --permanent firewall-cmd --reload Install net-snmp and utils - name: install snmp tools for nagios monitoring yum: name: - \"net-snmp\" - \"net-snmp-utils\" state: present Copy snmp.conf file in the repo to /etc/snmp/snmpd.conf - name: copy snmp.conf file copy: src: files/nagios/snmp/snmpd.conf dest: /etc/snmp/snmpd.conf Start snmp service - name: start and enable snmpd service: name: snmpd state: started enabled: true","title":"SNMP install for clients"},{"location":"spacewalk/","text":"Spacewalk documentation DHCP and DNS Entries Forward lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 Reverse lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 101 IN PTR spacewalk.sandbox.io. DHCP entry host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; } Installation Source installation instructions is found here , documenting for my own copy. Prerequisites Outbound open ports 80, 443 Inbound open ports 80, 443, 5222 (only if you want to push actions to client machines) and 5269 (only for push actions to a Spacewalk Proxy), 69 udp if you want to use tftp Storage for database: 250 KiB per client system + 500 KiB per channel + 230 KiB per package in channel (i.e. 1.1GiB for channel with 5000 packages) Storage for packages (default /var/satellite): Depends on what you are storing; Red Hat recommend 6GB per channel for their channels 2GB RAM minimum, 4GB recommended Make sure your underlying OS is fully up-to-date. If you use LDAP as a central identity service and wish to pull user and group information from it, see SpacewalkWithLDAP Additional repos & packages All other dependecies outside base repo will be available on EPEL repository rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm Database server I will let spacewalk setup postgresql database first, before moving it to a separate server. yum -y install spacewalk-setup-postgresql Installing Spacewalk yum -y install spacewalk-postgresql Configuring the Firewall Firewalld is installed on my server so the commands i will use is based on firewalld commands firewall-cmd --add-service=http firewall-cmd --add-service=https firewall-cmd --runtime-to-perm Configuring Spacewalk Please make sure that your spacewalk server have a resolvable FQDN. spacewalk-setup An example session is as follows: [root@spacewalk ~]# spacewalk-setup * Setting up SELinux.. ** Database: Setting up database connection for PostgreSQL backend. ** Database: Installing the database: ** Database: This is a long process that is logged in: ** Database: /var/log/rhn/install_db.log *** Progress: # ** Database: Installation complete. ** Database: Populating database. *** Progress: ############################ * Configuring tomcat. * Setting up users and groups. ** GPG: Initializing GPG and importing key. ** GPG: Creating /root/.gnupg directory You must enter an email address. Admin Email Address? [redacted] * Performing initial configuration. ** Package installation: Locking required rpm versions. * Configuring apache SSL virtual host. Should setup configure apaches default ssl server for you (saves original ssl.conf) [Y]? ** /etc/httpd/conf.d/ssl.conf has been backed up to ssl.conf-swsave * Configuring jabberd. * Creating SSL certificates. CA certificate password? password Re-enter CA certificate password? password Cname alias of the machine (comma seperated)? spacewalk.sandbox.io Organization? sandbox Organization Unit [spacewalk.sandbox.io]? Email Address [[redacted]]? City? [redacted] State? [redacted] Country code (Examples: \"US\", \"JP\", \"IN\", or type \"?\" to see a list)? [redacted] ** SSL: Generating CA certificate. ** SSL: Deploying CA certificate. ** SSL: Generating server certificate. ** SSL: Storing SSL certificates. * Deploying configuration files. * Update configuration in database. * Setting up Cobbler.. Cobbler requires tftp and xinetd services be turned on for PXE provisioning functionality. Enable these services [Y]? y * Restarting services. Installation complete. Visit https://spacewalk.sandbox.io to create the Spacewalk administrator account. Creating Spacewalk CentOS channel Creating software channel Channels -> Manage Software Channels -> Create Channel Fill up the fields accordingly: Channel Name: Centos 7 Base - x86_64 Channel Label: centos7-base-x86_64 Architecture: x86_64 Yum Repository Checksum Type: sha1 Channel Summary: Centos 7 Base - x86_64 Optional: For additional security, add GPG key fingerprint in Security:GPG section, you can get GPG key using gpg --with-fingerprint [key] . cd /etc/pki/rpm-gpg/ gpg --with-fingerprint RPM-GPG-KEY-CentOS-7 pub 4096R/F4A80EB5 2014-06-23 CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org> Key fingerprint = 6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5 Creating repositories Channels -> Manage Software Channels -> Manage Repositories -> Create Repository Repository Label: centos7-base-x86_64 Repository URL: http://mirror.centos.org/centos/7/os/x86_64/ Repository Type: yum Syncing the channel and the repos In the command line type: spacewalk-repo-sync -c centos7-base-x86_64 Creating Base Children Channels Creating Updates Channel Create a new channel again, now making the Base channel as the parent channel: Channel Name: Centos 7 Updates - x86_64 Channel Label: centos7-updates-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos7-updates-x86_64 Security:GPG section in this channel is the same with the parent channel: GPG key URL: http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7 GPG key ID: F4A80EB5 GPG key Fingerprint: 6341 AB27 53D78A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5 Click Create Channel to create the children channel Creating EPEL Channel Same with Updates channel, make the base channel as parent channel: Channel Name: Epel 7 - x86_64 Channel Label: epel7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: Extra Packages for Enterprise Linux 7 - x86_64 As for the Security: GPG, change the details to: GPG key URL: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 GPG key ID: 352C64E5 GPG key Fingerprint: 91E9 7D7C 4A5E 96F1 7F3E 888F 6A2F AEA2 352C 64E5 Alternatively, you can type gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 to the terminal. Creating Spacewalk Client Channel Same with Updates and EPEL channel, make the base channel as parent channel: Channel Name: Spacewalk Client 2.10 - x86_64 Channel Label: spacewalk-client-210-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: spacewalk-client-210-x86_64 As for the Security: GPG, change the details to: GPG key URL: https://download.copr.fedorainfracloud.org/results/@spacewalkproject/spacewalk-2.10/pubkey.gpg GPG key ID: BC2E6843 GPG key Fingerprint: E8C0 573E 5B62 BB7C 98C1 A2AC 770C E53E BC2E 6843 Creating Extras Channel Channel Name: Centos 7 Extras - x86_64 Channel Label: centos-7-extras-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos-7-extras-x86_64 Creating Puppet Channel This Channel is for the automation server, puppet-server package in EPEL is quite outdated and i want to use the new version. Channel Name: Puppet 7 EL7 x86_64 Channel Label: puppet-7-el7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: puppet-7-el7-x86_64 Creating Repositories for Children Channels Create a repository for each children channels: Repository URL for centos7-updates-x86_64: http://mirror.rise.ph/centos/7.8.2003/updates/x86_64/ # This might be changed since the updated CentOS version upon this writing is 7.8 Repository URL for epel7-x86_64: https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/ Repository URL for spacewalk-client-210-x86_64: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10-client/epel-7-x86_64/ Repository URL for centos-7-extras-x86_64: http://mirror.centos.org/centos/7/extras/x86_64/ Repositort URL for puppet-7-el7-x86_64: http://yum.puppetlabs.com/puppet/el/7/x86_64/ Pushing packages to spacewalk repo Add created repository for each children channels and sync the channels through spacewalk website or terminal: spacewalk-repo-sync -c spacewalk-client-210-x86_64 spacewalk-repo-sync -c centos7-updates-x86_64 Example output: [root@spacewalk rpm-gpg]# spacewalk-repo-sync -c spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 | Channel: spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 Sync of channel started. 22:50:25 22:50:25 Processing repository with URL: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10/epel-7-x86_64/ 22:50:27 Packages in repo: 206 22:50:27 Packages already synced: 0 22:50:27 Packages to sync: 138 22:50:27 New packages to download: 138 22:50:27 Downloading packages: Due to issues with EPEL repository is too big to sync to spacewalk, you may want to download the rpm packages you want to be in the EPEL channel, then push it to spacewalk rhnpush -v --channel=epel7-x86_64 --server=http://localhost --dir=/root/epel-packages Distribution and Kickstart Profile Creating Distributions On Systems tab -> Kickstart -> Distribution -> Create a new Distribution Download the latest CentOS ISO file: wget http://mirror.pregi.net/centos/7.8.2003/isos/x86_64/CentOS-7-x86_64-Everything-2003.iso Mount the ISO file and copy the contents to /var/spacewalk/repo/CentOS7-x86_64 cp --recursive --verbose /mnt/distros/CentOS7-x86_64/ /var/spacewalk/repo/ Change the permissions of the file and its SELINUX file content: cd /var/spacewalk/repo/ find ./ -type d -exec chmod 755 {} \\; find ./ -type f -exec chmod 644 {} \\; find /var/spacewalk/repo/ -type f -exec chown apache:apache {} \\; find /var/spacewalk/repo/ -type d -exec chown apache:root {} \\; semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv . Create the Distribution: Distribution Label: centos7-base-x86_64 Tree Path: /var/spacewalk/repo/CentOS7-x86_64 Base Channel: Centos 7 Base - x86_64 Installer Generation: Red Hat Enterprise Linux 7 Create Activation Key On Systems tab -> Activation Key -> Create New Key On Child Channels, click all child channels and hit Update Key: Creating Kickstart Profile On Systems tab -> Kickstart -> Profiles -> Create Kickstart Profile Label: CentOS7-ks Base Channel: Centos 7 Base - x86_64 CHECK: Always use the newest Tree for this base channel. \"Newest\" is determined by the date it was last modified. Virtualization Type: None On Step 2, leave it as default: On Step 3, Setup Root Password: You have now created your Kickstart profile: On your created kickstart profile, go to Kickstart Details then Operating System tab. Click all the child channels and click Update Kickstart: In Advance Options, Change timezone to Asia/Manila In Software Tab, in Package Groups, Add the following in the comment box and click update packages: @ Base rhn-setup rhn-check rhn-client-tools rhn-custom-info rhncfg-actions rhncfg-client rhncfg-management yum-rhn-plugin python-dmidecode python-hwdata In Activation Keys, Click CentOS 7 Activation Key: Setting up Errata, PXE configuration, Etc. Errata Importation Sources: link1 link2 import spacewalk-errata-import.sh on GitHub to your system. and make the script executable. chmod +x spacewalk-errata-import.sh Download the following packages: yum install fping perl-Frontier-RPC perl-Text-Unidecode -y Run the script: [root@spacewalk ~]# ./spacewalk-errata-import.sh -u '$username' -p '$password' -s '$spacewalk_fqdn' -v --2020-08-04 06:01:27-- https://cefs.steve-meier.de/errata.latest.xml Resolving cefs.steve-meier.de (cefs.steve-meier.de)... 143.204.243.87, 143.204.243.118, 143.204.243.3, ... Connecting to cefs.steve-meier.de (cefs.steve-meier.de)|143.204.243.87|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://cefs.b-cdn.net/errata.latest.xml [following] --2020-08-04 06:01:28-- https://cefs.b-cdn.net/errata.latest.xml Resolving cefs.b-cdn.net (cefs.b-cdn.net)... 89.187.162.241 Connecting to cefs.b-cdn.net (cefs.b-cdn.net)|89.187.162.241|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1284865 (1.2M) [text/xml] Saving to: \u2018/root/errata.latest.xml\u2019 100%[======================================>] 1,284,865 1.41MB/s in 0.9s Errata will now appear on Spacewalk UI: PXE Configuration At first, you will be stuck to the Boot Menu due to TFTP is still disabled in the system: Enable TFTP Service on Spacewalk Server: Install xinetd service, an additional required package for TFTP to work. yum install -y xinetd Set disable = no line on /etc/xinetd.d/tftp service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -c -s /tftpboot disable = no per_source = 15 cps = 80 2 flags = IPv4 } Enable TFTP and xinetd Service on Spacewalk Server: systemctl enable --now tftp.service xinetd firewall-cmd --add-service=tftp firewall-cmd --runtime-to-perm When you run the PXE again, you will be having error on audit logger of spacewalk: Follow what journalctl troubleshooter says: setsebool -P tftp_home_dir 1 Change the SELinux context of the following semanage fcontext -a -t public_content_t \"/var/lib/tftpboot/.*\" semanage fcontext -a -t public_content_t \"/var/www/cobbler/images/.*\" semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv /var/spacewalk/repo/CentOS7-x86_64 Run cobbler sync to set up the files on /var/lib/tftp folder, the output should be like this: [root@spacewalk CentOS7-x86_64]# cobbler sync task started: 2020-08-05_043208_sync task started (id=Sync, time=Wed Aug 5 04:32:08 2020) running pre-sync triggers cleaning trees removing: /var/www/cobbler/images/centos7-base-x86_64:1:sandbox removing: /var/lib/tftpboot/pxelinux.cfg/default removing: /var/lib/tftpboot/grub/images ... received on stdout: received on stderr: generating PXE menu structure running post-sync triggers ... *** TASK COMPLETE *** GPG Public Keys and SSL Certificates This must be set to automatically install certificates needed for package authenticity. Systems Tab -> Kickstart -> GPG and SSL Keys -> Create Stored Key/Cert After that, you can paste or upload GPG or SSL Keys. Go to Profiles then select you kickstart file, in the System Details choose GPG & SSL, and choose the GPG keys you save in the last instruction, then hit update keys.","title":"Spacewalk"},{"location":"spacewalk/#spacewalk-documentation","text":"","title":"Spacewalk documentation"},{"location":"spacewalk/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 Reverse lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 101 IN PTR spacewalk.sandbox.io. DHCP entry host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; }","title":"DHCP and DNS Entries"},{"location":"spacewalk/#installation","text":"Source installation instructions is found here , documenting for my own copy.","title":"Installation"},{"location":"spacewalk/#prerequisites","text":"Outbound open ports 80, 443 Inbound open ports 80, 443, 5222 (only if you want to push actions to client machines) and 5269 (only for push actions to a Spacewalk Proxy), 69 udp if you want to use tftp Storage for database: 250 KiB per client system + 500 KiB per channel + 230 KiB per package in channel (i.e. 1.1GiB for channel with 5000 packages) Storage for packages (default /var/satellite): Depends on what you are storing; Red Hat recommend 6GB per channel for their channels 2GB RAM minimum, 4GB recommended Make sure your underlying OS is fully up-to-date. If you use LDAP as a central identity service and wish to pull user and group information from it, see SpacewalkWithLDAP","title":"Prerequisites"},{"location":"spacewalk/#additional-repos-packages","text":"All other dependecies outside base repo will be available on EPEL repository rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm","title":"Additional repos &amp; packages"},{"location":"spacewalk/#database-server","text":"I will let spacewalk setup postgresql database first, before moving it to a separate server. yum -y install spacewalk-setup-postgresql","title":"Database server"},{"location":"spacewalk/#installing-spacewalk","text":"yum -y install spacewalk-postgresql","title":"Installing Spacewalk"},{"location":"spacewalk/#configuring-the-firewall","text":"Firewalld is installed on my server so the commands i will use is based on firewalld commands firewall-cmd --add-service=http firewall-cmd --add-service=https firewall-cmd --runtime-to-perm","title":"Configuring the Firewall"},{"location":"spacewalk/#configuring-spacewalk","text":"Please make sure that your spacewalk server have a resolvable FQDN. spacewalk-setup An example session is as follows: [root@spacewalk ~]# spacewalk-setup * Setting up SELinux.. ** Database: Setting up database connection for PostgreSQL backend. ** Database: Installing the database: ** Database: This is a long process that is logged in: ** Database: /var/log/rhn/install_db.log *** Progress: # ** Database: Installation complete. ** Database: Populating database. *** Progress: ############################ * Configuring tomcat. * Setting up users and groups. ** GPG: Initializing GPG and importing key. ** GPG: Creating /root/.gnupg directory You must enter an email address. Admin Email Address? [redacted] * Performing initial configuration. ** Package installation: Locking required rpm versions. * Configuring apache SSL virtual host. Should setup configure apaches default ssl server for you (saves original ssl.conf) [Y]? ** /etc/httpd/conf.d/ssl.conf has been backed up to ssl.conf-swsave * Configuring jabberd. * Creating SSL certificates. CA certificate password? password Re-enter CA certificate password? password Cname alias of the machine (comma seperated)? spacewalk.sandbox.io Organization? sandbox Organization Unit [spacewalk.sandbox.io]? Email Address [[redacted]]? City? [redacted] State? [redacted] Country code (Examples: \"US\", \"JP\", \"IN\", or type \"?\" to see a list)? [redacted] ** SSL: Generating CA certificate. ** SSL: Deploying CA certificate. ** SSL: Generating server certificate. ** SSL: Storing SSL certificates. * Deploying configuration files. * Update configuration in database. * Setting up Cobbler.. Cobbler requires tftp and xinetd services be turned on for PXE provisioning functionality. Enable these services [Y]? y * Restarting services. Installation complete. Visit https://spacewalk.sandbox.io to create the Spacewalk administrator account.","title":"Configuring Spacewalk"},{"location":"spacewalk/#creating-spacewalk-centos-channel","text":"","title":"Creating Spacewalk CentOS channel"},{"location":"spacewalk/#creating-software-channel","text":"Channels -> Manage Software Channels -> Create Channel Fill up the fields accordingly: Channel Name: Centos 7 Base - x86_64 Channel Label: centos7-base-x86_64 Architecture: x86_64 Yum Repository Checksum Type: sha1 Channel Summary: Centos 7 Base - x86_64 Optional: For additional security, add GPG key fingerprint in Security:GPG section, you can get GPG key using gpg --with-fingerprint [key] . cd /etc/pki/rpm-gpg/ gpg --with-fingerprint RPM-GPG-KEY-CentOS-7 pub 4096R/F4A80EB5 2014-06-23 CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org> Key fingerprint = 6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5","title":"Creating software channel"},{"location":"spacewalk/#creating-repositories","text":"Channels -> Manage Software Channels -> Manage Repositories -> Create Repository Repository Label: centos7-base-x86_64 Repository URL: http://mirror.centos.org/centos/7/os/x86_64/ Repository Type: yum","title":"Creating repositories"},{"location":"spacewalk/#syncing-the-channel-and-the-repos","text":"In the command line type: spacewalk-repo-sync -c centos7-base-x86_64","title":"Syncing the channel and the repos"},{"location":"spacewalk/#creating-base-children-channels","text":"","title":"Creating Base Children Channels"},{"location":"spacewalk/#creating-updates-channel","text":"Create a new channel again, now making the Base channel as the parent channel: Channel Name: Centos 7 Updates - x86_64 Channel Label: centos7-updates-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos7-updates-x86_64 Security:GPG section in this channel is the same with the parent channel: GPG key URL: http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7 GPG key ID: F4A80EB5 GPG key Fingerprint: 6341 AB27 53D78A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5 Click Create Channel to create the children channel","title":"Creating Updates Channel"},{"location":"spacewalk/#creating-epel-channel","text":"Same with Updates channel, make the base channel as parent channel: Channel Name: Epel 7 - x86_64 Channel Label: epel7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: Extra Packages for Enterprise Linux 7 - x86_64 As for the Security: GPG, change the details to: GPG key URL: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 GPG key ID: 352C64E5 GPG key Fingerprint: 91E9 7D7C 4A5E 96F1 7F3E 888F 6A2F AEA2 352C 64E5 Alternatively, you can type gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 to the terminal.","title":"Creating EPEL Channel"},{"location":"spacewalk/#creating-spacewalk-client-channel","text":"Same with Updates and EPEL channel, make the base channel as parent channel: Channel Name: Spacewalk Client 2.10 - x86_64 Channel Label: spacewalk-client-210-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: spacewalk-client-210-x86_64 As for the Security: GPG, change the details to: GPG key URL: https://download.copr.fedorainfracloud.org/results/@spacewalkproject/spacewalk-2.10/pubkey.gpg GPG key ID: BC2E6843 GPG key Fingerprint: E8C0 573E 5B62 BB7C 98C1 A2AC 770C E53E BC2E 6843","title":"Creating Spacewalk Client Channel"},{"location":"spacewalk/#creating-extras-channel","text":"Channel Name: Centos 7 Extras - x86_64 Channel Label: centos-7-extras-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos-7-extras-x86_64","title":"Creating Extras Channel"},{"location":"spacewalk/#creating-puppet-channel","text":"This Channel is for the automation server, puppet-server package in EPEL is quite outdated and i want to use the new version. Channel Name: Puppet 7 EL7 x86_64 Channel Label: puppet-7-el7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: puppet-7-el7-x86_64","title":"Creating Puppet Channel"},{"location":"spacewalk/#creating-repositories-for-children-channels","text":"Create a repository for each children channels: Repository URL for centos7-updates-x86_64: http://mirror.rise.ph/centos/7.8.2003/updates/x86_64/ # This might be changed since the updated CentOS version upon this writing is 7.8 Repository URL for epel7-x86_64: https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/ Repository URL for spacewalk-client-210-x86_64: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10-client/epel-7-x86_64/ Repository URL for centos-7-extras-x86_64: http://mirror.centos.org/centos/7/extras/x86_64/ Repositort URL for puppet-7-el7-x86_64: http://yum.puppetlabs.com/puppet/el/7/x86_64/","title":"Creating Repositories for Children Channels"},{"location":"spacewalk/#pushing-packages-to-spacewalk-repo","text":"Add created repository for each children channels and sync the channels through spacewalk website or terminal: spacewalk-repo-sync -c spacewalk-client-210-x86_64 spacewalk-repo-sync -c centos7-updates-x86_64 Example output: [root@spacewalk rpm-gpg]# spacewalk-repo-sync -c spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 | Channel: spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 Sync of channel started. 22:50:25 22:50:25 Processing repository with URL: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10/epel-7-x86_64/ 22:50:27 Packages in repo: 206 22:50:27 Packages already synced: 0 22:50:27 Packages to sync: 138 22:50:27 New packages to download: 138 22:50:27 Downloading packages: Due to issues with EPEL repository is too big to sync to spacewalk, you may want to download the rpm packages you want to be in the EPEL channel, then push it to spacewalk rhnpush -v --channel=epel7-x86_64 --server=http://localhost --dir=/root/epel-packages","title":"Pushing packages to spacewalk repo"},{"location":"spacewalk/#distribution-and-kickstart-profile","text":"","title":"Distribution and Kickstart Profile"},{"location":"spacewalk/#creating-distributions","text":"On Systems tab -> Kickstart -> Distribution -> Create a new Distribution Download the latest CentOS ISO file: wget http://mirror.pregi.net/centos/7.8.2003/isos/x86_64/CentOS-7-x86_64-Everything-2003.iso Mount the ISO file and copy the contents to /var/spacewalk/repo/CentOS7-x86_64 cp --recursive --verbose /mnt/distros/CentOS7-x86_64/ /var/spacewalk/repo/ Change the permissions of the file and its SELINUX file content: cd /var/spacewalk/repo/ find ./ -type d -exec chmod 755 {} \\; find ./ -type f -exec chmod 644 {} \\; find /var/spacewalk/repo/ -type f -exec chown apache:apache {} \\; find /var/spacewalk/repo/ -type d -exec chown apache:root {} \\; semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv . Create the Distribution: Distribution Label: centos7-base-x86_64 Tree Path: /var/spacewalk/repo/CentOS7-x86_64 Base Channel: Centos 7 Base - x86_64 Installer Generation: Red Hat Enterprise Linux 7","title":"Creating Distributions"},{"location":"spacewalk/#create-activation-key","text":"On Systems tab -> Activation Key -> Create New Key On Child Channels, click all child channels and hit Update Key:","title":"Create Activation Key"},{"location":"spacewalk/#creating-kickstart-profile","text":"On Systems tab -> Kickstart -> Profiles -> Create Kickstart Profile Label: CentOS7-ks Base Channel: Centos 7 Base - x86_64 CHECK: Always use the newest Tree for this base channel. \"Newest\" is determined by the date it was last modified. Virtualization Type: None On Step 2, leave it as default: On Step 3, Setup Root Password: You have now created your Kickstart profile: On your created kickstart profile, go to Kickstart Details then Operating System tab. Click all the child channels and click Update Kickstart: In Advance Options, Change timezone to Asia/Manila In Software Tab, in Package Groups, Add the following in the comment box and click update packages: @ Base rhn-setup rhn-check rhn-client-tools rhn-custom-info rhncfg-actions rhncfg-client rhncfg-management yum-rhn-plugin python-dmidecode python-hwdata In Activation Keys, Click CentOS 7 Activation Key:","title":"Creating Kickstart Profile"},{"location":"spacewalk/#setting-up-errata-pxe-configuration-etc","text":"","title":"Setting up Errata, PXE configuration, Etc."},{"location":"spacewalk/#errata-importation","text":"Sources: link1 link2 import spacewalk-errata-import.sh on GitHub to your system. and make the script executable. chmod +x spacewalk-errata-import.sh Download the following packages: yum install fping perl-Frontier-RPC perl-Text-Unidecode -y Run the script: [root@spacewalk ~]# ./spacewalk-errata-import.sh -u '$username' -p '$password' -s '$spacewalk_fqdn' -v --2020-08-04 06:01:27-- https://cefs.steve-meier.de/errata.latest.xml Resolving cefs.steve-meier.de (cefs.steve-meier.de)... 143.204.243.87, 143.204.243.118, 143.204.243.3, ... Connecting to cefs.steve-meier.de (cefs.steve-meier.de)|143.204.243.87|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://cefs.b-cdn.net/errata.latest.xml [following] --2020-08-04 06:01:28-- https://cefs.b-cdn.net/errata.latest.xml Resolving cefs.b-cdn.net (cefs.b-cdn.net)... 89.187.162.241 Connecting to cefs.b-cdn.net (cefs.b-cdn.net)|89.187.162.241|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1284865 (1.2M) [text/xml] Saving to: \u2018/root/errata.latest.xml\u2019 100%[======================================>] 1,284,865 1.41MB/s in 0.9s Errata will now appear on Spacewalk UI:","title":"Errata Importation"},{"location":"spacewalk/#pxe-configuration","text":"At first, you will be stuck to the Boot Menu due to TFTP is still disabled in the system: Enable TFTP Service on Spacewalk Server: Install xinetd service, an additional required package for TFTP to work. yum install -y xinetd Set disable = no line on /etc/xinetd.d/tftp service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -c -s /tftpboot disable = no per_source = 15 cps = 80 2 flags = IPv4 } Enable TFTP and xinetd Service on Spacewalk Server: systemctl enable --now tftp.service xinetd firewall-cmd --add-service=tftp firewall-cmd --runtime-to-perm When you run the PXE again, you will be having error on audit logger of spacewalk: Follow what journalctl troubleshooter says: setsebool -P tftp_home_dir 1 Change the SELinux context of the following semanage fcontext -a -t public_content_t \"/var/lib/tftpboot/.*\" semanage fcontext -a -t public_content_t \"/var/www/cobbler/images/.*\" semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv /var/spacewalk/repo/CentOS7-x86_64 Run cobbler sync to set up the files on /var/lib/tftp folder, the output should be like this: [root@spacewalk CentOS7-x86_64]# cobbler sync task started: 2020-08-05_043208_sync task started (id=Sync, time=Wed Aug 5 04:32:08 2020) running pre-sync triggers cleaning trees removing: /var/www/cobbler/images/centos7-base-x86_64:1:sandbox removing: /var/lib/tftpboot/pxelinux.cfg/default removing: /var/lib/tftpboot/grub/images ... received on stdout: received on stderr: generating PXE menu structure running post-sync triggers ... *** TASK COMPLETE ***","title":"PXE Configuration"},{"location":"spacewalk/#gpg-public-keys-and-ssl-certificates","text":"This must be set to automatically install certificates needed for package authenticity. Systems Tab -> Kickstart -> GPG and SSL Keys -> Create Stored Key/Cert After that, you can paste or upload GPG or SSL Keys. Go to Profiles then select you kickstart file, in the System Details choose GPG & SSL, and choose the GPG keys you save in the last instruction, then hit update keys.","title":"GPG Public Keys and SSL Certificates"},{"location":"storage/","text":"Storage Server DHCP and DNS Entries Forward lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 Reverse lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 106 IN PTR storage.sandbox.io. DHCP entry host storage { hardware ethernet 02:1D:98:67:47:CC; fixed-address 10.11.100.106; } Scope Storage location of Bacula server, LUN storage share will be for the bacula backup itself, NFS storage share will be for the rest of the nodes storage backup mounted on Details /root/disk1.img /dev/sdb /mnt/LUN Storage for main backup server /var/nfs storage.sandbox.io:/var/nfs /mnt/NFS Storage for all nodes Installation and setup Package Installation Install targetcli and iscsi-initiator-utils to configure this server as a LUN device - name: install targetcli and iscsi-initiator-utils yum: name: - targetcli - iscsi-initiator-utils - nfs-utils state: present Start necessary servers - name: enable and start nfs services service: name: \"{{services}}\" state: started enabled: true loop: - \"target\" - \"rpcbind\" - \"nfs-server\" - \"nfs-lock\" - \"nfs-idmap\" loop_control: loop_var: services Create /var/nfs directory, this will act as the shared directory of the storage server - name: create nfs directory to share file: state: directory path: /var/nfs owner: nfsnobody group: nfsnobody mode: '1777' NFS Share setup Setup /var/nfs directory as shared directory in /etc/exports file, restart the service. - name: setup required files and directories copy: dest: /etc/exports content: \"/var/nfs 10.11.100.0/24(rw,sync,no_root_squash,no_all_squash)\" register: nfs_config - name: check if nfs_config chaged, restart nfs-server service: name: nfs-server state: restarted when: nfs_config.changed Open necessary ports in firewalld # For defined service names in firewalld - name: open ports in firewalld for services firewalld: service: \"{{firewall_services}}\" state: enabled zone: public immediate: true permanent: true loop: - 'nfs' - 'mountd' - 'rpc-bind' loop_control: loop_var: firewall_services # For not defined service - name: open ports in firewalld for ports firewalld: port: '3260/tcp' state: enabled immediate: true permanent: true LUN Share setup As of now, i cant find any utilities for targetcli on ansible. I will pass the commands via shell module of ansible to and will only run 1 time(creates system checks file) - name: check if LUN-setup-done exist stat: path: /home/users/bot-acc/.system-checks/LUN-setup-done register: LUN_status - name: run targetcli commands when file LUN_status does not exist shell: cmd: '{{item}}' with_items: - 'targetcli /backstores/fileio create LUN_1 /root/disk1.img 7g' - 'targetcli /iscsi create iqn.2021-09.io.sandbox.storage' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/luns create /backstores/fileio/LUN_1 LUN1' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/acls create iqn.2021-09.io.sandbox.storage:backup-acl' - 'targetcli saveconfig' - 'touch /home/users/bot-acc/.system-checks/LUN-setup-done' warn: false when: not LUN_status.stat.exists Partitioning, Mounting, Etc. In order to partition the LUN device(this is needed right?) we need to discover the LUN device internally. Define the IQN in /etc/iscsi/initiatorname.iscsi and start the service - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started when: not LUN_login_status.stat.exists Discover and Login to LUN share - name: discover the LUN share open_iscsi: show_nodes: true portal: 'dev1.sandbox.io' discover: true when: not LUN_login_status.stat.exists - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage' when: not LUN_login_status.stat.exists Partition and create filesystem - name: partition the device parted: device: /dev/sdb number: 1 state: present - name: create filesystem of the device filesystem: dev: /dev/sdb1 fstype: xfs Since there is an issue when 2 nodes connecting to target, we need to logout on iscsi - name: logout to LUN share open_iscsi: login: false target: 'iqn.2021-09.io.sandbox.storage' when: LUN_login.changed == true register: LUN_logout Reboot once configuration has been setup - name: reboot after setup shell: cmd: echo 'reboot' | at now +1 minute when: LUN_logout.changed == true","title":"Storage"},{"location":"storage/#storage-server","text":"","title":"Storage Server"},{"location":"storage/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 Reverse lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 106 IN PTR storage.sandbox.io. DHCP entry host storage { hardware ethernet 02:1D:98:67:47:CC; fixed-address 10.11.100.106; }","title":"DHCP and DNS Entries"},{"location":"storage/#scope","text":"Storage location of Bacula server, LUN storage share will be for the bacula backup itself, NFS storage share will be for the rest of the nodes storage backup mounted on Details /root/disk1.img /dev/sdb /mnt/LUN Storage for main backup server /var/nfs storage.sandbox.io:/var/nfs /mnt/NFS Storage for all nodes","title":"Scope"},{"location":"storage/#installation-and-setup","text":"","title":"Installation and setup"},{"location":"storage/#package-installation","text":"Install targetcli and iscsi-initiator-utils to configure this server as a LUN device - name: install targetcli and iscsi-initiator-utils yum: name: - targetcli - iscsi-initiator-utils - nfs-utils state: present Start necessary servers - name: enable and start nfs services service: name: \"{{services}}\" state: started enabled: true loop: - \"target\" - \"rpcbind\" - \"nfs-server\" - \"nfs-lock\" - \"nfs-idmap\" loop_control: loop_var: services Create /var/nfs directory, this will act as the shared directory of the storage server - name: create nfs directory to share file: state: directory path: /var/nfs owner: nfsnobody group: nfsnobody mode: '1777'","title":"Package Installation"},{"location":"storage/#nfs-share-setup","text":"Setup /var/nfs directory as shared directory in /etc/exports file, restart the service. - name: setup required files and directories copy: dest: /etc/exports content: \"/var/nfs 10.11.100.0/24(rw,sync,no_root_squash,no_all_squash)\" register: nfs_config - name: check if nfs_config chaged, restart nfs-server service: name: nfs-server state: restarted when: nfs_config.changed Open necessary ports in firewalld # For defined service names in firewalld - name: open ports in firewalld for services firewalld: service: \"{{firewall_services}}\" state: enabled zone: public immediate: true permanent: true loop: - 'nfs' - 'mountd' - 'rpc-bind' loop_control: loop_var: firewall_services # For not defined service - name: open ports in firewalld for ports firewalld: port: '3260/tcp' state: enabled immediate: true permanent: true","title":"NFS Share setup"},{"location":"storage/#lun-share-setup","text":"As of now, i cant find any utilities for targetcli on ansible. I will pass the commands via shell module of ansible to and will only run 1 time(creates system checks file) - name: check if LUN-setup-done exist stat: path: /home/users/bot-acc/.system-checks/LUN-setup-done register: LUN_status - name: run targetcli commands when file LUN_status does not exist shell: cmd: '{{item}}' with_items: - 'targetcli /backstores/fileio create LUN_1 /root/disk1.img 7g' - 'targetcli /iscsi create iqn.2021-09.io.sandbox.storage' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/luns create /backstores/fileio/LUN_1 LUN1' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/acls create iqn.2021-09.io.sandbox.storage:backup-acl' - 'targetcli saveconfig' - 'touch /home/users/bot-acc/.system-checks/LUN-setup-done' warn: false when: not LUN_status.stat.exists","title":"LUN Share setup"},{"location":"storage/#partitioning-mounting-etc","text":"In order to partition the LUN device(this is needed right?) we need to discover the LUN device internally. Define the IQN in /etc/iscsi/initiatorname.iscsi and start the service - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started when: not LUN_login_status.stat.exists Discover and Login to LUN share - name: discover the LUN share open_iscsi: show_nodes: true portal: 'dev1.sandbox.io' discover: true when: not LUN_login_status.stat.exists - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage' when: not LUN_login_status.stat.exists Partition and create filesystem - name: partition the device parted: device: /dev/sdb number: 1 state: present - name: create filesystem of the device filesystem: dev: /dev/sdb1 fstype: xfs Since there is an issue when 2 nodes connecting to target, we need to logout on iscsi - name: logout to LUN share open_iscsi: login: false target: 'iqn.2021-09.io.sandbox.storage' when: LUN_login.changed == true register: LUN_logout Reboot once configuration has been setup - name: reboot after setup shell: cmd: echo 'reboot' | at now +1 minute when: LUN_logout.changed == true","title":"Partitioning, Mounting, Etc."}]}