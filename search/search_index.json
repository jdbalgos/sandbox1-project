{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deve Sandbox Project About this site I am creating my own infrastructure, all based on CentOS 7 Linux Servers. Server planning Overview I will use Proxmox virtual environment. Specs: i7, 16gb RAM, 1TB HDD DNS and DHCP server -> Bind and DHCPd server for static ip and domain configuration. System Management -> Spacewalk server, servers will also connect to this for PXE configuration Database Server -> PostgreSQL Server, for Spacewalk database. Export spacewalk server database Directory Server -> FreeIPA Server Automation Server -> Puppet, Ansible Puppet for general installation configuration, more like pre install. Ansible for node by node configuration/provisioning Storage Server -> NFS and LUN server. for backup of all servers Backup Server -> Bacula server For now this are only the servers that is managed, will add servers accordingly. Server details Hostname IP Details dns-dhcp.sandbox.io 10.11.100.1 DNS-DHCP server spacewalk.sandbox.io 10.11.100.101 Spacewalk server directory1.sandbox.io 10.11.100.102 IPA server docs.sandbox.io 10.11.100.103 Main documentation server sql.sandbox.io 10.11.100.104 PostgreSQL server bot.sandbox.io 10.11.100.105 Puppet, Ansible and Git Server storage.sandbox.io 10.11.100.106 LUN and NFS Server backup.sandbox.io 10.11.100.107 Bacula Server app1.sandbox.io 10.11.100.108 1st Tomcat Server app2.sandbox.io 10.11.100.109 2nd Tomcat Server ldb.sandbox.io 10.11.100.110 Load Balancer(HAProxy) Server TBD TBD TBD Deployments Since i am only using 16gb of RAM, I will introduce containers in the environment. Applications will be tested on KVM and LXC before moving to Main environment.","title":"Home"},{"location":"#deve-sandbox-project","text":"","title":"Deve Sandbox Project"},{"location":"#about-this-site","text":"I am creating my own infrastructure, all based on CentOS 7 Linux Servers.","title":"About this site"},{"location":"#server-planning","text":"","title":"Server planning"},{"location":"#overview","text":"I will use Proxmox virtual environment. Specs: i7, 16gb RAM, 1TB HDD DNS and DHCP server -> Bind and DHCPd server for static ip and domain configuration. System Management -> Spacewalk server, servers will also connect to this for PXE configuration Database Server -> PostgreSQL Server, for Spacewalk database. Export spacewalk server database Directory Server -> FreeIPA Server Automation Server -> Puppet, Ansible Puppet for general installation configuration, more like pre install. Ansible for node by node configuration/provisioning Storage Server -> NFS and LUN server. for backup of all servers Backup Server -> Bacula server For now this are only the servers that is managed, will add servers accordingly.","title":"Overview"},{"location":"#server-details","text":"Hostname IP Details dns-dhcp.sandbox.io 10.11.100.1 DNS-DHCP server spacewalk.sandbox.io 10.11.100.101 Spacewalk server directory1.sandbox.io 10.11.100.102 IPA server docs.sandbox.io 10.11.100.103 Main documentation server sql.sandbox.io 10.11.100.104 PostgreSQL server bot.sandbox.io 10.11.100.105 Puppet, Ansible and Git Server storage.sandbox.io 10.11.100.106 LUN and NFS Server backup.sandbox.io 10.11.100.107 Bacula Server app1.sandbox.io 10.11.100.108 1st Tomcat Server app2.sandbox.io 10.11.100.109 2nd Tomcat Server ldb.sandbox.io 10.11.100.110 Load Balancer(HAProxy) Server TBD TBD TBD","title":"Server details"},{"location":"#deployments","text":"Since i am only using 16gb of RAM, I will introduce containers in the environment. Applications will be tested on KVM and LXC before moving to Main environment.","title":"Deployments"},{"location":"app/","text":"Application Server Scope I will install 2 tomcat applications on each server running high availability setup, that is being proxied by httpd application to port 80, and will be load balance by another server installed with haproxy. app1 and app2 will synct through multicast ldb.sandbox.io --> app1.sandbox.io:80 -> app1.sandbox.io:8080 -- | |--> application(HA) -> app2.sandbox.io:80 -> app2.sandbox.io:8080 -- Simple application that will be run will be a java webpage that ensures sessions were retained after submitting request #index.jsp <%@page import=\"java.util.ArrayList\"%> <%@page import=\"java.util.Date\"%> <%@page import=\"java.util.List\"%> <%@page contentType=\"text/html\" pageEncoding=\"UTF-8\"%> <!DOCTYPE html> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"> <title>JSP Page</title> </head> <body> <FONT size = 5 COLOR=\"#0000FF\"> Instance 1 <br/><br/> </FONT> <hr/> <FONT size = 5 COLOR=\"#CC0000\"> <br/> Session Id : <%=request.getSession().getId()%> <br/> Is it New Session : <%=request.getSession().isNew()%><br/> Session Creation Date : <%=new Date(request.getSession().getCreationTime())%><br/> Session Access Date : <%=new Date(request.getSession().getLastAccessedTime())%><br/><br/> </FONT> <b>Cart List </b><br/> <hr/> <ul> <% String bookName = request.getParameter(\"bookName\"); List<String> listOfBooks = (List<String>) request.getSession().getAttribute(\"Books\"); if (listOfBooks == null) { listOfBooks = new ArrayList<String>(); request.getSession().setAttribute(\"Books\", listOfBooks); } if (bookName != null) { listOfBooks.add(bookName); request.getSession().setAttribute(\"Books\", listOfBooks); } for (String book : listOfBooks) { out.println(\"<li>\"+book + \"</li><br/>\"); } %> </ul> <hr/> <form action=\"index.jsp\" method=\"post\"> Book Name <input type=\"text\" name=\"bookName\" /> <input type=\"submit\" value=\"Add to Cart\"/> </form> <hr/> </body> </html> DHCP and DNS Entries Forward lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 app IN CNAME ldb Reverse lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 108 IN PTR app1.sandbox.io. 109 IN PTR app2.sandbox.io. 110 IN PTR ldb.sandbox.io. DHCP client entries host app1 { hardware ethernet 2E:A5:8D:FB:B9:F8; fixed-address 10.11.100.108; } -- host app2{ hardware ethernet AE:45:14:38:61:C3; fixed-address 10.11.100.109; } -- host ldb { hardware ethernet 4E:D6:AE:43:D4:84; fixed-address 10.11.100.110; } App1 and App2 Servers Installation and Setup Needed to install the following package: tomcat httpd tomcat-webapps tomcat-admin-webapps This can be done by yum or ansible yum -y install tomcat httpd tomcat-webapps tomcat-admin-webapps - name: install tomcat application yum: name: - tomcat - tomcat-webapps - tomcat-admin-webapps - httpd Instruct tomcat to only allow 256mb of memory. echo 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' > /usr/share/tomcat/conf/tomcat.conf - name: add JAVA_OPTS option to limit max permit memory size for each tomcat instances blockinfile: path: /usr/share/tomcat/conf/tomcat.conf block: 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' state: present Add manager credentials for testing tomcat - name: add admin user for manage login lineinfile: insertafter: '<tomcat-users>' path: /usr/share/tomcat/conf/tomcat-users.xml line: '<user username=\"admin\" password=\"Password123!\" roles=\"manager-gui,admin-gui\"/>' add ports for exception in firewall - name: add ports to firewalld firewalld: port: '{{item}}' state: enabled immediate: true permanent: true with_items: - 8080/tcp - 80/tcp - 4000/tcp Since this setup will be needing multicast, need to allow multicast address - name: add multicast address to firewalld firewalld: rich_rule: rule family=\"ipv4\" destination address=\"228.0.0.4\" protocol value=\"ip\" accept permanent: true immediate: true state: enabled Create directories - name: create directories for sites-enabled, sites-available and test directory for tomcat file: state: directory path: '{{item}}' with_items: - /etc/httpd/sites-enabled - /etc/httpd/sites-available - /var/lib/tomcat/webapps/test - /var/lib/tomcat/webapps/test/WEB-INF - /var/www/sandbox Insert the index.jsp file to the newly created test directory - name: create test application file copy: src: files/app/test-index.jsp dest: /var/lib/tomcat/webapps/test/index.jsp Copy each virtual host file and paste it on /etc/httpd/sites-available on each server - name: copy http tomcat configuration file for app1 copy: src: files/app/tomcat1.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app1\" - name: copy http tomcat configuration file for app2 copy: src: files/app/tomcat2.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app2\" Contents of tomcat1.conf and tomcat2.conf # tomcat1.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app1.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app1.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app1.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app1_error.log CustomLog /var/log/httpd/app1_common.log combined </VirtualHost> # tomcat2.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app2.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app2.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app2.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app2_error.log CustomLog /var/log/httpd/app2_common.log combined </VirtualHost> Create a softlink of virtualhost file to /etc/httpd/sites-enabled/ - name: create link of tomcat.conf to sites-enabled file: src: /etc/httpd/sites-available/tomcat.conf dest: /etc/httpd/sites-enabled/tomcat.conf state: link Add 'IncludeOptional sites-enabled/*.conf' in /etc/httpd/conf/httpd.conf to add all conf files in sites-enabled directory - name: add line to /etc/httpd/conf/httpd.conf lineinfile: path: /etc/httpd/conf/httpd.conf line: 'IncludeOptional sites-enabled/*.conf' - name: set boolean for httpd_can_network_connect to true Create labels for tomcat setup on both servers - name: create labels on each node in /etc/tomcat/server.xml for app1 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat1\">' when: ansible_hostname == \"app1\" - name: create labels on each node in /etc/tomcat/server.xml for app2 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat2\">' when: ansible_hostname == \"app2\" Create web.xml file /var/lib/tomcat/webapps/test/WEB-INF/ directory - name: add web.xml file to test directory copy: src: files/app/web.xml dest: /var/lib/tomcat/webapps/test/WEB-INF/web.xml Contents of web.xml <?xml version=\"1.0\" encoding=\"ISO-8859-1\"?> <web-app xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\" metadata-complete=\"true\"> <display-name>Welcome to Tomcat</display-name> <description> Welcome to Tomcat </description> <distributable/> </web-app> Need to add the cluster function on /etc/tomcat/server.xml ive done this via blockinline on ansible - name: add line block on /etc/tomcat/server.xml blockinfile: path: /etc/tomcat/server.xml marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\" insertafter: '<Engine name=\"Catalina\" defaultHost=\"localhost\"' backup: true block: | <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\" channelSendOptions=\"8\"> <Manager className=\"org.apache.catalina.ha.session.DeltaManager\" expireSessionsOnShutdown=\"false\" notifyListenersOnReplication=\"true\"/> <Channel className=\"org.apache.catalina.tribes.group.GroupChannel\"> <Membership className=\"org.apache.catalina.tribes.membership.McastService\" address=\"228.0.0.4\" port=\"45564\" frequency=\"500\" dropTime=\"3000\"/> <Sender className=\"org.apache.catalina.tribes.transport.ReplicationTransmitter\"> <Transport className=\"org.apache.catalina.tribes.transport.nio.PooledParallelSender\"/> </Sender> <Receiver className=\"org.apache.catalina.tribes.transport.nio.NioReceiver\" address=\"auto\" port=\"4000\" autoBind=\"100\" selectorTimeout=\"5000\" maxThreads=\"6\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.TcpFailureDetector\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor\"/> </Channel> <Valve className=\"org.apache.catalina.ha.tcp.ReplicationValve\" filter=\"\"/> <Valve className=\"org.apache.catalina.ha.session.JvmRouteBinderValve\"/> <ClusterListener className=\"org.apache.catalina.ha.session.JvmRouteSessionIDBinderListener\"/> <ClusterListener className=\"org.apache.catalina.ha.session.ClusterSessionListener\"/> </Cluster> Start the services - name: start httpd service: name: httpd state: restarted enabled: true - name: start tomcat service service: name: tomcat state: restarted enabled: true Issues Running on LXC Synching issues may happen if your running on linux container, this is due to tomcat app and /etc/hosts file of the lxc not correctly getting the servers own IP. Edit /etc/nsswitch if you want a simpler fix. - name: swap dns and files entry on /etc/nsswitch lineinfile: path: /etc/nsswitch.conf regexp: '^hosts' line: 'hosts: dns files myhostname' when: ansible_virtualization_type == \"lxc\" Running on KVM unlike LXC, you can run selinux on KVM and can cause issues, recommended to change booleans instead of disabling it permanently - name: set boolean for httpd_can_network_connect to true seboolean: name: '{{item}}' persistent: true state: true with_items: - httpd_can_network_connect - nis_enabled when: ansible_virtualization_type == \"kvm\" Load Balancer Setup Configuration on load balancer is simple Installation and Setup Install haproxy to the load balancer - name: install haproxy to the server yum: name: haproxy state: present replace haproxy.cfg with the configured one - name: copy haproxy.cfg file /etc/haproxy/haproxy.cfg copy: src: files/haproxy/haproxy.cfg dest: /etc/haproxy/haproxy.cfg Contents of configured haproxy.cfg #/etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 frontend tomcat_app_frontend bind *:80 stats uri /haproxy?stats default_backend tomcat_app_backend backend tomcat_app_backend balance roundrobin server app1 app1.sandbox.io:80 check server app2 app2.sandbox.io:80 checkv Open port 80/tcp on firewalld - name: open firewalld port 80 firewalld: service: http state: enabled permanent: true immediate: true Start haproxy service - name: start load balancing service: name: haproxy state: started enabled: true","title":"Application"},{"location":"app/#application-server","text":"","title":"Application Server"},{"location":"app/#scope","text":"I will install 2 tomcat applications on each server running high availability setup, that is being proxied by httpd application to port 80, and will be load balance by another server installed with haproxy. app1 and app2 will synct through multicast ldb.sandbox.io --> app1.sandbox.io:80 -> app1.sandbox.io:8080 -- | |--> application(HA) -> app2.sandbox.io:80 -> app2.sandbox.io:8080 -- Simple application that will be run will be a java webpage that ensures sessions were retained after submitting request #index.jsp <%@page import=\"java.util.ArrayList\"%> <%@page import=\"java.util.Date\"%> <%@page import=\"java.util.List\"%> <%@page contentType=\"text/html\" pageEncoding=\"UTF-8\"%> <!DOCTYPE html> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"> <title>JSP Page</title> </head> <body> <FONT size = 5 COLOR=\"#0000FF\"> Instance 1 <br/><br/> </FONT> <hr/> <FONT size = 5 COLOR=\"#CC0000\"> <br/> Session Id : <%=request.getSession().getId()%> <br/> Is it New Session : <%=request.getSession().isNew()%><br/> Session Creation Date : <%=new Date(request.getSession().getCreationTime())%><br/> Session Access Date : <%=new Date(request.getSession().getLastAccessedTime())%><br/><br/> </FONT> <b>Cart List </b><br/> <hr/> <ul> <% String bookName = request.getParameter(\"bookName\"); List<String> listOfBooks = (List<String>) request.getSession().getAttribute(\"Books\"); if (listOfBooks == null) { listOfBooks = new ArrayList<String>(); request.getSession().setAttribute(\"Books\", listOfBooks); } if (bookName != null) { listOfBooks.add(bookName); request.getSession().setAttribute(\"Books\", listOfBooks); } for (String book : listOfBooks) { out.println(\"<li>\"+book + \"</li><br/>\"); } %> </ul> <hr/> <form action=\"index.jsp\" method=\"post\"> Book Name <input type=\"text\" name=\"bookName\" /> <input type=\"submit\" value=\"Add to Cart\"/> </form> <hr/> </body> </html>","title":"Scope"},{"location":"app/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 app IN CNAME ldb Reverse lookup zone @ IN NS app1.sandbox.io. @ IN NS app2.sandbox.io. @ IN NS ldb.sandbox.io. app1 IN A 10.11.100.108 app2 IN A 10.11.100.109 ldb IN A 10.11.100.110 108 IN PTR app1.sandbox.io. 109 IN PTR app2.sandbox.io. 110 IN PTR ldb.sandbox.io. DHCP client entries host app1 { hardware ethernet 2E:A5:8D:FB:B9:F8; fixed-address 10.11.100.108; } -- host app2{ hardware ethernet AE:45:14:38:61:C3; fixed-address 10.11.100.109; } -- host ldb { hardware ethernet 4E:D6:AE:43:D4:84; fixed-address 10.11.100.110; }","title":"DHCP and DNS Entries"},{"location":"app/#app1-and-app2-servers","text":"","title":"App1 and App2 Servers"},{"location":"app/#installation-and-setup","text":"Needed to install the following package: tomcat httpd tomcat-webapps tomcat-admin-webapps This can be done by yum or ansible yum -y install tomcat httpd tomcat-webapps tomcat-admin-webapps - name: install tomcat application yum: name: - tomcat - tomcat-webapps - tomcat-admin-webapps - httpd Instruct tomcat to only allow 256mb of memory. echo 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' > /usr/share/tomcat/conf/tomcat.conf - name: add JAVA_OPTS option to limit max permit memory size for each tomcat instances blockinfile: path: /usr/share/tomcat/conf/tomcat.conf block: 'JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom -Djava.awt.headless=true -Xmx256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC\"' state: present Add manager credentials for testing tomcat - name: add admin user for manage login lineinfile: insertafter: '<tomcat-users>' path: /usr/share/tomcat/conf/tomcat-users.xml line: '<user username=\"admin\" password=\"Password123!\" roles=\"manager-gui,admin-gui\"/>' add ports for exception in firewall - name: add ports to firewalld firewalld: port: '{{item}}' state: enabled immediate: true permanent: true with_items: - 8080/tcp - 80/tcp - 4000/tcp Since this setup will be needing multicast, need to allow multicast address - name: add multicast address to firewalld firewalld: rich_rule: rule family=\"ipv4\" destination address=\"228.0.0.4\" protocol value=\"ip\" accept permanent: true immediate: true state: enabled Create directories - name: create directories for sites-enabled, sites-available and test directory for tomcat file: state: directory path: '{{item}}' with_items: - /etc/httpd/sites-enabled - /etc/httpd/sites-available - /var/lib/tomcat/webapps/test - /var/lib/tomcat/webapps/test/WEB-INF - /var/www/sandbox Insert the index.jsp file to the newly created test directory - name: create test application file copy: src: files/app/test-index.jsp dest: /var/lib/tomcat/webapps/test/index.jsp Copy each virtual host file and paste it on /etc/httpd/sites-available on each server - name: copy http tomcat configuration file for app1 copy: src: files/app/tomcat1.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app1\" - name: copy http tomcat configuration file for app2 copy: src: files/app/tomcat2.conf dest: /etc/httpd/sites-available/tomcat.conf when: ansible_hostname == \"app2\" Contents of tomcat1.conf and tomcat2.conf # tomcat1.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app1.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app1.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app1.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app1_error.log CustomLog /var/log/httpd/app1_common.log combined </VirtualHost> # tomcat2.conf <VirtualHost *:80> ServerAdmin webmaster@sandbox.io DocumentRoot /var/www/sandbox ServerName app2.sandbox.io ServerAlias RewriteEngine On RewriteRule ^/(.*) http://app2.sandbox.io:8080/test/ [P] ProxyPassReverse / http://app2.sandbox.io:8080/test/ ProxyPassReverseCookiePath /test / ErrorLog /var/log/httpd/app2_error.log CustomLog /var/log/httpd/app2_common.log combined </VirtualHost> Create a softlink of virtualhost file to /etc/httpd/sites-enabled/ - name: create link of tomcat.conf to sites-enabled file: src: /etc/httpd/sites-available/tomcat.conf dest: /etc/httpd/sites-enabled/tomcat.conf state: link Add 'IncludeOptional sites-enabled/*.conf' in /etc/httpd/conf/httpd.conf to add all conf files in sites-enabled directory - name: add line to /etc/httpd/conf/httpd.conf lineinfile: path: /etc/httpd/conf/httpd.conf line: 'IncludeOptional sites-enabled/*.conf' - name: set boolean for httpd_can_network_connect to true Create labels for tomcat setup on both servers - name: create labels on each node in /etc/tomcat/server.xml for app1 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat1\">' when: ansible_hostname == \"app1\" - name: create labels on each node in /etc/tomcat/server.xml for app2 lineinfile: path: /etc/tomcat/server.xml regexp: 'Engine name=\"Catalina\"' line: '<Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"tomcat2\">' when: ansible_hostname == \"app2\" Create web.xml file /var/lib/tomcat/webapps/test/WEB-INF/ directory - name: add web.xml file to test directory copy: src: files/app/web.xml dest: /var/lib/tomcat/webapps/test/WEB-INF/web.xml Contents of web.xml <?xml version=\"1.0\" encoding=\"ISO-8859-1\"?> <web-app xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\" metadata-complete=\"true\"> <display-name>Welcome to Tomcat</display-name> <description> Welcome to Tomcat </description> <distributable/> </web-app> Need to add the cluster function on /etc/tomcat/server.xml ive done this via blockinline on ansible - name: add line block on /etc/tomcat/server.xml blockinfile: path: /etc/tomcat/server.xml marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\" insertafter: '<Engine name=\"Catalina\" defaultHost=\"localhost\"' backup: true block: | <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\" channelSendOptions=\"8\"> <Manager className=\"org.apache.catalina.ha.session.DeltaManager\" expireSessionsOnShutdown=\"false\" notifyListenersOnReplication=\"true\"/> <Channel className=\"org.apache.catalina.tribes.group.GroupChannel\"> <Membership className=\"org.apache.catalina.tribes.membership.McastService\" address=\"228.0.0.4\" port=\"45564\" frequency=\"500\" dropTime=\"3000\"/> <Sender className=\"org.apache.catalina.tribes.transport.ReplicationTransmitter\"> <Transport className=\"org.apache.catalina.tribes.transport.nio.PooledParallelSender\"/> </Sender> <Receiver className=\"org.apache.catalina.tribes.transport.nio.NioReceiver\" address=\"auto\" port=\"4000\" autoBind=\"100\" selectorTimeout=\"5000\" maxThreads=\"6\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.TcpFailureDetector\"/> <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor\"/> </Channel> <Valve className=\"org.apache.catalina.ha.tcp.ReplicationValve\" filter=\"\"/> <Valve className=\"org.apache.catalina.ha.session.JvmRouteBinderValve\"/> <ClusterListener className=\"org.apache.catalina.ha.session.JvmRouteSessionIDBinderListener\"/> <ClusterListener className=\"org.apache.catalina.ha.session.ClusterSessionListener\"/> </Cluster> Start the services - name: start httpd service: name: httpd state: restarted enabled: true - name: start tomcat service service: name: tomcat state: restarted enabled: true","title":"Installation and Setup"},{"location":"app/#issues","text":"","title":"Issues"},{"location":"app/#running-on-lxc","text":"Synching issues may happen if your running on linux container, this is due to tomcat app and /etc/hosts file of the lxc not correctly getting the servers own IP. Edit /etc/nsswitch if you want a simpler fix. - name: swap dns and files entry on /etc/nsswitch lineinfile: path: /etc/nsswitch.conf regexp: '^hosts' line: 'hosts: dns files myhostname' when: ansible_virtualization_type == \"lxc\"","title":"Running on LXC"},{"location":"app/#running-on-kvm","text":"unlike LXC, you can run selinux on KVM and can cause issues, recommended to change booleans instead of disabling it permanently - name: set boolean for httpd_can_network_connect to true seboolean: name: '{{item}}' persistent: true state: true with_items: - httpd_can_network_connect - nis_enabled when: ansible_virtualization_type == \"kvm\"","title":"Running on KVM"},{"location":"app/#load-balancer-setup","text":"Configuration on load balancer is simple","title":"Load Balancer Setup"},{"location":"app/#installation-and-setup_1","text":"Install haproxy to the load balancer - name: install haproxy to the server yum: name: haproxy state: present replace haproxy.cfg with the configured one - name: copy haproxy.cfg file /etc/haproxy/haproxy.cfg copy: src: files/haproxy/haproxy.cfg dest: /etc/haproxy/haproxy.cfg Contents of configured haproxy.cfg #/etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 frontend tomcat_app_frontend bind *:80 stats uri /haproxy?stats default_backend tomcat_app_backend backend tomcat_app_backend balance roundrobin server app1 app1.sandbox.io:80 check server app2 app2.sandbox.io:80 checkv Open port 80/tcp on firewalld - name: open firewalld port 80 firewalld: service: http state: enabled permanent: true immediate: true Start haproxy service - name: start load balancing service: name: haproxy state: started enabled: true","title":"Installation and Setup"},{"location":"backup/","text":"Backup The Backup Server is a bacula server connected to the storage server DHCP and DNS Entries Forward lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 Reverse lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 107 IN PTR backup.sandbox.io. DHCP entry host backup { hardware ethernet 7A:B2:FA:F6:44:AB; fixed-address 10.11.100.107; } Process bconsole -> bacula-dir(9101) -> bacula-fd(9102) -> bacula-sd(9103) -> physical media bacula-dir -> SQL DBMS(Catalogs) bacula-fd is the important to open the port in client if you will install all functions in one server Installation and Setup Initial Setup Install necessary applications - name: install targetcli and iscsi-initiator-utils yum: name: - iscsi-initiator-utils - bacula-director - bacula-storage - bacula-console - bacula-client - postgresql - postgresql-server state: present Open ports for bacula-dir(9101) and bacula-sd(9103) - name: open ports on firewalld firewalld: port: '{{item}}' immediate: true permanent: true state: enabled with_items: - '9101/tcp' - '9103/tcp' Setup iscsi initiator to connect to LUN storage - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started enabled: true - name: discover the LUN share open_iscsi: show_nodes: true portal: 'storage.sandbox.io' discover: true - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage' Mounting the shares Add directory where the NFS and LUN storage will mount to and add context - name: create directory for bacula configuration file: path: '{{item}}' state: directory owner: bacula group: bacula with_items: - '/bacula' - '/bacula/LUN' - '/bacula/NFS' - '/etc/bacula/conf.d' - name: add fcontext to bacula directory for backup to run sefcontext: target: '/bacula(/.*)?' setype: bacula_store_t state: present - name: set use_nfs_home_dirs to true seboolean: name: use_nfs_home_dirs state: true persistent: true - name: restorecon the bacula directory shell: cmd: restorecon -Rv /bacula Create systemd files for mounting LUN and NFS - name: moved storage systemd files to /etc/systemd/system directory copy: src: 'files/backup/systemd/{{item}}' dest: /etc/systemd/system with_items: - 'bacula-LUN.mount' - 'bacula-LUN.automount' - 'bacula-NFS.automount' - 'bacula-NFS.mount' register: copy_files - name: systemd reload systemd: daemon_reload: true when: copy_files.changed == true Naming conventions of the systemd mount files should correlate where you want to mount the shared storage. For example we want to mount the NFS folder to /bacula/NFS, we need to create a systemd file that named bacula-NFS.mount Contents of bacula-LUN.mount [Unit] Description=Storage LUN mount for itself After=network.service NetworkManager.service [Mount] What=/dev/sdb1 Where=/bacula/LUN Type=xfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-LUN.automount [Unit] Description=Storage LUN automount for itself After=network.service NetworkManager.service [Automount] Where=/bacula/LUN [Install] WantedBy=multi-user.target Contents of bacula-NFS.mount [Unit] Description=Storage NFS mount for itself After=network.service NetworkManager.service [Mount] What=storage.sandbox.io:/var/nfs Where=/bacula/NFS Type=nfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-NFS.automount [Unit] Description=Storage NFS automount for itself After= network.service NetworkManager.service [Automount] Where=/bacula/NFS [Install] WantedBy=multi-user.target Bacula Setup Setup bacula database. - name: stat if bacula_sql_setup exists stat: path: /home/users/bot-acc/.system-checks/bacula_sql_setup register: bacula_sql_setup - name: execute commands for bacula database shell: cmd: '{{item}}' with_items: - sudo -Hiu postgres /usr/libexec/bacula/create_postgresql_database - sudo -Hiu postgres /usr/libexec/bacula/make_postgresql_tables - sudo -Hiu postgres /usr/libexec/bacula/grant_postgresql_privileges - sudo -Hiu postgres psql -U postgres -c \"ALTER DATABASE bacula OWNER TO bacula\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER bacula WITH PASSWORD 'Password123'\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER postgres WITH PASSWORD 'Password321'\" - touch /home/users/bot-acc/.system-checks/bacula_sql_setup when: not bacula_sql_setup.stat.exists Copy the files - name: replace files on /etc/bacula copy: src: 'files/backup/bacula-conf/{{item}}' dest: '/etc/bacula' with_items: - bacula-dir.conf - bacula-sd.conf - bacula-fd.conf - bconsole.conf - name: synchronized files in conf.d of /etc/bacula synchronize: src: 'files/backup/bacula-conf/nodes/' dest: '/etc/bacula/conf.d' Bacula Director Configuration Define Director function Director { Name = bacula-dir DIRport = 9101 QueryFile = \"/etc/bacula/query.sql\" WorkingDirectory = \"/var/spool/bacula\" PidDirectory = \"/var/run\" Maximum Concurrent Jobs = 1 Password = \"Password123!\" # Password for bconsole connection Messages = Daemon DirAddress = 127.0.0.1 # Director is installed in localhost } JobDefs is defined definition for jobs created later JobDefs { Name = \"DefaultJob\" Type = Backup Level = Incremental Client = Backup-Server #Move to Jobs FileSet = \"Full Set\" #Schedule = \"WeeklyCycle\" Storage = File-LUN Messages = Standard Pool = File Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } JobDefs { Name = \"RemoteDefault\" Type = Backup Level = Incremental FileSet = \"FileSet-Sandbox\" #Schedule = \"WeeklyCycle\" Storage = File-NFS Messages = Standard Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } Define Jobs to be run for backup Job { Name = \"Backup-Main\" JobDefs = \"DefaultJob\" } Job { Name = \"BackupCatalog\" JobDefs = \"DefaultJob\" Level = Full FileSet=\"Catalog\" #Schedule = \"WeeklyCycleAfterBackup\" RunBeforeJob = \"/usr/libexec/bacula/make_catalog_backup.pl MyCatalog\" RunAfterJob = \"/usr/libexec/bacula/delete_catalog_backup\" Write Bootstrap = \"/var/spool/bacula/%n.bsr\" Priority = 11 # run after main backup } Define Jobs to be run for restore Job { Name = \"Restore-Backup\" Type = Restore Client= Backup-Server FileSet=\"Full Set\" Storage = File-LUN Pool = Default Messages = Standard Where = /tmp/bacula-restore } Job { Name = \"Restore-Remote\" Type = Restore Client= DEV2 FileSet = \"FileSet-Sandbox\" Storage = File-NFS Pool = Default Messages = Standard Where = /tmp/bacula-restore } FileSet define what to include and exclude FileSet { Name = \"Full Set\" Include { Options { signature = MD5 compression = GZIP } File = / } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } FileSet { Name = \"Catalog\" Include { Options { signature = MD5 } File = \"/var/spool/bacula/bacula.sql\" } } FileSet { Name = \"FileSet-Sandbox\" Include { Options { signature = MD5 compression = GZIP } File = /home File = /etc File = /opt File = /var } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } Schedule define the time to run the backup, will not use it but good to be included Schedule { Name = \"WeeklyCycle\" Run = Full 1st sun at 23:05 Run = Differential 2nd-5th sun at 23:05 Run = Incremental mon-sat at 23:05 } Schedule { Name = \"WeeklyCycleAfterBackup\" Run = Full sun-sat at 23:10 } Storage define where the backup/restore location is, this is the connector to bacula-sd Storage { Name = File-LUN Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = LUNStorage Media Type = File } Storage { Name = File-NFS Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = NFSStorage Media Type = File } Define the Pool function Pool { Name = Default Pool Type = Backup Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days } Pool { Name = File Pool Type = Backup Label Format = BACKUP-Bacula- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G reasonable Maximum Volumes = 100 } Pool { Name = Scratch Pool Type = Backup } Pool { Name = Pool-DEV2 Pool Type = Backup Label Format = BACKUP-DEV2- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G g reasonable Maximum Volumes = 100 } Catalog function defines the database authentication Catalog { Name = MyCatalog dbname = \"bacula\"; dbuser = \"bacula\"; dbpassword = \"Password123\" } Not entirely sure on what console does as of the moment but will check on how Console { Name = bacula-mon Password = \"@@MON_DIR_PASSWORD@@\" CommandACL = status, .status } Client defines all the nodes that will need to be backup Client { Name = Backup-Server Address = backup.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Client { Name = @@CHANGE_THIS@@ Address = fqdn.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" n File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Add the following line in the config file for your config file to search created configurations in conf.d directory @|\"find /etc/bacula/conf.d -name '*.conf' -type f -exec echo @{} \\;\" Check your work bacula-dir -tc /etc/bacula/bacula-dir.conf Bacula Storage Daemon Configuration bacula-sd.conf will define all configuration for bacula-sd service Define Storage function Storage { Name = bacula-sd SDPort = 9103 WorkingDirectory = \"/var/spool/bacula\" Pid Directory = \"/var/run\" Maximum Concurrent Jobs = 20 SDAddress = backup.sandbox.io } Define Director authentication Director { Name = bacula-dir Password = \"Password123!\" } Device directive will choose where you want to put your files Device { Name = LUNStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled med ia Archive Device = /bacula/LUN Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Device { Name = NFSStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled me$ ia Archive Device = /bacula/NFS Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Check your configuration bacula-sd -tc /etc/bacula/bacula-sd.conf Bacula File Daemon(Client) Configuration Define director function for authentication on file daemon Director { Name = bacula-dir Password = \"Password123!\" } This is the only directives that is needed to change, else is optional","title":"Backup"},{"location":"backup/#backup","text":"The Backup Server is a bacula server connected to the storage server","title":"Backup"},{"location":"backup/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 Reverse lookup zone @ IN NS backup.sandbox.io. backup IN A 10.11.100.107 107 IN PTR backup.sandbox.io. DHCP entry host backup { hardware ethernet 7A:B2:FA:F6:44:AB; fixed-address 10.11.100.107; } Process bconsole -> bacula-dir(9101) -> bacula-fd(9102) -> bacula-sd(9103) -> physical media bacula-dir -> SQL DBMS(Catalogs) bacula-fd is the important to open the port in client if you will install all functions in one server","title":"DHCP and DNS Entries"},{"location":"backup/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"backup/#initial-setup","text":"Install necessary applications - name: install targetcli and iscsi-initiator-utils yum: name: - iscsi-initiator-utils - bacula-director - bacula-storage - bacula-console - bacula-client - postgresql - postgresql-server state: present Open ports for bacula-dir(9101) and bacula-sd(9103) - name: open ports on firewalld firewalld: port: '{{item}}' immediate: true permanent: true state: enabled with_items: - '9101/tcp' - '9103/tcp' Setup iscsi initiator to connect to LUN storage - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started enabled: true - name: discover the LUN share open_iscsi: show_nodes: true portal: 'storage.sandbox.io' discover: true - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage'","title":"Initial Setup"},{"location":"backup/#mounting-the-shares","text":"Add directory where the NFS and LUN storage will mount to and add context - name: create directory for bacula configuration file: path: '{{item}}' state: directory owner: bacula group: bacula with_items: - '/bacula' - '/bacula/LUN' - '/bacula/NFS' - '/etc/bacula/conf.d' - name: add fcontext to bacula directory for backup to run sefcontext: target: '/bacula(/.*)?' setype: bacula_store_t state: present - name: set use_nfs_home_dirs to true seboolean: name: use_nfs_home_dirs state: true persistent: true - name: restorecon the bacula directory shell: cmd: restorecon -Rv /bacula Create systemd files for mounting LUN and NFS - name: moved storage systemd files to /etc/systemd/system directory copy: src: 'files/backup/systemd/{{item}}' dest: /etc/systemd/system with_items: - 'bacula-LUN.mount' - 'bacula-LUN.automount' - 'bacula-NFS.automount' - 'bacula-NFS.mount' register: copy_files - name: systemd reload systemd: daemon_reload: true when: copy_files.changed == true Naming conventions of the systemd mount files should correlate where you want to mount the shared storage. For example we want to mount the NFS folder to /bacula/NFS, we need to create a systemd file that named bacula-NFS.mount Contents of bacula-LUN.mount [Unit] Description=Storage LUN mount for itself After=network.service NetworkManager.service [Mount] What=/dev/sdb1 Where=/bacula/LUN Type=xfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-LUN.automount [Unit] Description=Storage LUN automount for itself After=network.service NetworkManager.service [Automount] Where=/bacula/LUN [Install] WantedBy=multi-user.target Contents of bacula-NFS.mount [Unit] Description=Storage NFS mount for itself After=network.service NetworkManager.service [Mount] What=storage.sandbox.io:/var/nfs Where=/bacula/NFS Type=nfs Options=defaults [Install] WantedBy=multi-user.target Contents of bacula-NFS.automount [Unit] Description=Storage NFS automount for itself After= network.service NetworkManager.service [Automount] Where=/bacula/NFS [Install] WantedBy=multi-user.target","title":"Mounting the shares"},{"location":"backup/#bacula-setup","text":"Setup bacula database. - name: stat if bacula_sql_setup exists stat: path: /home/users/bot-acc/.system-checks/bacula_sql_setup register: bacula_sql_setup - name: execute commands for bacula database shell: cmd: '{{item}}' with_items: - sudo -Hiu postgres /usr/libexec/bacula/create_postgresql_database - sudo -Hiu postgres /usr/libexec/bacula/make_postgresql_tables - sudo -Hiu postgres /usr/libexec/bacula/grant_postgresql_privileges - sudo -Hiu postgres psql -U postgres -c \"ALTER DATABASE bacula OWNER TO bacula\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER bacula WITH PASSWORD 'Password123'\" - sudo -Hiu postgres psql -U postgres -c \"ALTER USER postgres WITH PASSWORD 'Password321'\" - touch /home/users/bot-acc/.system-checks/bacula_sql_setup when: not bacula_sql_setup.stat.exists Copy the files - name: replace files on /etc/bacula copy: src: 'files/backup/bacula-conf/{{item}}' dest: '/etc/bacula' with_items: - bacula-dir.conf - bacula-sd.conf - bacula-fd.conf - bconsole.conf - name: synchronized files in conf.d of /etc/bacula synchronize: src: 'files/backup/bacula-conf/nodes/' dest: '/etc/bacula/conf.d'","title":"Bacula Setup"},{"location":"backup/#bacula-director-configuration","text":"Define Director function Director { Name = bacula-dir DIRport = 9101 QueryFile = \"/etc/bacula/query.sql\" WorkingDirectory = \"/var/spool/bacula\" PidDirectory = \"/var/run\" Maximum Concurrent Jobs = 1 Password = \"Password123!\" # Password for bconsole connection Messages = Daemon DirAddress = 127.0.0.1 # Director is installed in localhost } JobDefs is defined definition for jobs created later JobDefs { Name = \"DefaultJob\" Type = Backup Level = Incremental Client = Backup-Server #Move to Jobs FileSet = \"Full Set\" #Schedule = \"WeeklyCycle\" Storage = File-LUN Messages = Standard Pool = File Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } JobDefs { Name = \"RemoteDefault\" Type = Backup Level = Incremental FileSet = \"FileSet-Sandbox\" #Schedule = \"WeeklyCycle\" Storage = File-NFS Messages = Standard Priority = 10 Write Bootstrap = \"/var/spool/bacula/%c.bsr\" } Define Jobs to be run for backup Job { Name = \"Backup-Main\" JobDefs = \"DefaultJob\" } Job { Name = \"BackupCatalog\" JobDefs = \"DefaultJob\" Level = Full FileSet=\"Catalog\" #Schedule = \"WeeklyCycleAfterBackup\" RunBeforeJob = \"/usr/libexec/bacula/make_catalog_backup.pl MyCatalog\" RunAfterJob = \"/usr/libexec/bacula/delete_catalog_backup\" Write Bootstrap = \"/var/spool/bacula/%n.bsr\" Priority = 11 # run after main backup } Define Jobs to be run for restore Job { Name = \"Restore-Backup\" Type = Restore Client= Backup-Server FileSet=\"Full Set\" Storage = File-LUN Pool = Default Messages = Standard Where = /tmp/bacula-restore } Job { Name = \"Restore-Remote\" Type = Restore Client= DEV2 FileSet = \"FileSet-Sandbox\" Storage = File-NFS Pool = Default Messages = Standard Where = /tmp/bacula-restore } FileSet define what to include and exclude FileSet { Name = \"Full Set\" Include { Options { signature = MD5 compression = GZIP } File = / } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } FileSet { Name = \"Catalog\" Include { Options { signature = MD5 } File = \"/var/spool/bacula/bacula.sql\" } } FileSet { Name = \"FileSet-Sandbox\" Include { Options { signature = MD5 compression = GZIP } File = /home File = /etc File = /opt File = /var } Exclude { File = /var/spool/bacula File = /tmp File = /proc File = /tmp File = /.journal File = /.fsck File = /bacula File = /dev File = /boot File = /run File = /sys } } Schedule define the time to run the backup, will not use it but good to be included Schedule { Name = \"WeeklyCycle\" Run = Full 1st sun at 23:05 Run = Differential 2nd-5th sun at 23:05 Run = Incremental mon-sat at 23:05 } Schedule { Name = \"WeeklyCycleAfterBackup\" Run = Full sun-sat at 23:10 } Storage define where the backup/restore location is, this is the connector to bacula-sd Storage { Name = File-LUN Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = LUNStorage Media Type = File } Storage { Name = File-NFS Address = backup.sandbox.io ed name here SDPort = 9103 Password = \"Password123!\" Device = NFSStorage Media Type = File } Define the Pool function Pool { Name = Default Pool Type = Backup Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days } Pool { Name = File Pool Type = Backup Label Format = BACKUP-Bacula- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G reasonable Maximum Volumes = 100 } Pool { Name = Scratch Pool Type = Backup } Pool { Name = Pool-DEV2 Pool Type = Backup Label Format = BACKUP-DEV2- Recycle = yes e Volumes AutoPrune = yes Volume Retention = 365 days Maximum Volume Bytes = 50G g reasonable Maximum Volumes = 100 } Catalog function defines the database authentication Catalog { Name = MyCatalog dbname = \"bacula\"; dbuser = \"bacula\"; dbpassword = \"Password123\" } Not entirely sure on what console does as of the moment but will check on how Console { Name = bacula-mon Password = \"@@MON_DIR_PASSWORD@@\" CommandACL = status, .status } Client defines all the nodes that will need to be backup Client { Name = Backup-Server Address = backup.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Client { Name = @@CHANGE_THIS@@ Address = fqdn.sandbox.io FDPort = 9102 Catalog = MyCatalog Password = \"Password123!\" n File Retention = 30 days Job Retention = 6 months AutoPrune = yes } Add the following line in the config file for your config file to search created configurations in conf.d directory @|\"find /etc/bacula/conf.d -name '*.conf' -type f -exec echo @{} \\;\" Check your work bacula-dir -tc /etc/bacula/bacula-dir.conf","title":"Bacula Director Configuration"},{"location":"backup/#bacula-storage-daemon-configuration","text":"bacula-sd.conf will define all configuration for bacula-sd service Define Storage function Storage { Name = bacula-sd SDPort = 9103 WorkingDirectory = \"/var/spool/bacula\" Pid Directory = \"/var/run\" Maximum Concurrent Jobs = 20 SDAddress = backup.sandbox.io } Define Director authentication Director { Name = bacula-dir Password = \"Password123!\" } Device directive will choose where you want to put your files Device { Name = LUNStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled med ia Archive Device = /bacula/LUN Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Device { Name = NFSStorage Media Type = File Archive Device = /tmp LabelMedia = yes; # lets Bacula label unlabeled me$ ia Archive Device = /bacula/NFS Random Access = Yes; AutomaticMount = yes; # when device opened, read it RemovableMedia = no; AlwaysOpen = no; } Check your configuration bacula-sd -tc /etc/bacula/bacula-sd.conf","title":"Bacula Storage Daemon Configuration"},{"location":"backup/#bacula-file-daemonclient-configuration","text":"Define director function for authentication on file daemon Director { Name = bacula-dir Password = \"Password123!\" } This is the only directives that is needed to change, else is optional","title":"Bacula File Daemon(Client) Configuration"},{"location":"bot/","text":"Automation Server The Servers FQDN will be bot.sandbox.io, for this one i dont want to lenghten all of my servers FQDN. DHCP and DNS Entries Forward lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 Reverse lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 105 IN PTR bot.sandbox.io. DHCP entry host bot { hardware ethernet 9A:5D:A5:C2:A3:25; fixed-address 10.11.100.105; } Initial Phase vimrc file setup .vimrc on the root account and copy the file to /etc/skel so all accounts created moving forward has the same configuration, this would be helpful as we would use ansible later set tabstop=4 set shiftwidth=4 set expandtab set number set smarttab set autoindent Firewall Ensure firewall port(8140) for puppet server is open. firewall-cmd --add-port=8140/tcp && firewall-cmd --runtime-to-permanent Puppet Server Installation and setup The Puppet server version would be puppet 7, this is why i downloaded the repository on the main website of puppet server and installed it to spacewalk. Note that the package i want to download is puppetserver instead of puppet-server, there is a puppet-server package lurking in EPEL repository and yum install -y puppetserver Once installed, edit /etc/sysconfig/puppetserver JAVA_ARGS option and change it according to your reference, i setup my puppet server to use only 512mb of my memory resources [root@bot ~]# cat /etc/sysconfig/puppetserver | grep ^JAVA_ARGS= JAVA_ARGS=\"-Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger\" [root@bot ~]# ps -o uname,rss,pmem,comm -u puppet USER RSS %MEM COMMAND puppet 610592 60.1 java Puppet autosign feature will make easier configuration when spawning a new VM, it will automatically attach to the puppet server and will not make authentication procedure, and thus making it unsecure. Since this is a development project and in a controlled network, i have enabled it. echo '*.sandbox.io' > /etc/puppetlabs/puppet/autosign.conf The working directory for you to setup configuration files is /etc/puppetlabs/code/environments/production . Create and edit site.pp file in manifests directory. # manifests/site.pp node default { include base } # This configuration will apply to all host connecting bot.sandbox.io Create directory in modules directory named base with a subdirectory called manifests. create init.pp file and that would be the main configuration file for every hosts that will connect to the puppet server # modules/base/manifests/init.pp class base { file { '/etc/yum.repos.d' : ensure => directory, recurse => true, purge => true } package { [git, ansible, policycoreutils-python, setroubleshoot-server, ipa-client, firewalld]: require => File['/etc/yum.repos.d'], ensure => present } } # Will remove repo files, and install necessary packages If the server has puppet-agent installed, you can type the following: puppet config set server bot.sandbox.io --section main puppet agent --test The output should look like this Info: Creating a new RSA SSL key for dev1.sandbox.io Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml Info: Creating a new SSL certificate request for dev1.sandbox.io Info: Certificate Request fingerprint (SHA256): 1D:D9:11:63:35:D8:00:26:EE:C0:11:E5:6D:B7:9B:CD:97:7E:44:B4:DA:59:FC:46:41:BC:54:AB:91:3A:5E:F3 Info: Downloaded certificate for dev1.sandbox.io from https://bot.sandbox.io:8140/puppet-ca/v1 Info: Using configured environment 'production' Info: Retrieving pluginfacts Info: Retrieving plugin Info: Caching catalog for dev1.sandbox.io Info: Applying configuration version '1629663815' Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Base.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-CR.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Debuginfo.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Media.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Sources.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Vault.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-fasttrack.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-x86_64-kernel.repo]/ensure: removed Notice: /Stage[main]/Base/Package[git]/ensure: created Notice: /Stage[main]/Base/Package[ansible]/ensure: created Notice: /Stage[main]/Base/Package[policycoreutils-python]/ensure: created Notice: /Stage[main]/Base/Package[setroubleshoot-server]/ensure: created Notice: /Stage[main]/Base/Package[ipa-client]/ensure: created Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml Notice: Applied catalog in 312.49 seconds We can move forward on adding additional configurations on the base/init.pp file Other Configurations ive added config.pp file in modules/base/manifests directory to put all my variables and not crowding the init.pp file, need it to be declared in init.pp file include base::config include base::secret Run and enable FirewallD service service { 'firewalld': ensure => running, enable => true, require => Package['firewalld'] } Add public key for easy access on root account file { '/root/.ssh': ensure => directory, owner => 0, group => 0, mode => '700' } file { '/root/.ssh/authorized_keys': ensure => file, owner => 0, group => 0, mode => '600', content => $::base::config::root_pub_key, require => File['/root/.ssh'] } Pre configured vimrc setup will be installed in root and skel directory file { ['/etc/skel/.vimrc', '/root/.vimrc']: ensure => file, content => $::base::config::vim_rc } Remove 'rhgb quiet' options in /etc/default/grub file, this is to view errors arising upon bootup. exec { 'remove rhgb and quiet': command => \"sed -i 's/ rhgb quiet//g' /etc/default/grub && grub2-mkconfig -o /boot/grub2/grub.cfg\", path => '/usr/bin:/usr/sbin', onlyif => \"grep ' rhgb quiet' /etc/default/grub\" } Disable root password authentication and use public key exec { 'disable root password auth': command => 'sed -i \"s/#PermitRootLogin yes/PermitRootLogin without-password/\" /etc/ssh/sshd_config', path => '/usr/bin:/usr/sbin', onlyif => 'grep \"#PermitRootLogin\" /etc/ssh/sshd_config', notify => Service['sshd'] } # SSH daemon restart placeholder for ssh to restart once /etc/ssh/sshd_config has been change. service { 'sshd': ensure=> running } Connect to FreeIPA Server exec { 'execute ipa-client-install': command => \"ipa-client-install -U -p '${::base::secret::ipa_user}' -w '${::base::secret::ipa_pass}' --domain=sandbox.io --server=directory1.sandbox.io --mkhomedir\", path => '/usr/bin:/usr/sbin', unless => \"grep 'URI ldaps://directory1.sandbox.io' /etc/openldap/ldap.conf 2>/dev/null\", require => Package['ipa-client'] } Create home directory of bot-acc for ansible file { '/home/users': ensure => directory, mode => '755' } file { 'bot-acc setup1': path => '/home/users/bot-acc', ensure => directory, source => '/etc/skel', recurse => true, mode => '600', owner => '779800006', group => '779800006', require => [ File['/home/users'] , Exec['execute ipa-client-install'] ] } Create local account so server is still accessible if cannot be access root via ssh user { 'user-account': ensure => present, name => $::base::secret::sys_user, password => $::base::secret::sys_passwd, managehome => true } Create ssh dir for bot-acc account, copy the files under modules/base/files/ansible_ssh/ directory file { 'bot-acc sshdir': ensure => directory, path => '/home/users/bot-acc/.ssh', mode => '600', owner => '779800006', group => '779800006', recurse => true, source => 'puppet:///modules/base/ansible_ssh', require => File['bot-acc setup1'] } Create .system-checks directory in /home/users/bot-acc, this will act as directory of files for the system to check prior to their execution file { 'bot-acc system_checks directory': ensure => directory, path => '/home/users/bot-acc/.system-checks', mode => '600', owner => '779800006', group => '779800006', require => Exec['copy /etc/skel to bot-acc'] } ansible_ssh files [root@bot production]# ll modules/base/files/ansible_ssh/ total 12 -rw-r--r--. 1 root root 381 Sep 4 14:29 authorized_keys -rw-r--r--. 1 root root 106 Sep 4 21:00 config -rw-r--r--. 1 root root 1675 Sep 4 23:42 id_rsa_ansible Set ansible password .ansible_pass file file { 'ansible_pass': ensure => file, path => '/home/users/bot-acc/.ansible_pass', mode => '600', owner => '779800006', group => '779800006', content => $::base::secret::ansible_pass } Setup sudoers file for freeipa groups file { 'sudoers for ipa': ensure => file, path => '/etc/sudoers.d/ipa', mode => '600', source => 'puppet:///modules/base/ipa_sudoers' } Current modules/base/files/ipa_sudoers contents %bot_admin ALL=(ALL) NOPASSWD: ALL Setup service file for ansible pull file { 'ansible_service file': ensure => file, path => '/etc/systemd/system/ansible-config.service', mode => '644', source => 'puppet:///modules/base/ansible_service/ansible-config.service' } ansible-config.service contents [Unit] Description=Run ansible-pull at first boot to apply environment configuration After=sssd.service [Service] ExecStart=/root/ansible-config.sh Type=oneshot [Install] WantedBy=multi-user.target Setup shell command ansible-command.sh file { 'ansible_pull command': ensure => file, path => '/root/ansible-config.sh', mode => '755', source => 'puppet:///modules/base/ansible_service/ansible-command.sh' } ansible-command.sh contents #!/bin/bash echo `date` > /home/users/bot-acc/bot-output.txt chmod 666 /home/users/bot-acc/bot-output.txt /opt/puppetlabs/bin/puppet agent --test | tee -a /home/users/bot-acc/bot-output.txt /usr/sbin/runuser -l bot-acc -c 'ansible-pull -U ansible:[redacted] -i inventory --accept-host-key --vault-password-file=~/.ansible_pass' | tee /root/ansible-output.txt In order to run puppet upon bootup, i will add it to kickstart file of spacewalk /opt/puppetlabs/bin/puppet config set server bot.sandbox.io --section main /opt/puppetlabs/bin/puppet agent --test | tee /root/puppet-log.txt Ansible Ansible package is already installed via PuppetServer Ansible Pull After creating the service for ansible to work, we are now on the setting up ansible for ansible pull [root@bot ansible_pull]# tree -L 2 . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 postconfig.yml \u2502 \u2514\u2500\u2500 preconfig.yml \u251c\u2500\u2500 files \u2502 \u251c\u2500\u2500 gpg_keys \u2502 \u2514\u2500\u2500 dev \u251c\u2500\u2500 inventory \u251c\u2500\u2500 local.yml \u2514\u2500\u2500 nodes \u2514\u2500\u2500 dev-setup.yml local.yml file will serve as our main file that will route configurations based on the hostname of the servers # Example local.yml file --- - hosts: localhost tasks: - name: run pre install setup include: configs/preconfig.yml - name: run setup for storage server include: nodes/dev-setup.yml when: ansible_hostname == \"dev\" - name: run post install setup include: configs/postconfig.yml ... inventory file will need all the hostnames of the servers that will sync on local.yml docs.sandbox.io dev.sandbox.io ansible.cfg will be our configuration setup for ansible-pull, host_key_checking = False will ignore StrictHostKeyChecking of the server. display_skipped_hosts = False will disable displaying of skipped tasks. other configurations are for privesc. [defaults] host_key_checking = False display_skipped_hosts = False [privilege_escalation] become = True become_method = sudo become_user = root become_ask_pass = False preconfig.yml and postconfig.yml in config directory will act as a base configuration for ansible. installing necessary things first before node configurations, and cleanup after. # Example preconfig.yml file - name: make sure base GPG keys are installed rpm_key: key: '{{item}}' state: present with_items: - 'files/gpg_keys/centos7/centos-base-gpg' - 'files/gpg_keys/centos7/centos-epel-gpg' - 'files/gpg_keys/centos7/puppet-release-1-gpg' - 'files/gpg_keys/centos7/puppet-release-2-gpg' - 'files/gpg_keys/centos7/spacewalk-base-gpg' - 'files/gpg_keys/centos7/spacewalk-java-gpg' In order for ansible pull to work, we need to configure the directory to use git. git init git add -A git commit -m \"initial commit\" ansible-command.sh already configured the ansible-pull command.","title":"Automation"},{"location":"bot/#automation-server","text":"The Servers FQDN will be bot.sandbox.io, for this one i dont want to lenghten all of my servers FQDN.","title":"Automation Server"},{"location":"bot/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 Reverse lookup zone @ IN NS bot.sandbox.io. bot IN A 10.11.100.105 105 IN PTR bot.sandbox.io. DHCP entry host bot { hardware ethernet 9A:5D:A5:C2:A3:25; fixed-address 10.11.100.105; }","title":"DHCP and DNS Entries"},{"location":"bot/#initial-phase","text":"","title":"Initial Phase"},{"location":"bot/#vimrc-file","text":"setup .vimrc on the root account and copy the file to /etc/skel so all accounts created moving forward has the same configuration, this would be helpful as we would use ansible later set tabstop=4 set shiftwidth=4 set expandtab set number set smarttab set autoindent","title":"vimrc file"},{"location":"bot/#firewall","text":"Ensure firewall port(8140) for puppet server is open. firewall-cmd --add-port=8140/tcp && firewall-cmd --runtime-to-permanent","title":"Firewall"},{"location":"bot/#puppet-server","text":"","title":"Puppet Server"},{"location":"bot/#installation-and-setup","text":"The Puppet server version would be puppet 7, this is why i downloaded the repository on the main website of puppet server and installed it to spacewalk. Note that the package i want to download is puppetserver instead of puppet-server, there is a puppet-server package lurking in EPEL repository and yum install -y puppetserver Once installed, edit /etc/sysconfig/puppetserver JAVA_ARGS option and change it according to your reference, i setup my puppet server to use only 512mb of my memory resources [root@bot ~]# cat /etc/sysconfig/puppetserver | grep ^JAVA_ARGS= JAVA_ARGS=\"-Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger\" [root@bot ~]# ps -o uname,rss,pmem,comm -u puppet USER RSS %MEM COMMAND puppet 610592 60.1 java Puppet autosign feature will make easier configuration when spawning a new VM, it will automatically attach to the puppet server and will not make authentication procedure, and thus making it unsecure. Since this is a development project and in a controlled network, i have enabled it. echo '*.sandbox.io' > /etc/puppetlabs/puppet/autosign.conf The working directory for you to setup configuration files is /etc/puppetlabs/code/environments/production . Create and edit site.pp file in manifests directory. # manifests/site.pp node default { include base } # This configuration will apply to all host connecting bot.sandbox.io Create directory in modules directory named base with a subdirectory called manifests. create init.pp file and that would be the main configuration file for every hosts that will connect to the puppet server # modules/base/manifests/init.pp class base { file { '/etc/yum.repos.d' : ensure => directory, recurse => true, purge => true } package { [git, ansible, policycoreutils-python, setroubleshoot-server, ipa-client, firewalld]: require => File['/etc/yum.repos.d'], ensure => present } } # Will remove repo files, and install necessary packages If the server has puppet-agent installed, you can type the following: puppet config set server bot.sandbox.io --section main puppet agent --test The output should look like this Info: Creating a new RSA SSL key for dev1.sandbox.io Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml Info: Creating a new SSL certificate request for dev1.sandbox.io Info: Certificate Request fingerprint (SHA256): 1D:D9:11:63:35:D8:00:26:EE:C0:11:E5:6D:B7:9B:CD:97:7E:44:B4:DA:59:FC:46:41:BC:54:AB:91:3A:5E:F3 Info: Downloaded certificate for dev1.sandbox.io from https://bot.sandbox.io:8140/puppet-ca/v1 Info: Using configured environment 'production' Info: Retrieving pluginfacts Info: Retrieving plugin Info: Caching catalog for dev1.sandbox.io Info: Applying configuration version '1629663815' Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Base.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-CR.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Debuginfo.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Media.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Sources.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-Vault.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-fasttrack.repo]/ensure: removed Notice: /Stage[main]/Base/File[/etc/yum.repos.d/CentOS-x86_64-kernel.repo]/ensure: removed Notice: /Stage[main]/Base/Package[git]/ensure: created Notice: /Stage[main]/Base/Package[ansible]/ensure: created Notice: /Stage[main]/Base/Package[policycoreutils-python]/ensure: created Notice: /Stage[main]/Base/Package[setroubleshoot-server]/ensure: created Notice: /Stage[main]/Base/Package[ipa-client]/ensure: created Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml Notice: Applied catalog in 312.49 seconds We can move forward on adding additional configurations on the base/init.pp file","title":"Installation and setup"},{"location":"bot/#other-configurations","text":"ive added config.pp file in modules/base/manifests directory to put all my variables and not crowding the init.pp file, need it to be declared in init.pp file include base::config include base::secret Run and enable FirewallD service service { 'firewalld': ensure => running, enable => true, require => Package['firewalld'] } Add public key for easy access on root account file { '/root/.ssh': ensure => directory, owner => 0, group => 0, mode => '700' } file { '/root/.ssh/authorized_keys': ensure => file, owner => 0, group => 0, mode => '600', content => $::base::config::root_pub_key, require => File['/root/.ssh'] } Pre configured vimrc setup will be installed in root and skel directory file { ['/etc/skel/.vimrc', '/root/.vimrc']: ensure => file, content => $::base::config::vim_rc } Remove 'rhgb quiet' options in /etc/default/grub file, this is to view errors arising upon bootup. exec { 'remove rhgb and quiet': command => \"sed -i 's/ rhgb quiet//g' /etc/default/grub && grub2-mkconfig -o /boot/grub2/grub.cfg\", path => '/usr/bin:/usr/sbin', onlyif => \"grep ' rhgb quiet' /etc/default/grub\" } Disable root password authentication and use public key exec { 'disable root password auth': command => 'sed -i \"s/#PermitRootLogin yes/PermitRootLogin without-password/\" /etc/ssh/sshd_config', path => '/usr/bin:/usr/sbin', onlyif => 'grep \"#PermitRootLogin\" /etc/ssh/sshd_config', notify => Service['sshd'] } # SSH daemon restart placeholder for ssh to restart once /etc/ssh/sshd_config has been change. service { 'sshd': ensure=> running } Connect to FreeIPA Server exec { 'execute ipa-client-install': command => \"ipa-client-install -U -p '${::base::secret::ipa_user}' -w '${::base::secret::ipa_pass}' --domain=sandbox.io --server=directory1.sandbox.io --mkhomedir\", path => '/usr/bin:/usr/sbin', unless => \"grep 'URI ldaps://directory1.sandbox.io' /etc/openldap/ldap.conf 2>/dev/null\", require => Package['ipa-client'] } Create home directory of bot-acc for ansible file { '/home/users': ensure => directory, mode => '755' } file { 'bot-acc setup1': path => '/home/users/bot-acc', ensure => directory, source => '/etc/skel', recurse => true, mode => '600', owner => '779800006', group => '779800006', require => [ File['/home/users'] , Exec['execute ipa-client-install'] ] } Create local account so server is still accessible if cannot be access root via ssh user { 'user-account': ensure => present, name => $::base::secret::sys_user, password => $::base::secret::sys_passwd, managehome => true } Create ssh dir for bot-acc account, copy the files under modules/base/files/ansible_ssh/ directory file { 'bot-acc sshdir': ensure => directory, path => '/home/users/bot-acc/.ssh', mode => '600', owner => '779800006', group => '779800006', recurse => true, source => 'puppet:///modules/base/ansible_ssh', require => File['bot-acc setup1'] } Create .system-checks directory in /home/users/bot-acc, this will act as directory of files for the system to check prior to their execution file { 'bot-acc system_checks directory': ensure => directory, path => '/home/users/bot-acc/.system-checks', mode => '600', owner => '779800006', group => '779800006', require => Exec['copy /etc/skel to bot-acc'] } ansible_ssh files [root@bot production]# ll modules/base/files/ansible_ssh/ total 12 -rw-r--r--. 1 root root 381 Sep 4 14:29 authorized_keys -rw-r--r--. 1 root root 106 Sep 4 21:00 config -rw-r--r--. 1 root root 1675 Sep 4 23:42 id_rsa_ansible Set ansible password .ansible_pass file file { 'ansible_pass': ensure => file, path => '/home/users/bot-acc/.ansible_pass', mode => '600', owner => '779800006', group => '779800006', content => $::base::secret::ansible_pass } Setup sudoers file for freeipa groups file { 'sudoers for ipa': ensure => file, path => '/etc/sudoers.d/ipa', mode => '600', source => 'puppet:///modules/base/ipa_sudoers' } Current modules/base/files/ipa_sudoers contents %bot_admin ALL=(ALL) NOPASSWD: ALL Setup service file for ansible pull file { 'ansible_service file': ensure => file, path => '/etc/systemd/system/ansible-config.service', mode => '644', source => 'puppet:///modules/base/ansible_service/ansible-config.service' } ansible-config.service contents [Unit] Description=Run ansible-pull at first boot to apply environment configuration After=sssd.service [Service] ExecStart=/root/ansible-config.sh Type=oneshot [Install] WantedBy=multi-user.target Setup shell command ansible-command.sh file { 'ansible_pull command': ensure => file, path => '/root/ansible-config.sh', mode => '755', source => 'puppet:///modules/base/ansible_service/ansible-command.sh' } ansible-command.sh contents #!/bin/bash echo `date` > /home/users/bot-acc/bot-output.txt chmod 666 /home/users/bot-acc/bot-output.txt /opt/puppetlabs/bin/puppet agent --test | tee -a /home/users/bot-acc/bot-output.txt /usr/sbin/runuser -l bot-acc -c 'ansible-pull -U ansible:[redacted] -i inventory --accept-host-key --vault-password-file=~/.ansible_pass' | tee /root/ansible-output.txt In order to run puppet upon bootup, i will add it to kickstart file of spacewalk /opt/puppetlabs/bin/puppet config set server bot.sandbox.io --section main /opt/puppetlabs/bin/puppet agent --test | tee /root/puppet-log.txt","title":"Other Configurations"},{"location":"bot/#ansible","text":"Ansible package is already installed via PuppetServer","title":"Ansible"},{"location":"bot/#ansible-pull","text":"After creating the service for ansible to work, we are now on the setting up ansible for ansible pull [root@bot ansible_pull]# tree -L 2 . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 postconfig.yml \u2502 \u2514\u2500\u2500 preconfig.yml \u251c\u2500\u2500 files \u2502 \u251c\u2500\u2500 gpg_keys \u2502 \u2514\u2500\u2500 dev \u251c\u2500\u2500 inventory \u251c\u2500\u2500 local.yml \u2514\u2500\u2500 nodes \u2514\u2500\u2500 dev-setup.yml local.yml file will serve as our main file that will route configurations based on the hostname of the servers # Example local.yml file --- - hosts: localhost tasks: - name: run pre install setup include: configs/preconfig.yml - name: run setup for storage server include: nodes/dev-setup.yml when: ansible_hostname == \"dev\" - name: run post install setup include: configs/postconfig.yml ... inventory file will need all the hostnames of the servers that will sync on local.yml docs.sandbox.io dev.sandbox.io ansible.cfg will be our configuration setup for ansible-pull, host_key_checking = False will ignore StrictHostKeyChecking of the server. display_skipped_hosts = False will disable displaying of skipped tasks. other configurations are for privesc. [defaults] host_key_checking = False display_skipped_hosts = False [privilege_escalation] become = True become_method = sudo become_user = root become_ask_pass = False preconfig.yml and postconfig.yml in config directory will act as a base configuration for ansible. installing necessary things first before node configurations, and cleanup after. # Example preconfig.yml file - name: make sure base GPG keys are installed rpm_key: key: '{{item}}' state: present with_items: - 'files/gpg_keys/centos7/centos-base-gpg' - 'files/gpg_keys/centos7/centos-epel-gpg' - 'files/gpg_keys/centos7/puppet-release-1-gpg' - 'files/gpg_keys/centos7/puppet-release-2-gpg' - 'files/gpg_keys/centos7/spacewalk-base-gpg' - 'files/gpg_keys/centos7/spacewalk-java-gpg' In order for ansible pull to work, we need to configure the directory to use git. git init git add -A git commit -m \"initial commit\" ansible-command.sh already configured the ansible-pull command.","title":"Ansible Pull"},{"location":"docs/","text":"Documentation DHCP and DNS Entries Forward lookup zone @ IN NS docs.sandbox.io. docs IN A 10.11.100.103 devdocs IN CNAME docs portfolio IN CNAME docs Reverse lookup zone @ IN NS docs.sandbox.io. docs IN A 10.11.100.103 103 IN PTR docs.sandbox.io. DHCP entry host docs { hardware ethernet 0E:32:D1:C2:8A:73; fixed-address 10.11.100.103; }","title":"Documentation"},{"location":"docs/#documentation","text":"","title":"Documentation"},{"location":"docs/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS docs.sandbox.io. docs IN A 10.11.100.103 devdocs IN CNAME docs portfolio IN CNAME docs Reverse lookup zone @ IN NS docs.sandbox.io. docs IN A 10.11.100.103 103 IN PTR docs.sandbox.io. DHCP entry host docs { hardware ethernet 0E:32:D1:C2:8A:73; fixed-address 10.11.100.103; }","title":"DHCP and DNS Entries"},{"location":"freeipa/","text":"FreeIPA Documentation DHCP and DNS Entries Forward lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 Reverse lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 102 IN PTR directory.sandbox.io. DHCP client entry host directory { hardware ethernet 32:80:18:70:D6:CA; fixed-address 10.11.100.102; } Installation Prerequisites TCP Open Ports: - 80, 443: HTTP/HTTPS - 389, 636: LDAP/LDAPS - 88, 464: Kerberos UDP Open Ports: - 88, 464: Kerberos - 123: NTP 2Gb Recommended RAM, though the RAM usage will average to 1.3Gb it will be faster to boot on 2Gb of memory. Installing FreeIPA Server Install FreeIPA server using yum -y install ipa-server Once the packages are install you can continue installation by using ipa-server-install [root@directory ~]# ipa-server-install The log file for this installation can be found in /var/log/ipaserver-install.log ============================================================================== This program will set up the IPA Server. This includes: * Configure a stand-alone CA (dogtag) for certificate management * Configure the Network Time Daemon (ntpd) * Create and configure an instance of Directory Server * Create and configure a Kerberos Key Distribution Center (KDC) * Configure Apache (httpd) * Configure the KDC to enable PKINIT The command will ask for things you need to fill up, you should leave all default if you are using the freeipa server as kerberos authentication too. All default answers will be encapsulated with [ ] To accept the default shown in brackets, press the Enter key. WARNING: conflicting time&date synchronization service 'chronyd' will be disabled in favor of ntpd Do you want to configure integrated DNS (BIND)? [no]: Enter the fully qualified domain name of the computer on which you are setting up server software. Using the form <hostname>.<domainname> Example: master.example.com. Server host name [directory.sandbox.io]: The domain name has been determined based on the host name. Please confirm the domain name [sandbox.io]: The kerberos protocol requires a Realm name to be defined. This is typically the domain name converted to uppercase. Please provide a realm name [SANDBOX.IO]: This will also ask for manager and admin password, set it up Certain directory server operations require an administrative user. This user is referred to as the Directory Manager and has full access to the Directory for system management tasks and will be added to the instance of directory server created for IPA. The password must be at least 8 characters long. Directory Manager password: Password (confirm): The IPA server requires an administrative user, named 'admin'. This user is a regular system account used for IPA server administration. IPA admin password: Password (confirm): Once done, it will prompt you to configure the freeipa server with the set values, type yes. The IPA Master Server will be configured with: Hostname: directory.sandbox.io IP address(es): 10.11.100.200 Domain name: sandbox.io Realm name: SANDBOX.IO Continue to configure the system with these values? [no]: yes It will start configuring itself, after setting up it will also note you to open necessary ports for the server to be able to communicate outside ... SSSD enabled Configured /etc/openldap/ldap.conf Configured /etc/ssh/ssh_config Configured /etc/ssh/sshd_config Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful Please add records in this file to your DNS system: /tmp/ipa.system.records.zcm0BH.db ============================================================================== Setup complete Next steps: 1. You must make sure these network ports are open: TCP Ports: * 80, 443: HTTP/HTTPS * 389, 636: LDAP/LDAPS * 88, 464: kerberos UDP Ports: * 88, 464: kerberos * 123: ntp 2. You can now obtain a kerberos ticket using the command: 'kinit admin' This ticket will allow you to use the IPA tools (e.g., ipa user-add) and the web user interface. Be sure to back up the CA certificates stored in /root/cacert.p12 These files are required to create replicas. The password for these files is the Directory Manager password The installer will also provide DNS entries for you to put in your DNS server _kerberos-master._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos-master._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos.sandbox.io. 86400 IN TXT \"SANDBOX.IO\" _kpasswd._tcp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _kpasswd._udp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _ldap._tcp.sandbox.io. 86400 IN SRV 0 100 389 directory.sandbox.io. _ntp._udp.sandbox.io. 86400 IN SRV 0 100 123 directory.sandbox.io. Open the necessary ports to firewall [root@directory ~]# firewall-cmd --add-port={80,443,389,636,88,464}/tcp success [root@directory ~]# firewall-cmd --add-port={88,464,123}/udp success [root@directory ~]# firewall-cmd --list-all public (active) target: default icmp-block-inversion: no interfaces: eth0 sources: services: dhcpv6-client ssh ports: 80/tcp 443/tcp 389/tcp 636/tcp 88/tcp 464/tcp 88/udp 464/udp 123/udp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: The installation is now complete, you may want to add users/group via command line or the browser. Installing FreeIPA Client Install FreeIPA client yum install -y ipa-client Setup FreeIPA Client using ipa-client-install --mkhomedir , the server will automatically search the directory server. [root@docs ~]# ipa-client-install --mkhomedir WARNING: ntpd time&date synchronization service will not be configured as conflicting service (chronyd) is enabled Use --force-ntpd option to disable it and force configuration of ntpd Discovery was successful! Client hostname: docs.sandbox.io Realm: SANDBOX.IO DNS Domain: sandbox.io IPA Server: directory.sandbox.io BaseDN: dc=sandbox,dc=io Continue to configure the system with these values? [no]: yes Skipping synchronizing time with NTP server. User authorized to enroll computers: dbalgos Password for dbalgos@SANDBOX.IO: Successfully retrieved CA cert Subject: CN=Certificate Authority,O=SANDBOX.IO Issuer: CN=Certificate Authority,O=SANDBOX.IO Valid From: 2021-08-16 05:49:51 Valid Until: 2041-08-16 05:49:51 Enrolled in IPA realm SANDBOX.IO Created /etc/ipa/default.conf New SSSD config will be created Configured sudoers in /etc/nsswitch.conf ... Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful You can now login any account registered in the FreeIPA server [root@docs ~]# id dbalgos uid=779800005(dbalgos) gid=779800005(dbalgos) groups=779800005(dbalgos),779800000(admins) In case we want to uninstall, we can use ipa-client-install --uninstall command [root@directory ~]# ipa-client-install --uninstall Unenrolling client from IPA server Unenrolling host failed: Error obtaining initial credentials: Cannot contact any KDC for requested realm. Removing Kerberos service principals from /etc/krb5.keytab Disabling client Kerberos and LDAP configurations Redundant SSSD configuration file /etc/sssd/sssd.conf was moved to /etc/sssd/sssd.conf.deleted Restoring client configuration files Unconfiguring the NIS domain. nscd daemon is not installed, skip configuration nslcd daemon is not installed, skip configuration Systemwide CA database updated. Client uninstall complete. The original nsswitch.conf configuration has been restored. You may need to restart services or reboot the machine. Do you want to reboot the machine? [no]: y","title":"FreeIPA"},{"location":"freeipa/#freeipa-documentation","text":"","title":"FreeIPA Documentation"},{"location":"freeipa/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 Reverse lookup zone @ IN NS directory.sandbox.io. directory IN A 10.11.100.102 102 IN PTR directory.sandbox.io. DHCP client entry host directory { hardware ethernet 32:80:18:70:D6:CA; fixed-address 10.11.100.102; }","title":"DHCP and DNS Entries"},{"location":"freeipa/#installation","text":"","title":"Installation"},{"location":"freeipa/#prerequisites","text":"TCP Open Ports: - 80, 443: HTTP/HTTPS - 389, 636: LDAP/LDAPS - 88, 464: Kerberos UDP Open Ports: - 88, 464: Kerberos - 123: NTP 2Gb Recommended RAM, though the RAM usage will average to 1.3Gb it will be faster to boot on 2Gb of memory.","title":"Prerequisites"},{"location":"freeipa/#installing-freeipa-server","text":"Install FreeIPA server using yum -y install ipa-server Once the packages are install you can continue installation by using ipa-server-install [root@directory ~]# ipa-server-install The log file for this installation can be found in /var/log/ipaserver-install.log ============================================================================== This program will set up the IPA Server. This includes: * Configure a stand-alone CA (dogtag) for certificate management * Configure the Network Time Daemon (ntpd) * Create and configure an instance of Directory Server * Create and configure a Kerberos Key Distribution Center (KDC) * Configure Apache (httpd) * Configure the KDC to enable PKINIT The command will ask for things you need to fill up, you should leave all default if you are using the freeipa server as kerberos authentication too. All default answers will be encapsulated with [ ] To accept the default shown in brackets, press the Enter key. WARNING: conflicting time&date synchronization service 'chronyd' will be disabled in favor of ntpd Do you want to configure integrated DNS (BIND)? [no]: Enter the fully qualified domain name of the computer on which you are setting up server software. Using the form <hostname>.<domainname> Example: master.example.com. Server host name [directory.sandbox.io]: The domain name has been determined based on the host name. Please confirm the domain name [sandbox.io]: The kerberos protocol requires a Realm name to be defined. This is typically the domain name converted to uppercase. Please provide a realm name [SANDBOX.IO]: This will also ask for manager and admin password, set it up Certain directory server operations require an administrative user. This user is referred to as the Directory Manager and has full access to the Directory for system management tasks and will be added to the instance of directory server created for IPA. The password must be at least 8 characters long. Directory Manager password: Password (confirm): The IPA server requires an administrative user, named 'admin'. This user is a regular system account used for IPA server administration. IPA admin password: Password (confirm): Once done, it will prompt you to configure the freeipa server with the set values, type yes. The IPA Master Server will be configured with: Hostname: directory.sandbox.io IP address(es): 10.11.100.200 Domain name: sandbox.io Realm name: SANDBOX.IO Continue to configure the system with these values? [no]: yes It will start configuring itself, after setting up it will also note you to open necessary ports for the server to be able to communicate outside ... SSSD enabled Configured /etc/openldap/ldap.conf Configured /etc/ssh/ssh_config Configured /etc/ssh/sshd_config Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful Please add records in this file to your DNS system: /tmp/ipa.system.records.zcm0BH.db ============================================================================== Setup complete Next steps: 1. You must make sure these network ports are open: TCP Ports: * 80, 443: HTTP/HTTPS * 389, 636: LDAP/LDAPS * 88, 464: kerberos UDP Ports: * 88, 464: kerberos * 123: ntp 2. You can now obtain a kerberos ticket using the command: 'kinit admin' This ticket will allow you to use the IPA tools (e.g., ipa user-add) and the web user interface. Be sure to back up the CA certificates stored in /root/cacert.p12 These files are required to create replicas. The password for these files is the Directory Manager password The installer will also provide DNS entries for you to put in your DNS server _kerberos-master._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos-master._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._tcp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos._udp.sandbox.io. 86400 IN SRV 0 100 88 directory.sandbox.io. _kerberos.sandbox.io. 86400 IN TXT \"SANDBOX.IO\" _kpasswd._tcp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _kpasswd._udp.sandbox.io. 86400 IN SRV 0 100 464 directory.sandbox.io. _ldap._tcp.sandbox.io. 86400 IN SRV 0 100 389 directory.sandbox.io. _ntp._udp.sandbox.io. 86400 IN SRV 0 100 123 directory.sandbox.io. Open the necessary ports to firewall [root@directory ~]# firewall-cmd --add-port={80,443,389,636,88,464}/tcp success [root@directory ~]# firewall-cmd --add-port={88,464,123}/udp success [root@directory ~]# firewall-cmd --list-all public (active) target: default icmp-block-inversion: no interfaces: eth0 sources: services: dhcpv6-client ssh ports: 80/tcp 443/tcp 389/tcp 636/tcp 88/tcp 464/tcp 88/udp 464/udp 123/udp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: The installation is now complete, you may want to add users/group via command line or the browser.","title":"Installing FreeIPA Server"},{"location":"freeipa/#installing-freeipa-client","text":"Install FreeIPA client yum install -y ipa-client Setup FreeIPA Client using ipa-client-install --mkhomedir , the server will automatically search the directory server. [root@docs ~]# ipa-client-install --mkhomedir WARNING: ntpd time&date synchronization service will not be configured as conflicting service (chronyd) is enabled Use --force-ntpd option to disable it and force configuration of ntpd Discovery was successful! Client hostname: docs.sandbox.io Realm: SANDBOX.IO DNS Domain: sandbox.io IPA Server: directory.sandbox.io BaseDN: dc=sandbox,dc=io Continue to configure the system with these values? [no]: yes Skipping synchronizing time with NTP server. User authorized to enroll computers: dbalgos Password for dbalgos@SANDBOX.IO: Successfully retrieved CA cert Subject: CN=Certificate Authority,O=SANDBOX.IO Issuer: CN=Certificate Authority,O=SANDBOX.IO Valid From: 2021-08-16 05:49:51 Valid Until: 2041-08-16 05:49:51 Enrolled in IPA realm SANDBOX.IO Created /etc/ipa/default.conf New SSSD config will be created Configured sudoers in /etc/nsswitch.conf ... Configuring sandbox.io as NIS domain. Client configuration complete. The ipa-client-install command was successful You can now login any account registered in the FreeIPA server [root@docs ~]# id dbalgos uid=779800005(dbalgos) gid=779800005(dbalgos) groups=779800005(dbalgos),779800000(admins) In case we want to uninstall, we can use ipa-client-install --uninstall command [root@directory ~]# ipa-client-install --uninstall Unenrolling client from IPA server Unenrolling host failed: Error obtaining initial credentials: Cannot contact any KDC for requested realm. Removing Kerberos service principals from /etc/krb5.keytab Disabling client Kerberos and LDAP configurations Redundant SSSD configuration file /etc/sssd/sssd.conf was moved to /etc/sssd/sssd.conf.deleted Restoring client configuration files Unconfiguring the NIS domain. nscd daemon is not installed, skip configuration nslcd daemon is not installed, skip configuration Systemwide CA database updated. Client uninstall complete. The original nsswitch.conf configuration has been restored. You may need to restart services or reboot the machine. Do you want to reboot the machine? [no]: y","title":"Installing FreeIPA Client"},{"location":"gateway/","text":"DNS and DHCP Server This server was previously created long before and cannot find my own documentation on this when i created it, so most of the instructions are from what i remember. Installation Ensure the machine has a second NIC, in this example the second NIC will be named as eth1 Install the following packages: iptables(remove firewalld if installed on the machine) bind, bind-utils, bind-libs dhcp yum install -y iptables bind bind-utils bind-libs dhcp systemctl stop firewalld && systemctl disable firewalld eth1 configuration must be set below: DEVICE=eth1 BOOTPROTO=none IPADDR=10.11.100.1 NETMASK=255.255.255.0 ONBOOT=yes NM_CONTROLLED=no TYPE=Ethernet DNS1=\"127.0.0.1\" DHCP server setup Edit dhcpd.conf to use spacewalk ip address as next-server for pxe default-lease-time 600; max-lease-time 7200; log-facility local7; authoritative; ignore client-updates; subnet 10.11.100.0 netmask 255.255.255.0{ range 10.11.100.10 10.11.100.50; option routers 10.11.100.1; option subnet-mask 255.255.255.0; option domain-name-servers 10.11.100.1; option domain-name \"sandbox.io\"; next-server 10.11.100.101; filename \"pxelinux.0\"; } For static IP assigning, see below as an example: host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; } Enable and start dhcpd server systemctl enable dhcpd systemctl start dhcpd DNS Server setup named.conf configuration Edit /etc/named.conf file, this file is the main configuration file of the bind package manager options { listen-on port 53 { 127.0.0.1; 10.11.100.1; }; listen-on-v6 port 53 { ::1; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; recursing-file \"/var/named/data/named.recursing\"; secroots-file \"/var/named/data/named.secroots\"; allow-query { localhost; 10.11.100.0/24; }; allow-transfer { localhost; 192.168.254.254; }; recursion yes; dnssec-enable yes; dnssec-validation yes; bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; pid-file \"/run/named/named.pid\"; session-keyfile \"/run/named/session.key\"; }; On the lower part of /etc/named.conf file, add the filenames on where you will put DNS records. zone \".\" IN { type hint; file \"named.ca\"; }; zone \"sandbox.io\" IN { type master; file \"sandbox.forward\"; allow-update { none; }; }; zone \"100.11.10.in-addr.arpa\" IN { type master; file \"sandbox.reverse\"; allow-update { none; }; }; Forward and reverse lookup zone The file sandbox.forward and sandbox.reverse that was stated in named.conf file needs to be created in /var/named touch /var/named/sandbox.forward touch /var/named/sandbox.reverse Both of those file will have the same heading template like below in order to work $TTL 86400 @ IN SOA dnsdhcp.sandbox.io. root.sandbox.io ( 2011071001 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL ) For every hostname, domain name should be stated with its ip address /etc/named/sandbox.forward @ IN NS dnsdhcp.sandbox.io. @ IN A 10.11.100.1 dnsdhcp IN A 10.11.100.1 dns-dhcp IN CNAME dnsdhcp #if CNAME is needed /etc/named/sandbox.reverse @ IN NS dnsdhcp.sandbox.io. dnsdhcp IN A 10.11.100.1 1 IN PTR dnsdhcp.sandbox.io. Start and enable bind service systemctl enable named systemctl start named Firewall plus other settings Iptables firewall configuration must be configured same as below *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [33:3512] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p tcp --dport 53 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p udp --dport 53 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p tcp --dport 80 -j ACCEPT -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -i eth0 -o eth1 -m state --state RELATED,ESTABLISHED -j ACCEPT -A FORWARD -i eth1 -o eth0 -j ACCEPT -A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT *nat :PREROUTING ACCEPT [76:8121] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [6:1968] :POSTROUTING ACCEPT [6:1968] -A POSTROUTING -o eth0 -j MASQUERADE COMMIT execute this on the command line to add ip forwarding in sysctl.conf echo net.ipv4.ip_forward = 1 >> /etc/sysctl.conf","title":"DNS and DHCP"},{"location":"gateway/#dns-and-dhcp-server","text":"This server was previously created long before and cannot find my own documentation on this when i created it, so most of the instructions are from what i remember.","title":"DNS and DHCP Server"},{"location":"gateway/#installation","text":"Ensure the machine has a second NIC, in this example the second NIC will be named as eth1 Install the following packages: iptables(remove firewalld if installed on the machine) bind, bind-utils, bind-libs dhcp yum install -y iptables bind bind-utils bind-libs dhcp systemctl stop firewalld && systemctl disable firewalld eth1 configuration must be set below: DEVICE=eth1 BOOTPROTO=none IPADDR=10.11.100.1 NETMASK=255.255.255.0 ONBOOT=yes NM_CONTROLLED=no TYPE=Ethernet DNS1=\"127.0.0.1\"","title":"Installation"},{"location":"gateway/#dhcp-server-setup","text":"Edit dhcpd.conf to use spacewalk ip address as next-server for pxe default-lease-time 600; max-lease-time 7200; log-facility local7; authoritative; ignore client-updates; subnet 10.11.100.0 netmask 255.255.255.0{ range 10.11.100.10 10.11.100.50; option routers 10.11.100.1; option subnet-mask 255.255.255.0; option domain-name-servers 10.11.100.1; option domain-name \"sandbox.io\"; next-server 10.11.100.101; filename \"pxelinux.0\"; } For static IP assigning, see below as an example: host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; } Enable and start dhcpd server systemctl enable dhcpd systemctl start dhcpd","title":"DHCP server setup"},{"location":"gateway/#dns-server-setup","text":"","title":"DNS Server setup"},{"location":"gateway/#namedconf-configuration","text":"Edit /etc/named.conf file, this file is the main configuration file of the bind package manager options { listen-on port 53 { 127.0.0.1; 10.11.100.1; }; listen-on-v6 port 53 { ::1; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; recursing-file \"/var/named/data/named.recursing\"; secroots-file \"/var/named/data/named.secroots\"; allow-query { localhost; 10.11.100.0/24; }; allow-transfer { localhost; 192.168.254.254; }; recursion yes; dnssec-enable yes; dnssec-validation yes; bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; pid-file \"/run/named/named.pid\"; session-keyfile \"/run/named/session.key\"; }; On the lower part of /etc/named.conf file, add the filenames on where you will put DNS records. zone \".\" IN { type hint; file \"named.ca\"; }; zone \"sandbox.io\" IN { type master; file \"sandbox.forward\"; allow-update { none; }; }; zone \"100.11.10.in-addr.arpa\" IN { type master; file \"sandbox.reverse\"; allow-update { none; }; };","title":"named.conf configuration"},{"location":"gateway/#forward-and-reverse-lookup-zone","text":"The file sandbox.forward and sandbox.reverse that was stated in named.conf file needs to be created in /var/named touch /var/named/sandbox.forward touch /var/named/sandbox.reverse Both of those file will have the same heading template like below in order to work $TTL 86400 @ IN SOA dnsdhcp.sandbox.io. root.sandbox.io ( 2011071001 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL ) For every hostname, domain name should be stated with its ip address /etc/named/sandbox.forward @ IN NS dnsdhcp.sandbox.io. @ IN A 10.11.100.1 dnsdhcp IN A 10.11.100.1 dns-dhcp IN CNAME dnsdhcp #if CNAME is needed /etc/named/sandbox.reverse @ IN NS dnsdhcp.sandbox.io. dnsdhcp IN A 10.11.100.1 1 IN PTR dnsdhcp.sandbox.io. Start and enable bind service systemctl enable named systemctl start named","title":"Forward and reverse lookup zone"},{"location":"gateway/#firewall-plus-other-settings","text":"Iptables firewall configuration must be configured same as below *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [33:3512] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p tcp --dport 53 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p udp --dport 53 -j ACCEPT -A INPUT -i eth1 -s 10.11.100.0/24 -p tcp --dport 80 -j ACCEPT -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -i eth0 -o eth1 -m state --state RELATED,ESTABLISHED -j ACCEPT -A FORWARD -i eth1 -o eth0 -j ACCEPT -A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT *nat :PREROUTING ACCEPT [76:8121] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [6:1968] :POSTROUTING ACCEPT [6:1968] -A POSTROUTING -o eth0 -j MASQUERADE COMMIT execute this on the command line to add ip forwarding in sysctl.conf echo net.ipv4.ip_forward = 1 >> /etc/sysctl.conf","title":"Firewall plus other settings"},{"location":"mail/","text":"Mail Server Design of the mail server is to gather all emails on the sandbox infastructure, once enough emails are gathered the emails will will be forwarded to my email DHCP and DNS Entries Forward lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 sandbox.io. IN MX 111 mail The MX record will forward all emails that is pointed to @sandbox.io to mail.sandbox.io Reverse lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 111 IN PTR mail.sandbox.io. DHCP entry host mail { hardware ethernet 02:AE:24:24:B7:7A; fixed-address 10.11.100.111; } Installation and Setup Install postfix, mailx, cyrus-sasl-plain packages - name: install postfix and other packages yum: name: - postfix - mailx - cyrus-sasl-plain state: present Open smtp port on the firewall - name: open smtp port 25 to firewalld firewalld: service: smtp immediate: true permanent: true state: enabled We need to change interface on postfix configuration file as we need the server to listen on its IP address to gather emails name: change inet_interface configuration on /etc/postfix/main.cf lineinfile: path: /etc/postfix/main.cf regexp: \"^inet_interfaces\" line: \"inet_interfaces = {{ ansible_fqdn }}\" register: interfaces_reg Append configuration settings on postfix main.cf file - name: append block of file on end of main.cf blockinfile: path: /etc/postfix/main.cf block: | myhostname = sandbox.io relayhost = [smtp.gmail.com]:587 smtp_use_tls = yes smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt smtp_sasl_security_options = noanonymous smtp_sasl_tls_security_options = noanonymous Create sasl_passwd to /etc/postfix directory, this will contain your gmail username and password so apply security if possible - name: copy sasl_passwd to /etc/postfix copy: src: files/mail/sasl_passwd dest: /etc/postfix/sasl_passwd Contents: [root@mail ~]# cat /etc/postfix/sasl_passwd [smtp.gmail.com]:587 [username]:[password] With sasl_passwd created, we can execute postmap binary to produce sasl db - name: execute shell command if sasl_db is does not exist shell: cmd: postmap /etc/postfix/sasl_passwd when: not sasl_db.stat.exists Restart the postfix server and enable it - name: start postfix service: name: postfix state: restarted enabled: true copy the file send_mail.sh to /root, it will be the shell to run every 10 minutes to check if there is enough emails to forward to my gmail account - name: copy files/mail/send_mail.sh to /root copy: src: files/mail/send_mail.sh dest: /root/send_mail.sh mode: '0755' Contents of send_mail.sh #!/bin/bash if ! [ -e /var/spool/mail/root ] ; then logger 'root mail does not exist' exit fi if [ -v $1 ] ; then echo './send_mail.sh $1' exit fi MAIL_COUNT=`grep \"^Subject:\" /var/spool/mail/root | wc -l` DATE_ATTACH=`date +%s` EMAIL=$1 if [ $MAIL_COUNT -ge 5 ]; then cat /var/spool/mail/root | sed -e '/^From /,+8d' | sed -e '/^User-Agent/,+4d' > /tmp/mail_body printf '\\n\\n-------------------------\\nSee attachment for full details'>> /tmp/mail_body cp /var/spool/mail/root /tmp/mail_attachment_$DATE_ATTACH mail -a /tmp/mail_attachment_$DATE_ATTACH -s \"All mails on `hostname -f`: `date \"+%m-%d-%Y %I:%M %p\"`\" $EMAIL < /tmp/mail_body if [ $? -eq 0 ]; then logger 'MAILER: Email sent' logger 'MAILER: Deleting mail logs' echo -n '' > /var/spool/mail/root logger 'MAILER: Deleting temporary files' rm -rf /tmp/mail_attachment_$DATE_ATTACH rm -rf /tmp/mail_body else logger 'MAILER: Email not sent' fi else logger 'MAILER: Not enough mails, skipping...' fi Create cron job for send_mail.sh - name: create cron job for send_mail.sh copy: src: files/mail/cron_mailer dest: /etc/cron.d/mailer Contents of mailer cronjob */10 * * * * root /root/send_mail.sh [email_address] Email from the mail server should look like below","title":"Mail"},{"location":"mail/#mail-server","text":"Design of the mail server is to gather all emails on the sandbox infastructure, once enough emails are gathered the emails will will be forwarded to my email","title":"Mail Server"},{"location":"mail/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 sandbox.io. IN MX 111 mail The MX record will forward all emails that is pointed to @sandbox.io to mail.sandbox.io Reverse lookup zone @ IN NS mail.sandbox.io. mail IN A 10.11.100.111 111 IN PTR mail.sandbox.io. DHCP entry host mail { hardware ethernet 02:AE:24:24:B7:7A; fixed-address 10.11.100.111; }","title":"DHCP and DNS Entries"},{"location":"mail/#installation-and-setup","text":"Install postfix, mailx, cyrus-sasl-plain packages - name: install postfix and other packages yum: name: - postfix - mailx - cyrus-sasl-plain state: present Open smtp port on the firewall - name: open smtp port 25 to firewalld firewalld: service: smtp immediate: true permanent: true state: enabled We need to change interface on postfix configuration file as we need the server to listen on its IP address to gather emails name: change inet_interface configuration on /etc/postfix/main.cf lineinfile: path: /etc/postfix/main.cf regexp: \"^inet_interfaces\" line: \"inet_interfaces = {{ ansible_fqdn }}\" register: interfaces_reg Append configuration settings on postfix main.cf file - name: append block of file on end of main.cf blockinfile: path: /etc/postfix/main.cf block: | myhostname = sandbox.io relayhost = [smtp.gmail.com]:587 smtp_use_tls = yes smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt smtp_sasl_security_options = noanonymous smtp_sasl_tls_security_options = noanonymous Create sasl_passwd to /etc/postfix directory, this will contain your gmail username and password so apply security if possible - name: copy sasl_passwd to /etc/postfix copy: src: files/mail/sasl_passwd dest: /etc/postfix/sasl_passwd Contents: [root@mail ~]# cat /etc/postfix/sasl_passwd [smtp.gmail.com]:587 [username]:[password] With sasl_passwd created, we can execute postmap binary to produce sasl db - name: execute shell command if sasl_db is does not exist shell: cmd: postmap /etc/postfix/sasl_passwd when: not sasl_db.stat.exists Restart the postfix server and enable it - name: start postfix service: name: postfix state: restarted enabled: true copy the file send_mail.sh to /root, it will be the shell to run every 10 minutes to check if there is enough emails to forward to my gmail account - name: copy files/mail/send_mail.sh to /root copy: src: files/mail/send_mail.sh dest: /root/send_mail.sh mode: '0755' Contents of send_mail.sh #!/bin/bash if ! [ -e /var/spool/mail/root ] ; then logger 'root mail does not exist' exit fi if [ -v $1 ] ; then echo './send_mail.sh $1' exit fi MAIL_COUNT=`grep \"^Subject:\" /var/spool/mail/root | wc -l` DATE_ATTACH=`date +%s` EMAIL=$1 if [ $MAIL_COUNT -ge 5 ]; then cat /var/spool/mail/root | sed -e '/^From /,+8d' | sed -e '/^User-Agent/,+4d' > /tmp/mail_body printf '\\n\\n-------------------------\\nSee attachment for full details'>> /tmp/mail_body cp /var/spool/mail/root /tmp/mail_attachment_$DATE_ATTACH mail -a /tmp/mail_attachment_$DATE_ATTACH -s \"All mails on `hostname -f`: `date \"+%m-%d-%Y %I:%M %p\"`\" $EMAIL < /tmp/mail_body if [ $? -eq 0 ]; then logger 'MAILER: Email sent' logger 'MAILER: Deleting mail logs' echo -n '' > /var/spool/mail/root logger 'MAILER: Deleting temporary files' rm -rf /tmp/mail_attachment_$DATE_ATTACH rm -rf /tmp/mail_body else logger 'MAILER: Email not sent' fi else logger 'MAILER: Not enough mails, skipping...' fi Create cron job for send_mail.sh - name: create cron job for send_mail.sh copy: src: files/mail/cron_mailer dest: /etc/cron.d/mailer Contents of mailer cronjob */10 * * * * root /root/send_mail.sh [email_address] Email from the mail server should look like below","title":"Installation and Setup"},{"location":"others/","text":"Other activities Self signed certificate setup for HTTPS access on webservers Create keys for self signed certificate openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/pki/tls/private/docs.key -out /etc/pki/tls/certs/docs.crt [root@docs ~]# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/pki/tls/private/docs.key -out /etc/pki/tls/certs/docs.crt Generating a 2048 bit RSA private key .........................................................................+++ ..................+++ writing new private key to '/etc/pki/tls/private/docs.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:[redacted] State or Province Name (full name) []:[redacted] Locality Name (eg, city) [Default City]:[redacted] Organization Name (eg, company) [Default Company Ltd]:Sandbox Organizational Unit Name (eg, section) []:IT Common Name (eg, your name or your server hostname) []:docs.sandbox.io Email Address []:web@sandbox.io Install mod_ssl, this will add ssl.conf in /etc/httpd/conf.d/ directory. yum install -y mod_ssl Edit /etc/httpd/conf.d/ssl.conf, Create a virtual host entry that redirects http(port 80) access to https(port 443) vim /etc/httpd/conf.d/ssl.conf <VirtualHost _default_:80> ServerName docs.sandbox.io:80 ServerAlias docs.sandbox.io Redirect permanent / https://docs.sandbox.io/ </VirtualHost> If virtual host for port 443 is not there, create it. <VirtualHost _default_:443> DocumentRoot \"/var/www/sandbox/site\" ServerName docs.sandbox.io:443 ErrorLog logs/ssl_error_log TransferLog logs/ssl_access_log LogLevel warn SSLEngine on SSLProtocol all -SSLv2 -SSLv3 SSLCipherSuite HIGH:3DES:!aNULL:!MD5:!SEED:!IDEA SSLCertificateFile /etc/pki/tls/certs/docs.crt SSLCertificateKeyFile /etc/pki/tls/private/docs.key <Files ~ \"\\.(cgi|shtml|phtml|php3?)$\"> SSLOptions +StdEnvVars </Files> <Directory \"/var/www/cgi-bin\"> SSLOptions +StdEnvVars </Directory> BrowserMatch \"MSIE [2-5]\" \\ nokeepalive ssl-unclean-shutdown \\ downgrade-1.0 force-response-1.0 CustomLog logs/ssl_request_log \\ \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\" </VirtualHost> Make sure to replace SSLCertificateFile and SSLCertificateKeyFile in the section below of the configuration file SSLCertificateFile /etc/pki/tls/certs/docs.crt SSLCertificateKeyFile /etc/pki/tls/private/docs.key Restart httpd service [root@docs ~]# systemctl status httpd | head \u25cf httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2021-10-29 03:28:39 PST; 15s ago","title":"Others"},{"location":"others/#other-activities","text":"","title":"Other activities"},{"location":"others/#self-signed-certificate-setup-for-https-access-on-webservers","text":"Create keys for self signed certificate openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/pki/tls/private/docs.key -out /etc/pki/tls/certs/docs.crt [root@docs ~]# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/pki/tls/private/docs.key -out /etc/pki/tls/certs/docs.crt Generating a 2048 bit RSA private key .........................................................................+++ ..................+++ writing new private key to '/etc/pki/tls/private/docs.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:[redacted] State or Province Name (full name) []:[redacted] Locality Name (eg, city) [Default City]:[redacted] Organization Name (eg, company) [Default Company Ltd]:Sandbox Organizational Unit Name (eg, section) []:IT Common Name (eg, your name or your server hostname) []:docs.sandbox.io Email Address []:web@sandbox.io Install mod_ssl, this will add ssl.conf in /etc/httpd/conf.d/ directory. yum install -y mod_ssl Edit /etc/httpd/conf.d/ssl.conf, Create a virtual host entry that redirects http(port 80) access to https(port 443) vim /etc/httpd/conf.d/ssl.conf <VirtualHost _default_:80> ServerName docs.sandbox.io:80 ServerAlias docs.sandbox.io Redirect permanent / https://docs.sandbox.io/ </VirtualHost> If virtual host for port 443 is not there, create it. <VirtualHost _default_:443> DocumentRoot \"/var/www/sandbox/site\" ServerName docs.sandbox.io:443 ErrorLog logs/ssl_error_log TransferLog logs/ssl_access_log LogLevel warn SSLEngine on SSLProtocol all -SSLv2 -SSLv3 SSLCipherSuite HIGH:3DES:!aNULL:!MD5:!SEED:!IDEA SSLCertificateFile /etc/pki/tls/certs/docs.crt SSLCertificateKeyFile /etc/pki/tls/private/docs.key <Files ~ \"\\.(cgi|shtml|phtml|php3?)$\"> SSLOptions +StdEnvVars </Files> <Directory \"/var/www/cgi-bin\"> SSLOptions +StdEnvVars </Directory> BrowserMatch \"MSIE [2-5]\" \\ nokeepalive ssl-unclean-shutdown \\ downgrade-1.0 force-response-1.0 CustomLog logs/ssl_request_log \\ \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\" </VirtualHost> Make sure to replace SSLCertificateFile and SSLCertificateKeyFile in the section below of the configuration file SSLCertificateFile /etc/pki/tls/certs/docs.crt SSLCertificateKeyFile /etc/pki/tls/private/docs.key Restart httpd service [root@docs ~]# systemctl status httpd | head \u25cf httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2021-10-29 03:28:39 PST; 15s ago","title":"Self signed certificate setup for HTTPS access on webservers"},{"location":"spacewalk/","text":"Spacewalk documentation DHCP and DNS Entries Forward lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 Reverse lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 101 IN PTR spacewalk.sandbox.io. DHCP entry host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; } Installation Source installation instructions is found here , documenting for my own copy. Prerequisites Outbound open ports 80, 443 Inbound open ports 80, 443, 5222 (only if you want to push actions to client machines) and 5269 (only for push actions to a Spacewalk Proxy), 69 udp if you want to use tftp Storage for database: 250 KiB per client system + 500 KiB per channel + 230 KiB per package in channel (i.e. 1.1GiB for channel with 5000 packages) Storage for packages (default /var/satellite): Depends on what you are storing; Red Hat recommend 6GB per channel for their channels 2GB RAM minimum, 4GB recommended Make sure your underlying OS is fully up-to-date. If you use LDAP as a central identity service and wish to pull user and group information from it, see SpacewalkWithLDAP Additional repos & packages All other dependecies outside base repo will be available on EPEL repository rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm Database server I will let spacewalk setup postgresql database first, before moving it to a separate server. yum -y install spacewalk-setup-postgresql Installing Spacewalk yum -y install spacewalk-postgresql Configuring the Firewall Firewalld is installed on my server so the commands i will use is based on firewalld commands firewall-cmd --add-service=http firewall-cmd --add-service=https firewall-cmd --runtime-to-perm Configuring Spacewalk Please make sure that your spacewalk server have a resolvable FQDN. spacewalk-setup An example session is as follows: [root@spacewalk ~]# spacewalk-setup * Setting up SELinux.. ** Database: Setting up database connection for PostgreSQL backend. ** Database: Installing the database: ** Database: This is a long process that is logged in: ** Database: /var/log/rhn/install_db.log *** Progress: # ** Database: Installation complete. ** Database: Populating database. *** Progress: ############################ * Configuring tomcat. * Setting up users and groups. ** GPG: Initializing GPG and importing key. ** GPG: Creating /root/.gnupg directory You must enter an email address. Admin Email Address? [redacted] * Performing initial configuration. ** Package installation: Locking required rpm versions. * Configuring apache SSL virtual host. Should setup configure apaches default ssl server for you (saves original ssl.conf) [Y]? ** /etc/httpd/conf.d/ssl.conf has been backed up to ssl.conf-swsave * Configuring jabberd. * Creating SSL certificates. CA certificate password? password Re-enter CA certificate password? password Cname alias of the machine (comma seperated)? spacewalk.sandbox.io Organization? sandbox Organization Unit [spacewalk.sandbox.io]? Email Address [[redacted]]? City? [redacted] State? [redacted] Country code (Examples: \"US\", \"JP\", \"IN\", or type \"?\" to see a list)? [redacted] ** SSL: Generating CA certificate. ** SSL: Deploying CA certificate. ** SSL: Generating server certificate. ** SSL: Storing SSL certificates. * Deploying configuration files. * Update configuration in database. * Setting up Cobbler.. Cobbler requires tftp and xinetd services be turned on for PXE provisioning functionality. Enable these services [Y]? y * Restarting services. Installation complete. Visit https://spacewalk.sandbox.io to create the Spacewalk administrator account. Creating Spacewalk CentOS channel Creating software channel Channels -> Manage Software Channels -> Create Channel Fill up the fields accordingly: Channel Name: Centos 7 Base - x86_64 Channel Label: centos7-base-x86_64 Architecture: x86_64 Yum Repository Checksum Type: sha1 Channel Summary: Centos 7 Base - x86_64 Optional: For additional security, add GPG key fingerprint in Security:GPG section, you can get GPG key using gpg --with-fingerprint [key] . cd /etc/pki/rpm-gpg/ gpg --with-fingerprint RPM-GPG-KEY-CentOS-7 pub 4096R/F4A80EB5 2014-06-23 CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org> Key fingerprint = 6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5 Creating repositories Channels -> Manage Software Channels -> Manage Repositories -> Create Repository Repository Label: centos7-base-x86_64 Repository URL: http://mirror.centos.org/centos/7/os/x86_64/ Repository Type: yum Syncing the channel and the repos In the command line type: spacewalk-repo-sync -c centos7-base-x86_64 Creating Base Children Channels Creating Updates Channel Create a new channel again, now making the Base channel as the parent channel: Channel Name: Centos 7 Updates - x86_64 Channel Label: centos7-updates-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos7-updates-x86_64 Security:GPG section in this channel is the same with the parent channel: GPG key URL: http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7 GPG key ID: F4A80EB5 GPG key Fingerprint: 6341 AB27 53D78A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5 Click Create Channel to create the children channel Creating EPEL Channel Same with Updates channel, make the base channel as parent channel: Channel Name: Epel 7 - x86_64 Channel Label: epel7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: Extra Packages for Enterprise Linux 7 - x86_64 As for the Security: GPG, change the details to: GPG key URL: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 GPG key ID: 352C64E5 GPG key Fingerprint: 91E9 7D7C 4A5E 96F1 7F3E 888F 6A2F AEA2 352C 64E5 Alternatively, you can type gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 to the terminal. Creating Spacewalk Client Channel Same with Updates and EPEL channel, make the base channel as parent channel: Channel Name: Spacewalk Client 2.10 - x86_64 Channel Label: spacewalk-client-210-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: spacewalk-client-210-x86_64 As for the Security: GPG, change the details to: GPG key URL: https://download.copr.fedorainfracloud.org/results/@spacewalkproject/spacewalk-2.10/pubkey.gpg GPG key ID: BC2E6843 GPG key Fingerprint: E8C0 573E 5B62 BB7C 98C1 A2AC 770C E53E BC2E 6843 Creating Extras Channel Channel Name: Centos 7 Extras - x86_64 Channel Label: centos-7-extras-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos-7-extras-x86_64 Creating Puppet Channel This Channel is for the automation server, puppet-server package in EPEL is quite outdated and i want to use the new version. Channel Name: Puppet 7 EL7 x86_64 Channel Label: puppet-7-el7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: puppet-7-el7-x86_64 Creating Repositories for Children Channels Create a repository for each children channels: Repository URL for centos7-updates-x86_64: http://mirror.rise.ph/centos/7.8.2003/updates/x86_64/ # This might be changed since the updated CentOS version upon this writing is 7.8 Repository URL for epel7-x86_64: https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/ Repository URL for spacewalk-client-210-x86_64: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10-client/epel-7-x86_64/ Repository URL for centos-7-extras-x86_64: http://mirror.centos.org/centos/7/extras/x86_64/ Repositort URL for puppet-7-el7-x86_64: http://yum.puppetlabs.com/puppet/el/7/x86_64/ Pushing packages to spacewalk repo Add created repository for each children channels and sync the channels through spacewalk website or terminal: spacewalk-repo-sync -c spacewalk-client-210-x86_64 spacewalk-repo-sync -c centos7-updates-x86_64 Example output: [root@spacewalk rpm-gpg]# spacewalk-repo-sync -c spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 | Channel: spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 Sync of channel started. 22:50:25 22:50:25 Processing repository with URL: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10/epel-7-x86_64/ 22:50:27 Packages in repo: 206 22:50:27 Packages already synced: 0 22:50:27 Packages to sync: 138 22:50:27 New packages to download: 138 22:50:27 Downloading packages: Due to issues with EPEL repository is too big to sync to spacewalk, you may want to download the rpm packages you want to be in the EPEL channel, then push it to spacewalk rhnpush -v --channel=epel7-x86_64 --server=http://localhost --dir=/root/epel-packages Distribution and Kickstart Profile Creating Distributions On Systems tab -> Kickstart -> Distribution -> Create a new Distribution Download the latest CentOS ISO file: wget http://mirror.pregi.net/centos/7.8.2003/isos/x86_64/CentOS-7-x86_64-Everything-2003.iso Mount the ISO file and copy the contents to /var/spacewalk/repo/CentOS7-x86_64 cp --recursive --verbose /mnt/distros/CentOS7-x86_64/ /var/spacewalk/repo/ Change the permissions of the file and its SELINUX file content: cd /var/spacewalk/repo/ find ./ -type d -exec chmod 755 {} \\; find ./ -type f -exec chmod 644 {} \\; find /var/spacewalk/repo/ -type f -exec chown apache:apache {} \\; find /var/spacewalk/repo/ -type d -exec chown apache:root {} \\; semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv . Create the Distribution: Distribution Label: centos7-base-x86_64 Tree Path: /var/spacewalk/repo/CentOS7-x86_64 Base Channel: Centos 7 Base - x86_64 Installer Generation: Red Hat Enterprise Linux 7 Create Activation Key On Systems tab -> Activation Key -> Create New Key On Child Channels, click all child channels and hit Update Key: Creating Kickstart Profile On Systems tab -> Kickstart -> Profiles -> Create Kickstart Profile Label: CentOS7-ks Base Channel: Centos 7 Base - x86_64 CHECK: Always use the newest Tree for this base channel. \"Newest\" is determined by the date it was last modified. Virtualization Type: None On Step 2, leave it as default: On Step 3, Setup Root Password: You have now created your Kickstart profile: On your created kickstart profile, go to Kickstart Details then Operating System tab. Click all the child channels and click Update Kickstart: In Advance Options, Change timezone to Asia/Manila In Software Tab, in Package Groups, Add the following in the comment box and click update packages: @ Base rhn-setup rhn-check rhn-client-tools rhn-custom-info rhncfg-actions rhncfg-client rhncfg-management yum-rhn-plugin python-dmidecode python-hwdata In Activation Keys, Click CentOS 7 Activation Key: Setting up Errata, PXE configuration, Etc. Errata Importation Sources: link1 link2 import spacewalk-errata-import.sh on GitHub to your system. and make the script executable. chmod +x spacewalk-errata-import.sh Download the following packages: yum install fping perl-Frontier-RPC perl-Text-Unidecode -y Run the script: [root@spacewalk ~]# ./spacewalk-errata-import.sh -u '$username' -p '$password' -s '$spacewalk_fqdn' -v --2020-08-04 06:01:27-- https://cefs.steve-meier.de/errata.latest.xml Resolving cefs.steve-meier.de (cefs.steve-meier.de)... 143.204.243.87, 143.204.243.118, 143.204.243.3, ... Connecting to cefs.steve-meier.de (cefs.steve-meier.de)|143.204.243.87|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://cefs.b-cdn.net/errata.latest.xml [following] --2020-08-04 06:01:28-- https://cefs.b-cdn.net/errata.latest.xml Resolving cefs.b-cdn.net (cefs.b-cdn.net)... 89.187.162.241 Connecting to cefs.b-cdn.net (cefs.b-cdn.net)|89.187.162.241|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1284865 (1.2M) [text/xml] Saving to: \u2018/root/errata.latest.xml\u2019 100%[======================================>] 1,284,865 1.41MB/s in 0.9s Errata will now appear on Spacewalk UI: PXE Configuration At first, you will be stuck to the Boot Menu due to TFTP is still disabled in the system: Enable TFTP Service on Spacewalk Server: Install xinetd service, an additional required package for TFTP to work. yum install -y xinetd Set disable = no line on /etc/xinetd.d/tftp service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -c -s /tftpboot disable = no per_source = 15 cps = 80 2 flags = IPv4 } Enable TFTP and xinetd Service on Spacewalk Server: systemctl enable --now tftp.service xinetd firewall-cmd --add-service=tftp firewall-cmd --runtime-to-perm When you run the PXE again, you will be having error on audit logger of spacewalk: Follow what journalctl troubleshooter says: setsebool -P tftp_home_dir 1 Change the SELinux context of the following semanage fcontext -a -t public_content_t \"/var/lib/tftpboot/.*\" semanage fcontext -a -t public_content_t \"/var/www/cobbler/images/.*\" semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv /var/spacewalk/repo/CentOS7-x86_64 Run cobbler sync to set up the files on /var/lib/tftp folder, the output should be like this: [root@spacewalk CentOS7-x86_64]# cobbler sync task started: 2020-08-05_043208_sync task started (id=Sync, time=Wed Aug 5 04:32:08 2020) running pre-sync triggers cleaning trees removing: /var/www/cobbler/images/centos7-base-x86_64:1:sandbox removing: /var/lib/tftpboot/pxelinux.cfg/default removing: /var/lib/tftpboot/grub/images ... received on stdout: received on stderr: generating PXE menu structure running post-sync triggers ... *** TASK COMPLETE *** GPG Public Keys and SSL Certificates This must be set to automatically install certificates needed for package authenticity. Systems Tab -> Kickstart -> GPG and SSL Keys -> Create Stored Key/Cert After that, you can paste or upload GPG or SSL Keys. Go to Profiles then select you kickstart file, in the System Details choose GPG & SSL, and choose the GPG keys you save in the last instruction, then hit update keys.","title":"Spacewalk"},{"location":"spacewalk/#spacewalk-documentation","text":"","title":"Spacewalk documentation"},{"location":"spacewalk/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 Reverse lookup zone @ IN NS spacewalk.sandbox.io. spacewalk IN A 10.11.100.101 101 IN PTR spacewalk.sandbox.io. DHCP entry host spacewalk { hardware ethernet 3E:D1:95:C2:49:9C; fixed-address 10.11.100.101; }","title":"DHCP and DNS Entries"},{"location":"spacewalk/#installation","text":"Source installation instructions is found here , documenting for my own copy.","title":"Installation"},{"location":"spacewalk/#prerequisites","text":"Outbound open ports 80, 443 Inbound open ports 80, 443, 5222 (only if you want to push actions to client machines) and 5269 (only for push actions to a Spacewalk Proxy), 69 udp if you want to use tftp Storage for database: 250 KiB per client system + 500 KiB per channel + 230 KiB per package in channel (i.e. 1.1GiB for channel with 5000 packages) Storage for packages (default /var/satellite): Depends on what you are storing; Red Hat recommend 6GB per channel for their channels 2GB RAM minimum, 4GB recommended Make sure your underlying OS is fully up-to-date. If you use LDAP as a central identity service and wish to pull user and group information from it, see SpacewalkWithLDAP","title":"Prerequisites"},{"location":"spacewalk/#additional-repos-packages","text":"All other dependecies outside base repo will be available on EPEL repository rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm","title":"Additional repos &amp; packages"},{"location":"spacewalk/#database-server","text":"I will let spacewalk setup postgresql database first, before moving it to a separate server. yum -y install spacewalk-setup-postgresql","title":"Database server"},{"location":"spacewalk/#installing-spacewalk","text":"yum -y install spacewalk-postgresql","title":"Installing Spacewalk"},{"location":"spacewalk/#configuring-the-firewall","text":"Firewalld is installed on my server so the commands i will use is based on firewalld commands firewall-cmd --add-service=http firewall-cmd --add-service=https firewall-cmd --runtime-to-perm","title":"Configuring the Firewall"},{"location":"spacewalk/#configuring-spacewalk","text":"Please make sure that your spacewalk server have a resolvable FQDN. spacewalk-setup An example session is as follows: [root@spacewalk ~]# spacewalk-setup * Setting up SELinux.. ** Database: Setting up database connection for PostgreSQL backend. ** Database: Installing the database: ** Database: This is a long process that is logged in: ** Database: /var/log/rhn/install_db.log *** Progress: # ** Database: Installation complete. ** Database: Populating database. *** Progress: ############################ * Configuring tomcat. * Setting up users and groups. ** GPG: Initializing GPG and importing key. ** GPG: Creating /root/.gnupg directory You must enter an email address. Admin Email Address? [redacted] * Performing initial configuration. ** Package installation: Locking required rpm versions. * Configuring apache SSL virtual host. Should setup configure apaches default ssl server for you (saves original ssl.conf) [Y]? ** /etc/httpd/conf.d/ssl.conf has been backed up to ssl.conf-swsave * Configuring jabberd. * Creating SSL certificates. CA certificate password? password Re-enter CA certificate password? password Cname alias of the machine (comma seperated)? spacewalk.sandbox.io Organization? sandbox Organization Unit [spacewalk.sandbox.io]? Email Address [[redacted]]? City? [redacted] State? [redacted] Country code (Examples: \"US\", \"JP\", \"IN\", or type \"?\" to see a list)? [redacted] ** SSL: Generating CA certificate. ** SSL: Deploying CA certificate. ** SSL: Generating server certificate. ** SSL: Storing SSL certificates. * Deploying configuration files. * Update configuration in database. * Setting up Cobbler.. Cobbler requires tftp and xinetd services be turned on for PXE provisioning functionality. Enable these services [Y]? y * Restarting services. Installation complete. Visit https://spacewalk.sandbox.io to create the Spacewalk administrator account.","title":"Configuring Spacewalk"},{"location":"spacewalk/#creating-spacewalk-centos-channel","text":"","title":"Creating Spacewalk CentOS channel"},{"location":"spacewalk/#creating-software-channel","text":"Channels -> Manage Software Channels -> Create Channel Fill up the fields accordingly: Channel Name: Centos 7 Base - x86_64 Channel Label: centos7-base-x86_64 Architecture: x86_64 Yum Repository Checksum Type: sha1 Channel Summary: Centos 7 Base - x86_64 Optional: For additional security, add GPG key fingerprint in Security:GPG section, you can get GPG key using gpg --with-fingerprint [key] . cd /etc/pki/rpm-gpg/ gpg --with-fingerprint RPM-GPG-KEY-CentOS-7 pub 4096R/F4A80EB5 2014-06-23 CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org> Key fingerprint = 6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5","title":"Creating software channel"},{"location":"spacewalk/#creating-repositories","text":"Channels -> Manage Software Channels -> Manage Repositories -> Create Repository Repository Label: centos7-base-x86_64 Repository URL: http://mirror.centos.org/centos/7/os/x86_64/ Repository Type: yum","title":"Creating repositories"},{"location":"spacewalk/#syncing-the-channel-and-the-repos","text":"In the command line type: spacewalk-repo-sync -c centos7-base-x86_64","title":"Syncing the channel and the repos"},{"location":"spacewalk/#creating-base-children-channels","text":"","title":"Creating Base Children Channels"},{"location":"spacewalk/#creating-updates-channel","text":"Create a new channel again, now making the Base channel as the parent channel: Channel Name: Centos 7 Updates - x86_64 Channel Label: centos7-updates-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos7-updates-x86_64 Security:GPG section in this channel is the same with the parent channel: GPG key URL: http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7 GPG key ID: F4A80EB5 GPG key Fingerprint: 6341 AB27 53D78A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5 Click Create Channel to create the children channel","title":"Creating Updates Channel"},{"location":"spacewalk/#creating-epel-channel","text":"Same with Updates channel, make the base channel as parent channel: Channel Name: Epel 7 - x86_64 Channel Label: epel7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: Extra Packages for Enterprise Linux 7 - x86_64 As for the Security: GPG, change the details to: GPG key URL: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 GPG key ID: 352C64E5 GPG key Fingerprint: 91E9 7D7C 4A5E 96F1 7F3E 888F 6A2F AEA2 352C 64E5 Alternatively, you can type gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 to the terminal.","title":"Creating EPEL Channel"},{"location":"spacewalk/#creating-spacewalk-client-channel","text":"Same with Updates and EPEL channel, make the base channel as parent channel: Channel Name: Spacewalk Client 2.10 - x86_64 Channel Label: spacewalk-client-210-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: spacewalk-client-210-x86_64 As for the Security: GPG, change the details to: GPG key URL: https://download.copr.fedorainfracloud.org/results/@spacewalkproject/spacewalk-2.10/pubkey.gpg GPG key ID: BC2E6843 GPG key Fingerprint: E8C0 573E 5B62 BB7C 98C1 A2AC 770C E53E BC2E 6843","title":"Creating Spacewalk Client Channel"},{"location":"spacewalk/#creating-extras-channel","text":"Channel Name: Centos 7 Extras - x86_64 Channel Label: centos-7-extras-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: centos-7-extras-x86_64","title":"Creating Extras Channel"},{"location":"spacewalk/#creating-puppet-channel","text":"This Channel is for the automation server, puppet-server package in EPEL is quite outdated and i want to use the new version. Channel Name: Puppet 7 EL7 x86_64 Channel Label: puppet-7-el7-x86_64 Parent Channel: Centos 7 Base - x86_64 Channel Summary: puppet-7-el7-x86_64","title":"Creating Puppet Channel"},{"location":"spacewalk/#creating-repositories-for-children-channels","text":"Create a repository for each children channels: Repository URL for centos7-updates-x86_64: http://mirror.rise.ph/centos/7.8.2003/updates/x86_64/ # This might be changed since the updated CentOS version upon this writing is 7.8 Repository URL for epel7-x86_64: https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/ Repository URL for spacewalk-client-210-x86_64: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10-client/epel-7-x86_64/ Repository URL for centos-7-extras-x86_64: http://mirror.centos.org/centos/7/extras/x86_64/ Repositort URL for puppet-7-el7-x86_64: http://yum.puppetlabs.com/puppet/el/7/x86_64/","title":"Creating Repositories for Children Channels"},{"location":"spacewalk/#pushing-packages-to-spacewalk-repo","text":"Add created repository for each children channels and sync the channels through spacewalk website or terminal: spacewalk-repo-sync -c spacewalk-client-210-x86_64 spacewalk-repo-sync -c centos7-updates-x86_64 Example output: [root@spacewalk rpm-gpg]# spacewalk-repo-sync -c spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 | Channel: spacewalk-client-210-x86_64 22:50:25 ====================================== 22:50:25 Sync of channel started. 22:50:25 22:50:25 Processing repository with URL: https://copr-be.cloud.fedoraproject.org/results/%40spacewalkproject/spacewalk-2.10/epel-7-x86_64/ 22:50:27 Packages in repo: 206 22:50:27 Packages already synced: 0 22:50:27 Packages to sync: 138 22:50:27 New packages to download: 138 22:50:27 Downloading packages: Due to issues with EPEL repository is too big to sync to spacewalk, you may want to download the rpm packages you want to be in the EPEL channel, then push it to spacewalk rhnpush -v --channel=epel7-x86_64 --server=http://localhost --dir=/root/epel-packages","title":"Pushing packages to spacewalk repo"},{"location":"spacewalk/#distribution-and-kickstart-profile","text":"","title":"Distribution and Kickstart Profile"},{"location":"spacewalk/#creating-distributions","text":"On Systems tab -> Kickstart -> Distribution -> Create a new Distribution Download the latest CentOS ISO file: wget http://mirror.pregi.net/centos/7.8.2003/isos/x86_64/CentOS-7-x86_64-Everything-2003.iso Mount the ISO file and copy the contents to /var/spacewalk/repo/CentOS7-x86_64 cp --recursive --verbose /mnt/distros/CentOS7-x86_64/ /var/spacewalk/repo/ Change the permissions of the file and its SELINUX file content: cd /var/spacewalk/repo/ find ./ -type d -exec chmod 755 {} \\; find ./ -type f -exec chmod 644 {} \\; find /var/spacewalk/repo/ -type f -exec chown apache:apache {} \\; find /var/spacewalk/repo/ -type d -exec chown apache:root {} \\; semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv . Create the Distribution: Distribution Label: centos7-base-x86_64 Tree Path: /var/spacewalk/repo/CentOS7-x86_64 Base Channel: Centos 7 Base - x86_64 Installer Generation: Red Hat Enterprise Linux 7","title":"Creating Distributions"},{"location":"spacewalk/#create-activation-key","text":"On Systems tab -> Activation Key -> Create New Key On Child Channels, click all child channels and hit Update Key:","title":"Create Activation Key"},{"location":"spacewalk/#creating-kickstart-profile","text":"On Systems tab -> Kickstart -> Profiles -> Create Kickstart Profile Label: CentOS7-ks Base Channel: Centos 7 Base - x86_64 CHECK: Always use the newest Tree for this base channel. \"Newest\" is determined by the date it was last modified. Virtualization Type: None On Step 2, leave it as default: On Step 3, Setup Root Password: You have now created your Kickstart profile: On your created kickstart profile, go to Kickstart Details then Operating System tab. Click all the child channels and click Update Kickstart: In Advance Options, Change timezone to Asia/Manila In Software Tab, in Package Groups, Add the following in the comment box and click update packages: @ Base rhn-setup rhn-check rhn-client-tools rhn-custom-info rhncfg-actions rhncfg-client rhncfg-management yum-rhn-plugin python-dmidecode python-hwdata In Activation Keys, Click CentOS 7 Activation Key:","title":"Creating Kickstart Profile"},{"location":"spacewalk/#setting-up-errata-pxe-configuration-etc","text":"","title":"Setting up Errata, PXE configuration, Etc."},{"location":"spacewalk/#errata-importation","text":"Sources: link1 link2 import spacewalk-errata-import.sh on GitHub to your system. and make the script executable. chmod +x spacewalk-errata-import.sh Download the following packages: yum install fping perl-Frontier-RPC perl-Text-Unidecode -y Run the script: [root@spacewalk ~]# ./spacewalk-errata-import.sh -u '$username' -p '$password' -s '$spacewalk_fqdn' -v --2020-08-04 06:01:27-- https://cefs.steve-meier.de/errata.latest.xml Resolving cefs.steve-meier.de (cefs.steve-meier.de)... 143.204.243.87, 143.204.243.118, 143.204.243.3, ... Connecting to cefs.steve-meier.de (cefs.steve-meier.de)|143.204.243.87|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://cefs.b-cdn.net/errata.latest.xml [following] --2020-08-04 06:01:28-- https://cefs.b-cdn.net/errata.latest.xml Resolving cefs.b-cdn.net (cefs.b-cdn.net)... 89.187.162.241 Connecting to cefs.b-cdn.net (cefs.b-cdn.net)|89.187.162.241|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1284865 (1.2M) [text/xml] Saving to: \u2018/root/errata.latest.xml\u2019 100%[======================================>] 1,284,865 1.41MB/s in 0.9s Errata will now appear on Spacewalk UI:","title":"Errata Importation"},{"location":"spacewalk/#pxe-configuration","text":"At first, you will be stuck to the Boot Menu due to TFTP is still disabled in the system: Enable TFTP Service on Spacewalk Server: Install xinetd service, an additional required package for TFTP to work. yum install -y xinetd Set disable = no line on /etc/xinetd.d/tftp service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -c -s /tftpboot disable = no per_source = 15 cps = 80 2 flags = IPv4 } Enable TFTP and xinetd Service on Spacewalk Server: systemctl enable --now tftp.service xinetd firewall-cmd --add-service=tftp firewall-cmd --runtime-to-perm When you run the PXE again, you will be having error on audit logger of spacewalk: Follow what journalctl troubleshooter says: setsebool -P tftp_home_dir 1 Change the SELinux context of the following semanage fcontext -a -t public_content_t \"/var/lib/tftpboot/.*\" semanage fcontext -a -t public_content_t \"/var/www/cobbler/images/.*\" semanage fcontext -a -t spacewalk_data_t '/var/spacewalk/repo/CentOS7-x86_64(/.*)?' restorecon -Rv /var/spacewalk/repo/CentOS7-x86_64 Run cobbler sync to set up the files on /var/lib/tftp folder, the output should be like this: [root@spacewalk CentOS7-x86_64]# cobbler sync task started: 2020-08-05_043208_sync task started (id=Sync, time=Wed Aug 5 04:32:08 2020) running pre-sync triggers cleaning trees removing: /var/www/cobbler/images/centos7-base-x86_64:1:sandbox removing: /var/lib/tftpboot/pxelinux.cfg/default removing: /var/lib/tftpboot/grub/images ... received on stdout: received on stderr: generating PXE menu structure running post-sync triggers ... *** TASK COMPLETE ***","title":"PXE Configuration"},{"location":"spacewalk/#gpg-public-keys-and-ssl-certificates","text":"This must be set to automatically install certificates needed for package authenticity. Systems Tab -> Kickstart -> GPG and SSL Keys -> Create Stored Key/Cert After that, you can paste or upload GPG or SSL Keys. Go to Profiles then select you kickstart file, in the System Details choose GPG & SSL, and choose the GPG keys you save in the last instruction, then hit update keys.","title":"GPG Public Keys and SSL Certificates"},{"location":"storage/","text":"Storage Server DHCP and DNS Entries Forward lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 Reverse lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 106 IN PTR storage.sandbox.io. DHCP entry host storage { hardware ethernet 02:1D:98:67:47:CC; fixed-address 10.11.100.106; } Scope Storage location of Bacula server, LUN storage share will be for the bacula backup itself, NFS storage share will be for the rest of the nodes storage backup mounted on Details /root/disk1.img /dev/sdb /mnt/LUN Storage for main backup server /var/nfs storage.sandbox.io:/var/nfs /mnt/NFS Storage for all nodes Installation and setup Package Installation Install targetcli and iscsi-initiator-utils to configure this server as a LUN device - name: install targetcli and iscsi-initiator-utils yum: name: - targetcli - iscsi-initiator-utils - nfs-utils state: present Start necessary servers - name: enable and start nfs services service: name: \"{{services}}\" state: started enabled: true loop: - \"target\" - \"rpcbind\" - \"nfs-server\" - \"nfs-lock\" - \"nfs-idmap\" loop_control: loop_var: services Create /var/nfs directory, this will act as the shared directory of the storage server - name: create nfs directory to share file: state: directory path: /var/nfs owner: nfsnobody group: nfsnobody mode: '1777' NFS Share setup Setup /var/nfs directory as shared directory in /etc/exports file, restart the service. - name: setup required files and directories copy: dest: /etc/exports content: \"/var/nfs 10.11.100.0/24(rw,sync,no_root_squash,no_all_squash)\" register: nfs_config - name: check if nfs_config chaged, restart nfs-server service: name: nfs-server state: restarted when: nfs_config.changed Open necessary ports in firewalld # For defined service names in firewalld - name: open ports in firewalld for services firewalld: service: \"{{firewall_services}}\" state: enabled zone: public immediate: true permanent: true loop: - 'nfs' - 'mountd' - 'rpc-bind' loop_control: loop_var: firewall_services # For not defined service - name: open ports in firewalld for ports firewalld: port: '3260/tcp' state: enabled immediate: true permanent: true LUN Share setup As of now, i cant find any utilities for targetcli on ansible. I will pass the commands via shell module of ansible to and will only run 1 time(creates system checks file) - name: check if LUN-setup-done exist stat: path: /home/users/bot-acc/.system-checks/LUN-setup-done register: LUN_status - name: run targetcli commands when file LUN_status does not exist shell: cmd: '{{item}}' with_items: - 'targetcli /backstores/fileio create LUN_1 /root/disk1.img 7g' - 'targetcli /iscsi create iqn.2021-09.io.sandbox.storage' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/luns create /backstores/fileio/LUN_1 LUN1' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/acls create iqn.2021-09.io.sandbox.storage:backup-acl' - 'targetcli saveconfig' - 'touch /home/users/bot-acc/.system-checks/LUN-setup-done' warn: false when: not LUN_status.stat.exists Partitioning, Mounting, Etc. In order to partition the LUN device(this is needed right?) we need to discover the LUN device internally. Define the IQN in /etc/iscsi/initiatorname.iscsi and start the service - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started when: not LUN_login_status.stat.exists Discover and Login to LUN share - name: discover the LUN share open_iscsi: show_nodes: true portal: 'dev1.sandbox.io' discover: true when: not LUN_login_status.stat.exists - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage' when: not LUN_login_status.stat.exists Partition and create filesystem - name: partition the device parted: device: /dev/sdb number: 1 state: present - name: create filesystem of the device filesystem: dev: /dev/sdb1 fstype: xfs Since there is an issue when 2 nodes connecting to target, we need to logout on iscsi - name: logout to LUN share open_iscsi: login: false target: 'iqn.2021-09.io.sandbox.storage' when: LUN_login.changed == true register: LUN_logout Reboot once configuration has been setup - name: reboot after setup shell: cmd: echo 'reboot' | at now +1 minute when: LUN_logout.changed == true","title":"Storage"},{"location":"storage/#storage-server","text":"","title":"Storage Server"},{"location":"storage/#dhcp-and-dns-entries","text":"Forward lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 Reverse lookup zone @ IN NS storage.sandbox.io. storage IN A 10.11.100.106 106 IN PTR storage.sandbox.io. DHCP entry host storage { hardware ethernet 02:1D:98:67:47:CC; fixed-address 10.11.100.106; }","title":"DHCP and DNS Entries"},{"location":"storage/#scope","text":"Storage location of Bacula server, LUN storage share will be for the bacula backup itself, NFS storage share will be for the rest of the nodes storage backup mounted on Details /root/disk1.img /dev/sdb /mnt/LUN Storage for main backup server /var/nfs storage.sandbox.io:/var/nfs /mnt/NFS Storage for all nodes","title":"Scope"},{"location":"storage/#installation-and-setup","text":"","title":"Installation and setup"},{"location":"storage/#package-installation","text":"Install targetcli and iscsi-initiator-utils to configure this server as a LUN device - name: install targetcli and iscsi-initiator-utils yum: name: - targetcli - iscsi-initiator-utils - nfs-utils state: present Start necessary servers - name: enable and start nfs services service: name: \"{{services}}\" state: started enabled: true loop: - \"target\" - \"rpcbind\" - \"nfs-server\" - \"nfs-lock\" - \"nfs-idmap\" loop_control: loop_var: services Create /var/nfs directory, this will act as the shared directory of the storage server - name: create nfs directory to share file: state: directory path: /var/nfs owner: nfsnobody group: nfsnobody mode: '1777'","title":"Package Installation"},{"location":"storage/#nfs-share-setup","text":"Setup /var/nfs directory as shared directory in /etc/exports file, restart the service. - name: setup required files and directories copy: dest: /etc/exports content: \"/var/nfs 10.11.100.0/24(rw,sync,no_root_squash,no_all_squash)\" register: nfs_config - name: check if nfs_config chaged, restart nfs-server service: name: nfs-server state: restarted when: nfs_config.changed Open necessary ports in firewalld # For defined service names in firewalld - name: open ports in firewalld for services firewalld: service: \"{{firewall_services}}\" state: enabled zone: public immediate: true permanent: true loop: - 'nfs' - 'mountd' - 'rpc-bind' loop_control: loop_var: firewall_services # For not defined service - name: open ports in firewalld for ports firewalld: port: '3260/tcp' state: enabled immediate: true permanent: true","title":"NFS Share setup"},{"location":"storage/#lun-share-setup","text":"As of now, i cant find any utilities for targetcli on ansible. I will pass the commands via shell module of ansible to and will only run 1 time(creates system checks file) - name: check if LUN-setup-done exist stat: path: /home/users/bot-acc/.system-checks/LUN-setup-done register: LUN_status - name: run targetcli commands when file LUN_status does not exist shell: cmd: '{{item}}' with_items: - 'targetcli /backstores/fileio create LUN_1 /root/disk1.img 7g' - 'targetcli /iscsi create iqn.2021-09.io.sandbox.storage' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/luns create /backstores/fileio/LUN_1 LUN1' - 'targetcli /iscsi/iqn.2021-09.io.sandbox.storage/tpg1/acls create iqn.2021-09.io.sandbox.storage:backup-acl' - 'targetcli saveconfig' - 'touch /home/users/bot-acc/.system-checks/LUN-setup-done' warn: false when: not LUN_status.stat.exists","title":"LUN Share setup"},{"location":"storage/#partitioning-mounting-etc","text":"In order to partition the LUN device(this is needed right?) we need to discover the LUN device internally. Define the IQN in /etc/iscsi/initiatorname.iscsi and start the service - name: insert IQN in initiatorname.iscsi copy: dest: /etc/iscsi/initiatorname.iscsi content: 'InitiatorName=iqn.2021-09.io.sandbox.storage:backup-acl' - name: iscsid service start service: name: iscsid state: started when: not LUN_login_status.stat.exists Discover and Login to LUN share - name: discover the LUN share open_iscsi: show_nodes: true portal: 'dev1.sandbox.io' discover: true when: not LUN_login_status.stat.exists - name: login to LUN share open_iscsi: login: true target: 'iqn.2021-09.io.sandbox.storage' when: not LUN_login_status.stat.exists Partition and create filesystem - name: partition the device parted: device: /dev/sdb number: 1 state: present - name: create filesystem of the device filesystem: dev: /dev/sdb1 fstype: xfs Since there is an issue when 2 nodes connecting to target, we need to logout on iscsi - name: logout to LUN share open_iscsi: login: false target: 'iqn.2021-09.io.sandbox.storage' when: LUN_login.changed == true register: LUN_logout Reboot once configuration has been setup - name: reboot after setup shell: cmd: echo 'reboot' | at now +1 minute when: LUN_logout.changed == true","title":"Partitioning, Mounting, Etc."}]}